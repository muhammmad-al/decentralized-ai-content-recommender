{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/ean/xFmbBdDrle4XdC1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammmad-al/decentralized-ai-content-recommender/blob/main/reddit_federated_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Initial Setup and Imports (same as before)\n",
        "%cd /content\n",
        "!rm -rf *  # Remove everything first\n",
        "!git clone https://github.com/muhammmad-al/decentralized-ai-content-recommender.git\n",
        "%cd decentralized-ai-content-recommender\n",
        "\n",
        "# Install required packages for federated learning\n",
        "!pip install flwr tensorflow pandas scikit-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import flwr as fl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import logging\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Cell 2: Load and Preprocess Dataset for Node 1\n",
        "def load_node_data(node_number=1, primary_category='ai'):\n",
        "    \"\"\"Load and preprocess data for a specific node\n",
        "\n",
        "    Args:\n",
        "        node_number: Which node this is (1,2,3)\n",
        "        primary_category: Main category for this node ('ai','web3','music')\n",
        "    \"\"\"\n",
        "    # Load all datasets\n",
        "    datasets = {\n",
        "        'ai': pd.read_csv('data/raw/reddit_analysis_ai.csv'),\n",
        "        'music': pd.read_csv('data/raw/reddit_analysis_music.csv'),\n",
        "        'web3': pd.read_csv('data/raw/reddit_analysis_web3.csv')\n",
        "    }\n",
        "\n",
        "    # Primary category gets full data\n",
        "    primary_df = datasets[primary_category]\n",
        "\n",
        "    # Sample from other categories (50% of primary size from each)\n",
        "    other_categories = [cat for cat in datasets.keys() if cat != primary_category]\n",
        "    sample_size = len(primary_df) // 2\n",
        "\n",
        "    other_dfs = []\n",
        "    for cat in other_categories:\n",
        "        sampled_df = datasets[cat].sample(n=min(sample_size, len(datasets[cat])),\n",
        "                                        random_state=42+node_number)\n",
        "        other_dfs.append(sampled_df)\n",
        "\n",
        "    # Combine datasets\n",
        "    df = pd.concat([primary_df] + other_dfs, ignore_index=True)\n",
        "\n",
        "    # Create derived features\n",
        "    df['score_log'] = np.log1p(df['score'])\n",
        "    df['comments_log'] = np.log1p(df['num_comments'])\n",
        "    df['text_word_count'] = df['cleaned_text'].fillna('').str.split().str.len()\n",
        "    df['sentiment_compound'] = (df['textblob_sentiment'] + df['transformer_score']) / 2\n",
        "\n",
        "    # Print node info\n",
        "    print(f\"Node {node_number} Dataset Overview:\")\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "    print(\"\\nCategory distribution:\")\n",
        "    print(df['category'].value_counts())\n",
        "    return df\n",
        "\n",
        "# Load data for Node 1\n",
        "node1_df = load_node_data(node_number=1, primary_category='ai')\n",
        "\n",
        "# Cell 3: Node Client Setup\n",
        "class NodeClient(fl.client.NumPyClient):\n",
        "    def __init__(self, df, node_number):\n",
        "        \"\"\"Initialize node client with dataset\"\"\"\n",
        "        self.df = df\n",
        "        self.node_number = node_number\n",
        "        self.prepare_data()\n",
        "        self.model = self.create_model()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Prepare features and labels\"\"\"\n",
        "        # Combine text features\n",
        "        text_data = self.df['title'].fillna('') + ' ' + \\\n",
        "                   self.df['cleaned_text'].fillna('')\n",
        "\n",
        "        # Create text features\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=1000,\n",
        "            stop_words='english'\n",
        "        )\n",
        "        X_text = self.vectorizer.fit_transform(text_data).toarray()\n",
        "\n",
        "        # Add numerical features\n",
        "        numerical_features = ['score_log', 'comments_log',\n",
        "                            'sentiment_compound', 'text_word_count']\n",
        "        X_numerical = self.df[numerical_features].fillna(0).values\n",
        "\n",
        "        # Combine features\n",
        "        self.X = np.hstack([X_text, X_numerical])\n",
        "\n",
        "        # Prepare labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.y = self.label_encoder.fit_transform(self.df['category'])\n",
        "\n",
        "        # Split data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
        "            train_test_split(self.X, self.y, test_size=0.2,\n",
        "                           random_state=42+self.node_number)\n",
        "\n",
        "        print(f\"\\nNode {self.node_number} Training Data:\")\n",
        "        print(f\"Training data shape: {self.X_train.shape}\")\n",
        "        print(f\"Testing data shape: {self.X_test.shape}\")\n",
        "\n",
        "    # Rest of the methods remain the same, but change Dense(1) to Dense(3)\n",
        "    # in create_model() and use categorical_crossentropy as loss function\n",
        "    def create_model(self):\n",
        "        \"\"\"Create neural network model\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu',\n",
        "                                input_shape=(self.X_train.shape[1],)),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(3, activation='softmax')  # 3 categories\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "# Cell 4: Test Node 1\n",
        "# Initialize client\n",
        "node1 = NodeClient(node1_df, node_number=1)\n",
        "\n",
        "# Test local training\n",
        "print(\"\\nTesting local training:\")\n",
        "history = node1.model.fit(\n",
        "    node1.X_train,\n",
        "    node1.y_train,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "snaLTMWxnpTE",
        "outputId": "f4d8b73f-fe32-4ff4-895c-fbebfa31417c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'decentralized-ai-content-recommender'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 50 (delta 16), reused 23 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (50/50), 1.49 MiB | 15.42 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "/content/decentralized-ai-content-recommender\n",
            "Requirement already satisfied: flwr in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: cryptography<43.0.0,>=42.0.4 in /usr/local/lib/python3.10/dist-packages (from flwr) (42.0.8)\n",
            "Requirement already satisfied: grpcio!=1.64.2,!=1.65.1,!=1.65.2,!=1.65.4,!=1.65.5,!=1.66.0,!=1.66.1,<2.0.0,>=1.60.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (1.67.1)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (0.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (1.26.4)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from flwr) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.25.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (4.25.5)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (3.21.0)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from flwr) (2.0.2)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (1.1.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.10/dist-packages (from flwr) (0.12.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0,>=42.0.4->flwr) (1.17.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.4->flwr) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Node 1 Dataset Overview:\n",
            "Total samples: 400\n",
            "\n",
            "Category distribution:\n",
            "category\n",
            "ai       200\n",
            "music    100\n",
            "web3     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Node 1 Training Data:\n",
            "Training data shape: (320, 1004)\n",
            "Testing data shape: (80, 1004)\n",
            "\n",
            "Testing local training:\n",
            "Epoch 1/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.4064 - loss: 2.6277 - val_accuracy: 0.4375 - val_loss: 6.5369\n",
            "Epoch 2/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5041 - loss: 2.8605 - val_accuracy: 0.4062 - val_loss: 1.2902\n",
            "Epoch 3/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4243 - loss: 1.9621 - val_accuracy: 0.4531 - val_loss: 4.6600\n",
            "Epoch 4/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5698 - loss: 1.8856 - val_accuracy: 0.4375 - val_loss: 2.5742\n",
            "Epoch 5/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5772 - loss: 1.3254 - val_accuracy: 0.4062 - val_loss: 2.4637\n"
          ]
        }
      ]
    }
  ]
}