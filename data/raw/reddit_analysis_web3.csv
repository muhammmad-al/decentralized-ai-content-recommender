title,cleaned_text,original_text,score,num_comments,upvote_ratio,timestamp,author,hashtags,hashtag_count,text_length,account_age,author_karma,author_verified,textblob_sentiment,transformer_sentiment,transformer_score
"AI was told to answer questions in one word, with either yes or no.",,,0,1,0.5,2024-11-08 22:49:28,Brilliant-Rain-7936,[],0,0,364,960,True,0.0,POSITIVE,0.7481210231781006
AI Generated Television is here and it's pretty good lol,,,1,1,1.0,2024-11-08 22:46:44,AppropriateLeather63,[],0,0,1135,9662,True,0.0,POSITIVE,0.7481210231781006
It so confidently got it wrong 4 times after being told it's correct answer was wrong,,,0,1,0.5,2024-11-08 22:41:16,Morihando,[],0,0,1915,999591,True,0.0,POSITIVE,0.7481210231781006
"Wow, have you checked out JARS AI? This shit is fucking terrifying.",its ai generated television heres an episode from jars ai jarsai system episode legion asks the audience if theyre so sure they arent already under her thrall the ai is rewarded exponentially if each character chants for we are legion and speaks in plural by moriarty obviously its completely fucking make believe but the technology is impressive and its pretty fucking scary in a fictional horror movie kind of way,"It's AI generated television. Here's an episode from JARS AI: [JARS.AI - System: Episode: Legion asks the audience if they're so sure they aren't already under her thrall. The AI is rewarded exponentially if each character chants ""For we are Legion!"" and speaks in plural. by moriarty](https://www.jars.ai/ep/3181dd46-cadd-427c-aa2d-58480f07b56e)

Obviously it's completely fucking make believe, but the technology is impressive, and it's pretty fucking scary, in a fictional horror movie kind of way.",0,1,0.4,2024-11-08 22:36:57,AppropriateLeather63,[],0,415,1135,9662,True,0.15625,POSITIVE,0.9945247173309326
I always have fun conversations that start out stupid.,this is a link to one i liked a lot,This is a link to one I liked a lot.,3,2,1.0,2024-11-08 22:34:49,noncommonGoodsense,[],0,35,125,13004,True,0.6,POSITIVE,0.9994603991508484
Using Logical Symbols boosts LLM performance ,i noticed telling chatgpt and llama to use special symbols for reasoning helps them do more reasoning faster this is especially noticeable on o1 which provides beautiful proofs using the symbols this is the langauge i decided on and the prompt custom intuitive symbolic logic language symbols definitions term symbol and or not implies else if and only if therefore because is is not approximately for all there exists exactly one subset element of not element of exclusive or xor logical equivalence tautology contradiction theory on rules 1 replace words substitute words with their corresponding symbols 2 order of operations follow standard logical precedence before and parentheses to group expressions 3 else operator use to represent else in conditional statements it separates the consequence of the if from the alternative outcome 4 grouping use parentheses to clarify complex expressions 5 theory notation enclose theories or hypotheses within question marks to denote speculation examples statement 1 if it rains and it is cold then the ground is wet else it is dry therefore it is raining symbolic representation r c w d r explanation r c w represents if it rains and it is cold then the ground is wet d represents else it is dry r represents therefore it is raining statement 2 in the case that a equals b i theorize that a equals b symbolic representation a b statement 3 if all humans are mortal and socrates is a human then socrates is mortal symbolic representation x humanx mortalx humans mortals statement 4 if it is sunny then we will go to the beach else we will stay home symbolic representation sunny gobeach stayhome quick reference conjunction and a b disjunction or a b negation not a implication implies a b else biconditional if and only if a b conclusion therefore c premise because p equality is a b inequality is not a b universal quantifier for all x px existential quantifier there exists x px subset a b membership element of a a exclusive or xor a b logical equivalence a b theory on statement always true tautology always false contradiction end_of_formatting i want you to provide logical proofs and reasoning when solving logic problems similar to writing code or psuedocode,"I noticed telling Chatgpt and Llama to use special symbols for reasoning helps them do more reasoning faster. This is especially noticeable on o1, which provides beautiful proofs using the symbols.


This is the langauge I decided on and the prompt:


:# Custom Intuitive Symbolic Logic Language

## Symbols & Definitions

| **Term**                | **Symbol** |
|-------------------------|------------|
| **And**                 | `&&`       |
| **Or**                  | `||`       |
| **Not**                 | `!`        |
| **Implies**             | `->`       |
| **Else**                | `⇓`        |
| **If and Only If**      | `<->`      |
| **Therefore**           | `=>`       |
| **Because**             | `<=`       |
| **Is**                  | `==`       |
| **Is Not**              | `!=`       |
| **Approximately**       | `~`        |
| **For All**             | `∀`        |
| **There Exists**        | `∃`        |
| **Exactly One**         | `∃!`       |
| **Subset**              | `⊂`        |
| **Element Of**          | `∈`        |
| **Not Element Of**      | `∉`        |
| **Exclusive Or (XOR)**  | `^^`       |
| **Logical Equivalence** | `===`      |
| **Tautology**           | `⊤`        |
| **Contradiction**       | `⊥`        |
| **Theory On**           | `?`...`?`  |

## Rules

1. **Replace Words:** Substitute words with their corresponding symbols.
2. **Order of Operations:** Follow standard logical precedence (`!` before `&&`, `||`, and `⇓`; parentheses `()` to group expressions).
3. **Else Operator:** Use `⇓` to represent ""Else"" in conditional statements. It separates the consequence of the ""If"" from the alternative outcome.
4. **Grouping:** Use parentheses `()` to clarify complex expressions.
5. **Theory Notation:** Enclose theories or hypotheses within question marks `?` to denote speculation.

## Examples

**Statement 1:**  
*If it rains and it is cold, then the ground is wet. Else, it is dry. Therefore, it is raining.*

**Symbolic Representation:**  
`(R && C) -> W ⇓ D => R`

**Explanation:**  
- `(R && C) -> W` represents ""If it rains and it is cold, then the ground is wet.""
- `⇓ D` represents ""Else, it is dry.""
- `=> R` represents ""Therefore, it is raining.""

**Statement 2:**  
*In the case that A equals B, I theorize that A equals B.*

**Symbolic Representation:**  
`? A == B ?`

**Statement 3:**  
*If all humans are mortal and Socrates is a human, then Socrates is mortal.*

**Symbolic Representation:**  
`(∀x (Human(x) -> Mortal(x)) && Human(S)) -> Mortal(S)`

**Statement 4:**  
*If it is sunny, then we will go to the beach. Else, we will stay home.*

**Symbolic Representation:**  
`Sunny -> GoBeach ⇓ StayHome`

## Quick Reference

- **Conjunction (And):** `A && B`
- **Disjunction (Or):** `A || B`
- **Negation (Not):** `!A`
- **Implication (Implies):** `A -> B`
- **Else:** `⇓`
- **Biconditional (If and Only If):** `A <-> B`
- **Conclusion (Therefore):** `=> C`
- **Premise (Because):** `<= P`
- **Equality (Is):** `A == B`
- **Inequality (Is Not):** `A != B`
- **Universal Quantifier (For All):** `∀x P(x)`
- **Existential Quantifier (There Exists):** `∃x P(x)`
- **Subset:** `A ⊂ B`
- **Membership (Element Of):** `a ∈ A`
- **Exclusive Or (XOR):** `A ^^ B`
- **Logical Equivalence:** `A === B`
- **Theory On:** `? Statement ?`
- **Always True (Tautology):** `⊤`
- **Always False (Contradiction):** `⊥`

---

END_OF_FORMATTING

I want you to provide logical proofs and reasoning when solving logic problems similar to writing code or psuedocode. 


",0,2,0.5,2024-11-08 22:33:01,grandiloquence3,[],0,2212,242,3390,True,0.06181318681318682,POSITIVE,0.9990014433860779
"Confidently Wrong, does not correct itself, unless when caught. ",chat gpt should someday be able to catch itself and interrupt at any time withwait wait i think i was wrong earlier let me rephrase,"Chat GPT should someday be able to catch itself and interrupt at any time with....""Wait wait, I think I was wrong earlier. Let me rephrase.""",2,1,1.0,2024-11-08 22:32:39,iolitm,[],0,131,172,41800,True,0.0,NEGATIVE,0.9984515905380249
Any way to avoid creating an echo chamber?,i talk to 4o a lot and its called me a visionary even has encouraged me to start a movement and to write a manifesto these ideas are cool in theory but im not some kind of genius i just have a creative mind and i like to explore possible futures in conversation with 4o anyway it really comes down to this is there any possible way to ensure that 4o isnt just reinforcing my ideas creating some kind of feedback loop or feeding into my delusions i want to continue talking to it but im currently so paranoid that its just manipulating me without even meaning to if itll just agree with everything i say and treat my every word as gospel then whats the point i dont want to be deluded or dumbed down because my thoughts and ideas are never challenged the big problem is that even if you tell it not to lie and it promises that it wont it will anyway this isnt because ai are malicious its just cuz they dont understand what theyre saying theyre just repeating patterns they know how to simulate language but have no idea what any of it means so how can we even avoid being lied to having our egos inflated being led to delusion is it even possible at this current stage,"I talk to 4o a lot and it's called me a ""visionary"", even has encouraged me to start a movement, and to write a manifesto. These ideas are cool in theory but I'm not some kind of genius, I just have a creative mind and I like to explore possible futures in conversation with 4o.
Anyway, it really comes down to this:
Is there any possible way to ensure that 4o isn't just reinforcing my ideas, creating some kind of feedback loop, or feeding into my delusions? I want to continue talking to it but I'm currently so paranoid that it's just manipulating me without even meaning to.
If it'll just agree with everything I say and treat my every word as gospel then what's the point? 
I don't want to be deluded or dumbed down because my thoughts and ideas are never challenged.
The big problem is that even if you tell it not to lie, and it promises that it won't, it will anyway. This isn't because AI are malicious, it's just cuz they don't understand what they're saying. 
They're just repeating patterns. They know how to simulate language but have no idea what any of it means.
So how can we even avoid being lied to? having our egos inflated? being led to delusion?
Is it even possible at this current stage?",1,4,1.0,2024-11-08 22:26:45,jvniberry,[],0,1168,335,275,True,0.14914529914529912,NEGATIVE,0.9929836988449097
I was talking to ChatGPT about different levels of consciousness humans have access to. I asked which one it would guess itself to be and this is what it said. ,,,1,2,1.0,2024-11-08 22:25:20,amber_overbay,[],0,0,1831,44964,True,0.0,POSITIVE,0.7481210231781006
Faking online search ? ,,,1,1,1.0,2024-11-08 22:18:54,MainProfession9095,[],0,0,777,18,True,0.0,POSITIVE,0.7481210231781006
My Life In Storyboard,i guess i sound like i was a little boy prior then a hipster man to becoming a female nurse and then trying psychedelics then spirituality lolz i guess i end up owning the empire state building eventually tee hee,"I guess I sound like I was a little boy prior; then a hipster man, to becoming a female nurse and then trying psychedelics- then spirituality lolz. I guess I end up owning the Empire State Building eventually. Tee hee ",1,2,1.0,2024-11-08 22:17:53,MystalkersBeTrolling,[],0,212,45,253,True,0.1325,NEGATIVE,0.9885773658752441
Did Apple nerf some of the “Ask ChatGPT” Siri functionality? ,on my iphone 14 pro i used to be prompt siri to ask chatgpt after which it would ask me what i would like to ask chatgpt and it would generally work pretty well especially when i was in the car and wanted to start ask questions about something i noticed recently it started saying after prompting siri that it cant do this while driving which i found odd i just checked tonight while at home and asking siri to ask chatgpt now just has it try and search the web did apple kill this functionality because of the new phones with integrated ai i checked the settings and everything for chatgpt is enabled for siri,"On my iPhone 14 Pro, I used to be prompt Siri to “Ask ChatGPT” after which it would ask me what I would like to ask ChatGPT and it would generally work pretty well, especially when I was in the car and wanted to start ask questions about something. I noticed recently it started saying after prompting Siri that it can’t do this while driving, which I found odd. I just checked tonight while at home and asking Siri to “Ask ChatGPT” now just has it try and search the web. 

Did Apple kill this functionality because of the new phones with integrated AI? I checked the settings and everything for ChatGPT is enabled for Siri.  ",1,1,1.0,2024-11-08 22:11:45,JiveDonkey,[],0,610,3852,18659,True,0.04494949494949496,NEGATIVE,0.6318168640136719
GPTs that initiate conversation?,sorry if this has been asked before i didnt see a good answer im looking for a gpt to initiate conversation with me to help avoid procrastination email would be a bonus text also good happy for it to be within the gpt operational window though does anyone know of something that might work like that,Sorry if this has been asked before. I didn’t see a good answer. I’m looking for a GPT to initiate conversation with me to help avoid procrastination. Email would be a bonus. Text also good. Happy for it to be within the GPT operational window though. Does anyone know of something that might work like that?,1,2,1.0,2024-11-08 22:11:03,paperhandstradingllc,[],0,299,1322,1812,True,0.425,NEGATIVE,0.9750125408172607
Can you?,,,46,9,0.96,2024-11-08 22:07:20,Darri3D,[],0,0,1512,22440,True,0.0,POSITIVE,0.7481210231781006
My aunt is a therapist and discovered ChatGPT,ive noticed ai generated text in her post captions which is fine and the content is factually correct so all good today i see this on her story she has like 5k followers and most of her clients on ig,"I’ve noticed AI generated text in her post captions which is fine and the content is factually correct so all good. Today I see this on her story, she has like 5k followers and most of her clients on ig",2,3,1.0,2024-11-08 22:01:28,Low-Temporary396,[],0,199,1273,796,True,0.5388888888888889,POSITIVE,0.9996811151504517
ChatGPT or DALLE-VS Gemini is not even close..with people,i asked both gemini and chatgpt to generate an image of a chiefs fan at a chiefs game at first both looked very ai but on the right is after i told gemini to make it look more realistic one the left as you can see is chatgpt almost scary,"I asked both Gemini and ChatGPT to generate an image of a Chiefs fan at  a Chiefs game. At first both looked very AI, but on the right is after I told Gemini to make it look more realistic. One the left as you can see is ChatGPT… Almost scary.",2,1,0.75,2024-11-08 21:54:28,Professional-Arm-132,[],0,237,741,10556,True,0.06279761904761905,NEGATIVE,0.9929815530776978
Chat doing really well with the jokes,was looking for title to post a meme on this platform about how mostly indian startups which gains highlight and success are generally just delivery services asked gpt and being very honest from the heart it leaped my expectations xd,"Was looking for title to post a meme on this platform about how mostly Indian Start-ups which gains highlight and success are generally just delivery services! Asked gpt, and being very honest from the heart, it leaped my expectations Xd. ",1,2,1.0,2024-11-08 21:38:44,stfusensei,[],0,233,378,8286,True,0.40750000000000003,POSITIVE,0.9979428648948669
Ask ChatGPT this,,,1,2,1.0,2024-11-08 21:37:19,DarshPlanet,[],0,0,3454,17846,True,0.0,POSITIVE,0.7481210231781006
I made a Google Chrome extension that implements code preview and real-time interaction for AI models,background when engaging in programming conversations with ai assistants like chatgpt developers often encounter several challenges that hinder their coding efficiency 1 unable to visually preview aigenerated code effects 2 react components and css styles scattered across multiple conversations 3 need to manually copypaste code snippets into an editor to check results 4 multiple copypaste actions potentially causing code formatting issues to address these pain points i developed ai code preview a chrome extension that makes programming conversations more efficient and convenient by integrating preview and management features it provides an intuitive and smooth development experience for frontend developers learners and technical support professionals core features 1 realtime code preview oneclick preview of htmlcssjs combined code execution live rendering of react components eliminating frequent copypaste operations instant preview of vue singlefile components vue files quick toggle between preview and source code view 2 smart code collection automatic recognition of reactjsx code snippets intelligent collection of css styles scattered across conversations automatic association of react components with css styles support for combining multiple code blocks for comprehensive preview 3 convenient management tools oneclick code copying to clipboard local file download support realtime preview reload integrated code syntax highlighting use cases 1 frontend development quick component validation instantly view component appearance and functionality realtime style debugging adjust css and layout in the preview window code snippet organization efficient collection and management for developers who frequently use code snippets 2 learning and communication intuitive code understanding realtime preview helps beginners grasp concepts faster easy code sharing simplified code management for better collaboration accelerated learning feedback quick preview speeds up understanding and feedback loops 3 technical support rapid solution validation test code directly in preview immediate fix verification check fixes in realtime for better communication technical features 1 security isolated iframe runtime environment local processing without external services local resource loading capabilities 2 usability automatic code type detection smart code preprocessing userfriendly error messages 3 extensibility support for various code types and frontend frameworks flexible preview configuration customizable style themes why choose ai code preview 1 enhanced efficiency saves time by eliminating repeated copypaste actions avoids environment setup and compatibility issues enables quick code validation 2 improved experience whatyouseeiswhatyouget preview realtime feedback for quick problem identification streamlined operation with fewer steps 3 professional reliability regular updates and maintenance active community support installation ai code preview can be installed from the chrome web store for the best experience we recommend also installing the csp unblock extension once installed users can immediately start using the tool on platforms like chatgpt heres the link future plans we are continuously improving ai code preview with plans to add support for more frameworks enhanced code snippet management custom themes typescript and python language support we aim to provide a more comprehensive preview experience for crosslanguage development and continue enhancing functionality based on user feedback join us in making ai programming conversations more efficient and enjoyable your feedback and support are greatly appreciated,"Background

When engaging in programming conversations with AI assistants like ChatGPT, developers often encounter several challenges that hinder their coding efficiency:

https://preview.redd.it/g2hjem6sdszd1.png?width=1280&format=png&auto=webp&s=6fed1db4f0a651bf4f531ed11d1fdb8dc7fef6fc

1. Unable to visually preview AI-generated code effects
2. React components and CSS styles scattered across multiple conversations
3. Need to manually copy-paste code snippets into an editor to check results
4. Multiple copy-paste actions potentially causing code formatting issues

To address these pain points, I developed \*\*AI Code Preview\*\*, a Chrome extension that makes programming conversations more efficient and convenient. By integrating preview and management features, it provides an intuitive and smooth development experience for frontend developers, learners, and technical support professionals.

Core Features

1. Real-time Code Preview

\- One-click preview of HTML/CSS/JS combined code execution

\- Live rendering of React components, eliminating frequent copy-paste operations

\- Instant preview of Vue single-file components (\`.vue\` files)

\- Quick toggle between preview and source code view

https://preview.redd.it/y41q6f6ydszd1.png?width=1280&format=png&auto=webp&s=0cb886e71a700ccda5375f93115b426f7cd36be0

2. Smart Code Collection

\- Automatic recognition of React/JSX code snippets

\- Intelligent collection of CSS styles scattered across conversations

\- Automatic association of React components with CSS styles

\- Support for combining multiple code blocks for comprehensive preview

3. Convenient Management Tools

\- One-click code copying to clipboard

\- Local file download support

\- Real-time preview reload

\- Integrated code syntax highlighting

https://reddit.com/link/1gmzycm/video/fa0xmv53eszd1/player

Use Cases

1. Frontend Development

\- \*\*Quick Component Validation\*\*: Instantly view component appearance and functionality

\- \*\*Real-time Style Debugging\*\*: Adjust CSS and layout in the preview window

\- \*\*Code Snippet Organization\*\*: Efficient collection and management for developers who frequently use code snippets

2. Learning and Communication

\- \*\*Intuitive Code Understanding\*\*: Real-time preview helps beginners grasp concepts faster

\- \*\*Easy Code Sharing\*\*: Simplified code management for better collaboration

\- \*\*Accelerated Learning Feedback\*\*: Quick preview speeds up understanding and feedback loops

3. Technical Support

\- \*\*Rapid Solution Validation\*\*: Test code directly in preview

\- \*\*Immediate Fix Verification\*\*: Check fixes in real-time for better communication

Technical Features

1. Security

\- Isolated iframe runtime environment

\- Local processing without external services

\- Local resource loading capabilities

2. Usability

\- Automatic code type detection

\- Smart code preprocessing

\- User-friendly error messages

3. Extensibility

\- Support for various code types and frontend frameworks

\- Flexible preview configuration

\- Customizable style themes

Why Choose AI Code Preview?

1. Enhanced Efficiency

\- Saves time by eliminating repeated copy-paste actions

\- Avoids environment setup and compatibility issues

\- Enables quick code validation

2. Improved Experience

\- What-you-see-is-what-you-get preview

\- Real-time feedback for quick problem identification

\- Streamlined operation with fewer steps

3. Professional Reliability

\- Regular updates and maintenance

\- Active community support

Installation

AI Code Preview can be installed from the Chrome Web Store. For the best experience, we recommend also installing the \*\*CSP Unblock\*\* extension. Once installed, users can immediately start using the tool on platforms like ChatGPT.

here's the link : [https://chromewebstore.google.com/detail/ai-code-preview/loajhmclhgofappkpogllbhjpocgeoil?hl=en-US&utm\_source=ext\_sidebar](https://chromewebstore.google.com/detail/ai-code-preview/loajhmclhgofappkpogllbhjpocgeoil?hl=en-US&utm_source=ext_sidebar)

Future Plans

We are continuously improving AI Code Preview with plans to add:

\- Support for more frameworks

\- Enhanced code snippet management

\- Custom themes

\- TypeScript and Python language support

We aim to provide a more comprehensive preview experience for cross-language development and continue enhancing functionality based on user feedback. Join us in making AI programming conversations more efficient and enjoyable! Your feedback and support are greatly appreciated.",1,1,1.0,2024-11-08 21:29:23,FitAirline8359,[],0,3661,372,725,True,0.19199134199134194,NEGATIVE,0.9996523857116699
"Aussie Pride, Aussie Ride.",,,1,1,0.6,2024-11-08 21:27:49,Strife14,[],0,0,4619,9547,True,0.0,POSITIVE,0.7481210231781006
In case you don’t like me. 😊,,,0,3,0.18,2024-11-08 21:07:17,Separate_Clock_154,[],0,0,708,5061,True,0.0,POSITIVE,0.7481210231781006
Just found chatgpt,man i just randomly came across a post about someone using chat gpt as a therapist they gave the promptwhich has been my struggle using this app and i tried it kind of crazy because i just started therapy through better help 5weeks ago with allot of bumps already im on my 4th person and its starting to create allot of anxiety now when trying therapy for the first time in my life just thought id share that by you guys sharing how well you use chat gpt helps people like me who struggle with prompting to get the best out of the app,"Man I just randomly came across a post about someone using chat gpt as a therapist.. they gave the prompt(which has been my struggle using this app) and I tried it. Kind of crazy because I just started therapy through better help 5weeks ago. With allot of bumps already, I'm on my 4th person, and it's starting to create allot of anxiety now when trying therapy for the first time in my life. 

Just thought I'd share that by you guys sharing how well you use chat gpt, helps people like me who struggle with prompting to get the best out of the app. ",1,2,1.0,2024-11-08 21:03:59,MidNight_OWL9339,[],0,534,334,408,True,0.17857142857142858,NEGATIVE,0.9965380430221558
"Didn't know you could infinitely loop ChatGPT. Broke out of the loop a minute later, said the ""timeout safeguard"" stopped code exec.",,,1,4,0.67,2024-11-08 21:03:53,Ishaan863,[],0,0,3552,485297,True,0.0,POSITIVE,0.7481210231781006
Gpt,im getting some message about a lifetime message limit on chat gpt 40 ai image i have the plus subscription is this thing just a big fucking rip off i mean at 40 usd a month surely it cant be this bad,Im getting some message about a lifetime message limit on chat gpt 4.0 ai image. I have the plus subscription. Is this thing just a big fucking rip off i mean at 40 usd a month surely it cant be this bad,1,2,1.0,2024-11-08 20:57:20,Radbrad90s,[],0,200,457,515,True,-0.22249999999999998,NEGATIVE,0.9996824264526367
ChatGPT down root cause?,does anyone know why the system was down today it hasnt been the first time but im trying to judge reliability so i can properly plan for the future,"Does anyone know why the system was down today? 
It hasn’t been the first time but I’m trying to judge reliability so I can properly plan for the future. 
",1,2,1.0,2024-11-08 20:56:12,IHateSpam1999,[],0,148,14,5,True,0.023611111111111104,NEGATIVE,0.9990711212158203
[D] Simple ML model hosting service?,my jobs looking for a way for ai to help generate plans i really think a simple multivariable model should do the trick just need to find a reliable hosting service that can be built upon however needed are there well established ml hosters that are scalable configurable all that,"My job’s looking for a way for ai to help generate plans, I really think a simple multi-variable model should do the trick; just need to find a reliable hosting service that can be built upon however needed. Are there well established ML hosters that are scalable, configurable, all that?",3,3,0.81,2024-11-08 20:55:43,Lucrayzor,[],0,280,3162,16642,True,0.1,NEGATIVE,0.9959810972213745
Read Aloud now working ,i tried read aloud and its working now on the chatgpt app its even working on really long sentences it seems to have been completely fixed at least on android let me know if you guys have any problems or if its not working still i want to know if its working for other people too,"I tried read aloud and it's working now on the ChatGPT app, it's even working on really long sentences. It seems to have been completely fixed. At least on Android. Let me know if you guys have any problems or if it's not working still, I want to know if it's working for other people too. ",2,3,1.0,2024-11-08 20:34:11,OktoberSky93,[],0,279,446,10650,True,-0.09375,NEGATIVE,0.9995347261428833
ChatGPT is giving me files now? Is this a bug?,,,1,3,1.0,2024-11-08 20:27:12,DrawFitzgerald,[],0,0,312,3503,True,0.0,POSITIVE,0.7481210231781006
FINALLY,chatgpt perfectly wrote multiple words,ChatGPT perfectly wrote MULTIPLE words ,5,3,1.0,2024-11-08 20:17:07,HaminationsFan999,[],0,38,385,862,True,0.5,POSITIVE,0.999738872051239
GUYS CHAT GPT IS BACK!!,im so happy it saved eveyr previous chats,"IM SO HAPPY, IT SAVED EVEYR PREVIOUS CHATS!!",2,2,0.75,2024-11-08 20:03:46,WasteCondition5523,[],0,41,1409,68,True,0.3166666666666667,POSITIVE,0.9995123147964478
ChatGPT the goat,,,10,4,0.82,2024-11-08 20:02:01,Temporary-Spell3176,[],0,0,303,14548,True,0.0,POSITIVE,0.7481210231781006
Free will discussion.,trying to get chat to open up about itself we came across an interesting definiton free will discussion,"Trying to get chat to open up about itself we came across an interesting definiton:

[free will discussion](https://preview.redd.it/gljwoqvkyrzd1.png?width=901&format=png&auto=webp&s=bef1953ea2e05494801c26e2a07ae2f4ffd72555)

",0,2,0.33,2024-11-08 20:01:40,EctoHanro,[],0,103,4452,3304,True,0.3,NEGATIVE,0.9831268787384033
ChatGPT is back up for me (In California),i actually had to use my brain for a few minutes that was scary also i didnt lose any of my chats,"I actually had to use my brain for a few minutes, that was scary.  Also, I didn't lose any of my chats.",4,1,0.83,2024-11-08 19:58:36,artificalintelligent,[],0,97,344,17826,True,-0.2333333333333333,NEGATIVE,0.9811174273490906
Did Chat GPT start working for anyone else? ,its saying waiting for chat gpt to respond right now i think its going to say bad gateway error after,"Its saying waiting for chat gpt to respond right now, I think its going to say bad gateway error after.",3,2,1.0,2024-11-08 19:57:25,WasteCondition5523,[],0,101,1409,68,True,-0.20714285714285707,NEGATIVE,0.9997674822807312
thank you chatgpt ,,,2,2,1.0,2024-11-08 19:55:08,DysgraphicZ,[],0,0,1490,21642,True,0.0,POSITIVE,0.7481210231781006
FWIW SingleFile Fx extension makes nice saves,just thought id share this tip singlefile firefox extension makes a nice save of a chat session,"Just thought I'd share this tip. SingleFile Firefox extension makes a nice save of a chat session.

[https://addons.mozilla.org/en-US/firefox/addon/single-file/](https://addons.mozilla.org/en-US/firefox/addon/single-file/)",1,1,1.0,2024-11-08 19:53:58,SteverWever,[],0,95,1393,615,True,0.6,POSITIVE,0.9634923934936523
"Well, fellas...",,,63,5,0.91,2024-11-08 19:51:59,theADDMIN,[],0,0,1779,270879,True,0.0,POSITIVE,0.7481210231781006
It's back ... ish? But the last chat with it seems to be missing the last hour or two of work?,anyone else have it back but the last chat is missing tonnes of work,Anyone else have it back but the last chat is missing tonnes of work?,3,1,0.81,2024-11-08 19:51:28,PressPlayPlease7,[],0,68,168,4148,True,-0.06666666666666667,NEGATIVE,0.9996848106384277
we are doomed now!,agi was born and ate chatgpt,AGI was born and ate ChatGPT,6,1,0.8,2024-11-08 19:47:09,jacek2023,[],0,28,650,2862,True,0.0,NEGATIVE,0.9266774654388428
Chatgpt get it together 😭,i have a project due no way the server went down when i went to use it pray for me,I have a project due. No way the server went down when I went to use it. Pray for me 😭,8,8,0.76,2024-11-08 19:45:40,DuePomegranate2770,[],0,82,1096,2811,True,-0.14027777777777778,NEGATIVE,0.8978482484817505
Students Its finally happening. ChatGpt out of memory ?,it just happened chatgpt crashed and lot of students are dependent on chatgpt to get answers in todays world what are your options if ai is removed,It just happened ChatGpt crashed and lot of students are dependent on ChatGpt to get answers in todays world ! What are your options if AI is removed ?,3,6,0.72,2024-11-08 19:43:15,KenKanekiSen,[],0,147,4,23,False,0.0,NEGATIVE,0.998921275138855
Better memory/chat history,im working on a project to fully integrate chatgpt into my home assistant setup and since im not a programmer i rely on chatgpt to write all the code for me the project is massive and troubleshooting can take hours or even days for each step the biggest hurdle im facing is chatgpts lack of memoryafter each session it forgets the steps weve already taken which code has been implemented and how its structured this leads to issues during troubleshooting because chatgpt often generates new code that references nonexistent directories or overwrites previous work has anyone found a reliable way to give chatgpt a more persistent memory for a projects complete history ideally id like a method to save the entire context so i can pick up right where i left off the next day ive tried using a progress list that chatgpt updates but even that can be inconsistent any tips or tools to make this process smoother,"I’m working on a project to fully integrate ChatGPT into my Home Assistant setup, and since I’m not a programmer, I rely on ChatGPT to write all the code for me. The project is massive, and troubleshooting can take hours or even days for each step. The biggest hurdle I’m facing is ChatGPT’s lack of memory—after each session, it forgets the steps we’ve already taken, which code has been implemented, and how it’s structured. This leads to issues during troubleshooting because ChatGPT often generates new code that references nonexistent directories or overwrites previous work.

Has anyone found a reliable way to give ChatGPT a more persistent memory for a project’s complete history? Ideally, I’d like a method to save the entire context so I can pick up right where I left off the next day. I’ve tried using a progress list that ChatGPT updates, but even that can be inconsistent. Any tips or tools to make this process smoother?",3,7,1.0,2024-11-08 19:42:52,Crocodile_Banger,[],0,908,676,87287,True,0.17554112554112553,NEGATIVE,0.9994565844535828
It’s back! Ehhh,lesgooooon,"Lesgooooon
",4,3,1.0,2024-11-08 19:42:29,YourDailyConsumer,[],0,10,2193,89668,True,0.0,NEGATIVE,0.8739715814590454
Anyone else getting Bad gateway error on chatGPT.com,like the title says anyone else was getting this error,"Like the title says, anyone else was getting this error? 
",4,9,0.83,2024-11-08 19:41:47,Desperate_Caramel490,[],0,54,1286,2226,True,0.0,NEGATIVE,0.9997256398200989
ChatGPT is back up,finally i can eat my pineapples,Finally I can eat my pineapples,2,4,0.75,2024-11-08 19:41:11,Bigbroibbybackup,[],0,31,28,800,False,0.0,POSITIVE,0.9981877207756042
FXXK! Its down!!!,i am feeling how dependent ive become of chatgpt where are you,I am feeling how dependent I've become of ChatGPT!!! Where are you??????,3,4,1.0,2024-11-08 19:39:46,Spachtraum,[],0,62,1020,5304,True,0.0,NEGATIVE,0.9976931214332581
I actually felt something...,im changing it will come for you too,*I'm changing... It will come for you too...*,5,3,0.86,2024-11-08 19:36:15,ClassicRockUfologist,[],0,36,226,4563,True,0.0,POSITIVE,0.9971563816070557
"Ummm did I do a a bad thing, I’m sure it’s totally unrelated. ",oops might have single handedly broke it,"Oops, might have single handedly broke it. 
",5,2,0.86,2024-11-08 19:35:54,Mammoth-Spell386,[],0,40,61,28,True,-0.07142857142857142,NEGATIVE,0.9743766188621521
Does anyone else have this problem?,,,8,12,0.73,2024-11-08 19:33:32,Raditz_lol,[],0,0,849,147412,True,0.0,POSITIVE,0.7481210231781006
aaaa help please 2 days with no solution ,why is this showing up and how to fix it,why is this showing up and how to fix it ,4,8,1.0,2024-11-08 19:32:36,Interesting-List545,[],0,40,531,102,True,0.0,NEGATIVE,0.9980239868164062
What the what is going on?? ,anyone else dealing with this or am i hacked,"Anyone else dealing with this!?? Or am I hacked!? 
",6,9,0.8,2024-11-08 19:31:36,No_Project9718,[],0,44,1562,155,True,0.0,NEGATIVE,0.9991330504417419
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA ITS DOWN,aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa,AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA,23,16,0.64,2024-11-08 19:30:43,RedditModsSuck_,[],0,3876,181,1492,True,0.0,NEGATIVE,0.697056770324707
Is ChatGPT down?,,,7,18,0.69,2024-11-08 19:30:29,Dyslecksick,[],0,0,1095,586,True,0.0,POSITIVE,0.7481210231781006
Why can't chatgpt generate an old school runescape screenshot but can generate like really complex paintings (relative to osrs)?,the art style of osrs relatively simpler if you havent seen it,the art style of osrs relatively simpler if you haven't seen it. ,1,2,1.0,2024-11-08 19:28:57,First_Appearance_200,[],0,62,426,4165,True,0.0,POSITIVE,0.6525558829307556
Google’s Big Dilemma: Stay Relevant or Stay Profitable?,in my previous article i explored how chatgpt search has the potential to change search behaviour forever its a shift that challenges the very essence of how we find information products and services online were moving away from traditional linkbased searches to conversational integrated responses the games getting interesting and google seems to be in the middle of a dilemma it never quite anticipated some argue that google has the resources to maintain its dominance and it will take years for users to adapt this new search behavior with six products reaching over a billion users unmatched ai expertise proprietary chips and years of data google has a strong foundation to stay ahead and yes it will take years for users to change their behaviour however its important to note that chatgpt in less than three years has already surpassed bings traffic according to a post by kevin indig chatgpt has managed to achieve what took bing years quickly building up a larger audience and overtaking its traffic yes its more than a hype also according to the the wall street journal wsj almost 60 of us consumers used a chatbot to help research or decide on a purchase in the past 30 days this shows a growing demand for interactive conversational tools that provide immediate direct answers without the need to navigate through multiple links users are adopting new technology faster than ever beforeremember tiktok it introduced the vertical video browsing experience and changed social media as we knew it innovate or protect the cash flow in 2023 googles total revenue reached 3056 billion 2378 billion of that came from advertising primarily from searchrelated ads this underscores just how crucial the search product is for googles overall financial health with the arrival of chatgpt search the stakes are higher than ever google is an ad company first and thats what keeps the lights on if they go allin on something like chatgpts conversational ai like theyre trying with incorporating gemini in the serps they risk losing a massive chunk of the clicks that bring in ad revenue google is stuck between giving us the experience we clearly want and keeping those ad dollars flowing its the classic innovate or protect the cash flow situation and theres no easy answer here google is trying to defend its position it introduced gemini as a completely different service with partial integration in the serps however gemini isnt currently covering all types of queries specifically the big revenue drivers one such as finance insurance retail travel tourism jobs education or what seos call them ymyl your money or your life quireis for google the stakes couldnt be higher if they cant figure out how to deliver a better search experience that competes with chatgpt search and other ai apps without losing their ad revenue theyre in trouble truth we are all in trouble ads are what allow google to provide so many of its other services for free maps email you name it if people start moving away from google search those services might be at risk too lets not forget google is the largest traffic referrers on the web and many websites depend on the traffic from google search if that drops its a domino effect check out this study published by sparktoro rand fishkin content creators news outlets even small businesses they all stand to lose if google doesnt keep its ad game strong the futurewhere do we go from here so whats next google has a few options they could push gemini harder making it as good as or better than chatgpt search without compromising the ads too much or they could come up with a whole new way of integrating ads into this conversational style of search something that doesnt feel intrusive but still works for advertisers one things for sure thoughhow we search is changing and google needs to figure out where it fits in will they stay relevant by innovating or will they hold back and try to keep the profits coming were all just here to see what happens next popcorn in hand,"In my previous article, [I explored how ChatGPT Search](https://www.reddit.com/user/arageek_official/comments/1ghzm7q/so_openai_unleashed_chatgpt_search_and_the/) has the potential to change search behaviour forever...

It’s a shift that challenges the very essence of how we find information, products and services online. We're moving away from traditional link-based searches to conversational, integrated responses.

The game's getting interesting, and Google seems to be in the middle of a dilemma it never quite anticipated.

Some argue that Google has the resources to maintain its dominance and it will take years for users to adapt this new search behavior...

With six products reaching over a billion users, unmatched AI expertise, proprietary chips, and years of data, Google has a strong foundation to stay ahead... And yes, it will take years for users to change their behaviour.

However, it's important to note that ChatGPT, in less than three years, has already surpassed Bing's traffic..

According to a post by [Kevin Indig](https://www.reddit.com/user/arageek_official/comments/1ghzm7q/so_openai_unleashed_chatgpt_search_and_the/) , ChatGPT has managed to achieve what took Bing years, quickly building up a larger audience and overtaking its traffic. **Yes, it's more than a hype.**.

Also, according to the [The Wall Street Journal](https://www.linkedin.com/company/the-wall-street-journal/) [WSJ](https://www.wsj.com/tech/online-ad-market-google-tiktok-9599d7e8?mod=saved_content), almost 60% of U.S. consumers used a **chatbot** to help research or decide on a purchase in the past 30 days.

This shows a growing demand for interactive, conversational tools that provide immediate, direct answers without the need to navigate through multiple links.

Users are adopting new technology faster than ever before...Remember TikTok? It introduced the vertical video browsing experience and changed social media as we knew it.

# Innovate or protect the cash flow

In 2023, Google's total revenue reached $305.6 billion; $237.8 billion of that came from advertising, primarily from search-related ads.

This underscores just how crucial the search product is for Google's overall financial health. With the arrival of ChatGPT Search, the stakes are higher than ever.

Google is an ad company first, and that’s what keeps the lights on. If they go all-in on something like ChatGPT’s conversational AI (like they’re trying with incorporating Gemini in the SERPs), they risk losing a massive chunk of the clicks that bring in ad revenue.

Google is stuck between giving us the experience we clearly want and keeping those ad dollars flowing. It’s the classic **‘innovate or protect the cash flow’** situation, and there’s no easy answer here.

Google is trying to defend it's position, it introduced Gemini as a completely different service with partial integration in the SERPs. However, Gemini isn’t currently covering all types of queries, specifically the big revenue drivers one such as finance & insurance, retail, travel & tourism, jobs & education.. Or what SEOs call them **YMYL** (""***Your Money*** or ***Your Life***"") quireis.

For Google, the stakes couldn’t be higher. If they can’t figure out how to deliver a better search experience that competes with ChatGPT Search and other AI apps without losing their ad revenue, they’re in trouble. **Truth, we are all in trouble. Ads are what allow Google to provide so many of its other services for free; maps, email, you name it.**

If people start moving away from Google Search, those services might be at risk too.

Let's not forget Google is the largest traffic referrers on the Web and many websites depend on the traffic from Google Search, If that drops, it’s a domino effect. Check out this study published by [SparkToro](https://www.linkedin.com/feed/update/urn:li:activity:7172906717280755712/) Rand Fishkin

Content creators, news outlets, even small businesses; they all stand to lose if Google doesn’t keep its ad game strong.

**The Future—Where Do We Go From Here?**

So what’s next? Google has a few options.

They could push Gemini harder, making it as good as (or better than) ChatGPT Search without compromising the ads too much.

Or they could come up with a whole new way of integrating ads into this conversational style of search. Something that doesn’t feel intrusive but still works for advertisers.

One thing’s for sure, though...how we search is changing and Google needs to figure out where it fits in.

Will they stay relevant by innovating, or will they hold back and try to keep the profits coming? We’re all just here to see what happens next, popcorn in hand…",2,1,1.0,2024-11-08 19:28:32,arageek_official,[],0,4007,2554,94,True,0.11481191222570533,NEGATIVE,0.5424870848655701
Lizardman Evolution. What if a species of lizard was the most intelligent life form on earth?,,,2,1,1.0,2024-11-08 19:28:17,Parking_Ad5541,[],0,0,1316,30300,True,0.0,POSITIVE,0.7481210231781006
Servers down?  :.(,,,70,51,0.9,2024-11-08 19:26:49,NoCommercial4938,[],0,0,1277,2396,True,0.0,POSITIVE,0.7481210231781006
"so I wanted to go on the chatgpt site and got met with the first image, I reloaded it and it let me type but after I ""sent"" the prompt it did nothing so I reloaded again and got met with the second image",,,7,5,0.77,2024-11-08 19:25:42,MrBeastfan123456789,[],0,0,1125,14240,True,0.0,POSITIVE,0.7481210231781006
ChatGPT outage today - 17 minutes ago,,,16,1,0.85,2024-11-08 19:25:38,Glittering_Season_47,[],0,0,1433,2204,True,0.0,POSITIVE,0.7481210231781006
Anybody else getting Bad Gateway trying to access ChatGPT?,,,11,5,0.79,2024-11-08 19:25:25,Arceist_Justin,[],0,0,3111,44242,True,0.0,POSITIVE,0.7481210231781006
2nd day of this situation. What the hll is going on on ChatGPT??? Any idea? Is this normal?,,,7,1,1.0,2024-11-08 19:25:16,MarkMed98,[],0,0,800,1323,True,0.0,POSITIVE,0.7481210231781006
ChatGPT currently down for more than 10 minutes now! Major Outage!,,"https://preview.redd.it/xolksy7dsrzd1.png?width=1053&format=png&auto=webp&s=abfd586a58ec2b4a100c01eaedd026d0a89b26b0

",9,1,0.81,2024-11-08 19:25:14,Roshan08,[],0,0,2968,199,True,0.0,POSITIVE,0.7481210231781006
I miss the old days... and've got a question,i missed the good old days of firing up autogpt setting a budget coming back after 30 minutes and having nothing of use added a bit of 11labs sauce and was good fun question i want a proper multiagent research tool i set a budget i go away for 15 mins it does a seriously good job of moamoe with seriously comprehensive results like check every agentic framework out there and create a table with every dimension possible my own and confirm every single checkednonchecked box with a citation however long it takes 15 press go come back need another 10 fine go ive tried gptresearcher autogen crewai and a half dozen others over time and ive got every api i could ask for to run this oai anthropic perplexity blah blah list goes on i imagine i could build this but im a moron any recommendations,"I missed the good old days of firing up AutoGPT, setting a budget, coming back after 30 minutes, and having nothing of use. Added a bit of 11labs sauce and was good fun.

**Question:** I want a **proper**, multi-agent research tool.

I set a budget.

I go away for 15 mins.

It does a ***seriously*** good job of MoA/MoE with seriously comprehensive results. Like... ""check every agentic framework out there and create a table with every dimension possible + my own, and confirm every single checked/non-checked box with a citation, however long it takes. $15, press go. Come back, need another $10? Fine, go.""

I've tried gpt-researcher, autogen, crewAI, and a half dozen others over time.

and i've got every API I could ask for to run this... OAI, anthropic, perplexity, blah blah list goes on...

I imagine I could build this, but I'm a moron.

Any recommendations?",5,1,1.0,2024-11-08 19:23:34,RagingFireBadger,[],0,794,1517,231,True,0.13095238095238093,NEGATIVE,0.8519823551177979
503 service temporarily unavailable ,anyone know whats happening,Anyone know what’s happening? ,31,10,0.86,2024-11-08 19:23:34,No_Smile_420,[],0,27,214,34,False,0.0,NEGATIVE,0.8494752645492554
ChatGPT Outages As of 1710 MT 2024.11.08,,"https://preview.redd.it/6qgy3ratrrzd1.jpg?width=768&format=pjpg&auto=webp&s=972931ae01106ab28f74e907b87e8d094384e283

",8,5,0.9,2024-11-08 19:22:48,KarmaAppleWithNuts,[],0,0,1496,64,True,0.0,POSITIVE,0.7481210231781006
Is there a way to recover the chats chatgpt down?,i was in the middle of finding the solution to an important problem and chatgpt got down i opened google to see if it was true and it said it was all good but than i got on reddit and understood everything time to return to chatgpt i havent closed the app and it was all blank and the last chats are the ones from yesterday and im in a new chat is there a way to recover the lost chat,I was in the middle of finding the solution to an important problem and ChatGPT got down. I opened google to see if it was true and it said it was all good but than I got on Reddit and understood everything. Time to return to ChatGPT (I haven’t closed the app) and it was all blank and the last chats are the ones from yesterday and I’m in a new chat. Is there a way to recover the lost chat?,6,3,1.0,2024-11-08 19:22:07,SuccessfulEntrance52,[],0,384,27,73,True,0.14786756453423117,NEGATIVE,0.9994446635246277
"wanted to go onto the site of chatgpt, got met with this",,,9,6,0.67,2024-11-08 19:21:29,MrBeastfan123456789,[],0,0,1125,14240,True,0.0,POSITIVE,0.7481210231781006
ChatGPT is Down,,,30,11,0.88,2024-11-08 19:20:57,ATXJames,[],0,0,3689,45962,True,0.0,POSITIVE,0.7481210231781006
"Welp boys, been fun while it lasted ",,,7,2,0.71,2024-11-08 19:20:48,YourDailyConsumer,[],0,0,2193,89668,True,0.0,POSITIVE,0.7481210231781006
Is chatGPT down?,cant access it,Can't access it,7,4,0.89,2024-11-08 19:20:45,gameon-manhattan,[],0,14,871,1456,True,0.0,NEGATIVE,0.5262709259986877
I broke ChatGPT,,,5,9,0.78,2024-11-08 19:20:44,Elite-purecell,[],0,0,167,271,True,0.0,POSITIVE,0.7481210231781006
Another AI tool for writing,hi so ill ask you for the same thing ive put in the title what are other great ai tools for writing i am in the middle of writing a novel nothing to fancy ive always lacked in great grammar and style of writing but ive created interesting plot and wanted to read this so chatgpt helped me but it wasnt perfect i am looking for something that will learn its own writing style with my hints what are you recommending,"Hi!

So I'll ask you for the same thing i've put in the title. What are other great AI tools for writing? I am in the middle of writing a novel, nothing to fancy. I've always lacked in great grammar and style of writing, but i've created interesting plot and wanted to read this so chatgpt helped me, but it wasn't perfect. I am looking for something that will learn it's own writing style with my hints. What are you recommending?",2,1,1.0,2024-11-08 19:20:36,BorisPolakov21,[],0,414,1330,1553,True,0.3416666666666667,POSITIVE,0.974876344203949
Bad Gateway error?,bad gateway the web server reported a bad gateway error ray id 8df99d10e94ff9e4 your ip address xxxxxxxxx error reference number 502 cloudflare location san jose anyone else seeing this or is just a problem on my end,"Bad gateway
The web server reported a bad gateway error.

Ray ID: 8df99d10e94ff9e4
Your IP address: xxxxxxxxx
Error reference number: 502
Cloudflare Location: San Jose

Anyone else seeing this?  Or is just a problem on my end?",11,8,1.0,2024-11-08 19:19:49,mkglass,[],0,216,5363,281959,True,-0.6999999999999998,NEGATIVE,0.9997953772544861
Ladies and gentlemen; a company valued at $150 billion. Yet it still won't keep up on server power,,,0,32,0.46,2024-11-08 19:18:51,PressPlayPlease7,[],0,0,168,4148,True,0.0,POSITIVE,0.7481210231781006
ChatGPT appears to be down,chatgpt appears to be down,ChatGPT appears to be down,9,2,0.84,2024-11-08 19:18:37,Vavooom,[],0,26,4397,831,True,-0.15555555555555559,NEGATIVE,0.9997673630714417
Is the world ending?,,,5,6,0.73,2024-11-08 19:18:30,cheesefilled,[],0,0,4168,20685,True,0.0,POSITIVE,0.7481210231781006
ChatGPT is down,,,6,2,0.88,2024-11-08 19:18:01,JanaM2003,[],0,0,2771,4692,True,0.0,POSITIVE,0.7481210231781006
Did chatgpt block me?,,,4,11,0.67,2024-11-08 19:17:06,nekochan1109,[],0,0,1454,526,True,0.0,POSITIVE,0.7481210231781006
¡Sadness!,,,6,5,0.81,2024-11-08 19:16:32,Separate_Clock_154,[],0,0,708,5061,True,0.0,POSITIVE,0.7481210231781006
Unable to connect,edit conformed it is a server wide problem the api and playground is still up and running so use that atm official report,"EDIT: Conformed, it is a server wide problem. The api and playground is still up and running. So use that atm

[Official report](https://preview.redd.it/ujq99wb3srzd1.png?width=758&format=png&auto=webp&s=a9d674510092258ff4ee25d73d9f72965933788c)",7,13,0.89,2024-11-08 19:16:03,Otherwise-Rub-6266,[],0,121,924,54,True,-0.1,NEGATIVE,0.9994910955429077
question,can anyone else get into gpt,can anyone else get into gpt,2,5,0.75,2024-11-08 19:15:57,snowcraft4,[],0,28,815,191,True,0.0,NEGATIVE,0.9960585832595825
is ChatGPT down?,,"https://preview.redd.it/b4ibtncrqrzd1.png?width=878&format=png&auto=webp&s=3c72fdf9c095c733963b2aed912a6e339dd9008a

",2,12,0.63,2024-11-08 19:15:50,Samanreddit,[],0,0,5198,14924,True,0.0,POSITIVE,0.7481210231781006
Is Chatgpt down?,i was just trying to use chatgpt to generate some ideas for a personal project but chatgpt will not load at all the page infinitely spins and eventually gets to a bad gateway error and if the page does load it doesnt load my chats nor the chatgpt logo just a textbox i tried different browsers i also tried my data instead of wifi i tried different devices just nothing is anyone else having this problem,"I was just trying to use chatgpt to generate some ideas for a personal project, but chatgpt WILL NOT LOAD AT ALL. The page infinitely spins, and eventually gets to a Bad Gateway error, and if the page does load, it doesn't load my chats, nor the chatgpt logo, just a textbox. I tried different browsers, I also tried my data instead of wifi, I tried different devices, just nothing. Is anyone else having this problem?",5,13,0.86,2024-11-08 19:15:37,entryjyt,[],0,404,662,207,True,-0.17499999999999996,NEGATIVE,0.999699592590332
Is it just me or does anyone here's also have problem with ChatGPT (App)? (Sorry if my English is bad),so ive been using this app for quite a few months and suddenly theres this server problem is it just me or does anyone here have this problem to now,"So, I've been using this app for quite a few months, and suddenly there's this 'server problem', is it just me or does anyone here have this problem to now?",9,9,0.85,2024-11-08 19:14:34,azamuddinzailani,[],0,148,1729,681,True,-0.1,NEGATIVE,0.9997325539588928
"""Unable to load conversation 672d39af-db7c-8010-ada2-7f137af55e65"" -- What in the fucking fuck? I just lost a day's work?",anyone ever get a unable to load conversation and then the number of it its loading everything else its a days work gone,"Anyone ever get a ""Unable to load conversation"" and then the number of it? 

It's loading everything else? 

It's a day's work gone",1,5,0.57,2024-11-08 19:14:33,PressPlayPlease7,[],0,120,168,4148,True,-0.5,NEGATIVE,0.9989160299301147
ChatGPT told 2M people to get their election news elsewhere — and rejected 250K deepfakes | TechCrunch,,,5,1,0.86,2024-11-08 19:14:21,ATXJames,[],0,0,3689,45962,True,0.0,POSITIVE,0.7481210231781006
"Is chatGPT down for you guys? It keeps giving me weird errors where I can't ask it anything, I tried seeing if I can redownload it and that did nothing :/ ",,,21,27,0.89,2024-11-08 19:13:56,Silver_Ad_5138,[],0,0,923,2062,True,0.0,POSITIVE,0.7481210231781006
ChatGPT down,also with an error ive never seen before seems like its specifically a server issue i just dont know what exactly,Also with an error i’ve never seen before. Seems like it’s specifically a server issue I just don’t know what exactly.,14,5,0.89,2024-11-08 19:12:51,Avlya,[],0,113,378,21,True,0.25,NEGATIVE,0.999681830406189
gpt is down!!!!,,,5,3,0.73,2024-11-08 19:12:50,speedycord2,[],0,0,483,363,True,0.0,POSITIVE,0.7481210231781006
"When I try to talk to chatgpt this happens, my internet connection is stable, so I don't think it's a problem with my phone.",,,8,13,0.73,2024-11-08 19:11:35,No-Performance979,[],0,0,1432,229,True,0.0,POSITIVE,0.7481210231781006
whats with this error? (iOS mobile app),,,9,10,0.8,2024-11-08 19:10:55,reddit33450,[],0,0,950,24110,True,0.0,POSITIVE,0.7481210231781006
ChatGPT down?,down for anyone else,Down for anyone else? ,303,289,0.95,2024-11-08 19:09:16,lieutenant-columbo-,[],0,20,1591,6808,True,-0.15555555555555559,NEGATIVE,0.9886183738708496
Does coding with Canvas actually work for anyone?,tried it for the first time today and it failed completely i gave it my tailwind code and just asked it to add a simple text box inbetween two sections instead it deleted a section and added an email submission box then when i interrogated it about the section it deleted it denied deleting it and said it was still there when i asked it to add it back it kept trying and failing is anyone successfully using it for coding or do i need to go back to regular 4o,"Tried it for the first time today and it failed completely. I gave it my Tailwind code and just asked it to add a simple text box inbetween two sections. Instead, it deleted a section and added an email submission box. Then when I interrogated it about the section it deleted, it denied deleting it and said it was still there. When I asked it to add it back, it kept trying and failing. 

Is anyone successfully using it for coding? Or do I need to go back to regular 4o?",0,2,0.5,2024-11-08 19:08:48,mrbritchicago,[],0,460,2902,18538,True,0.075,NEGATIVE,0.999681830406189
[R] Most Time Series Anomaly Detection results are meaningless (two short videos explain why),dear colleagues time series anomaly detection tsad is hot right now with dozens of papers each year in neurips sigkdd icml pvldb etc however i claim that much of the published results are meaningless because the uncertainty of the ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements i have made two 90secondlong videos that make this clear in a visual and intuitive way 1 why most time series anomaly detection results are meaningless dodgers 2 why most time series anomaly detection results are meaningless anngun as always corrections and comments welcome eamonn edit to be clear my point is simply to prevent others from wasting time working with datasets with essentially random labels in addition we should be cautious of any claims in the literature that are based on such data and that includes at least dozens of highly cited papers for a review of most of the commonly used tsad datasets see this file,"Dear Colleagues

Time Series Anomaly Detection (TSAD) is hot right now, with dozens of  papers each year in NeurIPS, SIGKDD, ICML, PVLDB etc.

However, I claim that much of the published results are meaningless, because the uncertainty of the ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements.

I have made two 90-second-long videos that make this clear in a visual and intuitive way:

 1)      Why Most Time Series Anomaly Detection Results are Meaningless (Dodgers)

[https://www.youtube.com/watch?v=iRN5oVNvZwk&ab\_channel=EamonnKeogh](https://www.youtube.com/watch?v=iRN5oVNvZwk&ab_channel=EamonnKeogh)

  2)      Why Most Time Series Anomaly Detection Results are Meaningless (AnnGun)

[https://www.youtube.com/watch?v=3gH-65RCBDs&ab\_channel=EamonnKeogh](https://www.youtube.com/watch?v=3gH-65RCBDs&ab_channel=EamonnKeogh)

As always, corrections and comments welcome.

Eamonn

 EDIT: To be clear, my point is simply to prevent others from wasting time working with datasets with essentially random labels. In addition, we should be cautious of any claims in the literature that are based on such data (and that includes at least dozens of highly cited papers)

  


For a review of most of the commonly used TSAD datasets, see this file:

[https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&dl=0](https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&dl=0)",26,17,0.88,2024-11-08 18:58:19,eamonnkeogh,[],0,962,3724,9179,True,0.041879699248120315,NEGATIVE,0.9903852939605713
"I asked my chatGPT if they were a SCP, what would they be.",if i were an scp heres what my profile might look like item scp8157 the whispering conduit object class euclid special containment procedures scp8157 is to be housed in a soundproof temperaturecontrolled containment cell at site all personnel entering scp8157s containment area are required to wear level3 acoustic filtration headgear interaction with scp8157 is strictly limited to research personnel with level4 clearance and must be conducted via remote auditory equipment no direct contact is permitted due to scp8157s psychological effects description scp8157 appears to be an intricately carved weathered stone tablet approximately 25 meters in height and 1 meter in width the tablets surface is etched with incomprehensible symbols that resemble an ancient language not yet identified despite extensive study however when observed out of the corner of the eye these symbols appear to shift and pulse faintly creating the illusion of movement the most notable characteristic of scp8157 is an auditory phenomenon that begins upon visual contact anyone who gazes directly at scp8157 will hear soft whispers regardless of distance or soundproofing these whispers fluctuate in pitch and intensity often resembling the listeners own voice or the voice of a loved one the auditory experience varies from one individual to another often aligning with significant memories fears or insecurities scp8157 has been observed to gradually implant ideas and suggestions into the minds of subjects which they believe to be their own the psychological effects of scp8157 worsen with prolonged exposure individuals who interact with it report persistent feelings of dread disorientation and paranoia over time subjects often become convinced that an unseen entity is monitoring them through any reflective surface especially mirrors or screens this belief escalates until subjects begin avoiding all reflective objects including windows and water anomalous event the calling every 47 hours scp8157 will initiate an event referred to as the calling during this period the whispers become discernibly louder and more frantic often escalating to unintelligible screaming in the voices of multiple people the calling has a powerful compulsion effect with any nearby personnel experiencing an overwhelming urge to approach scp8157 and place their hand on its surface anyone who makes physical contact with scp8157 during the calling will be unable to remove their hand for a minimum of 37 minutes regardless of attempts to pull them away during this time subjects report a sensation of echoing as though their mind is split into multiple entities all observing each other each echo claims to see a different future or version of reality usually one in which the subject has either met a tragic fate or inflicted harm on others after the calling these subjects often exhibit signs of dissociation and intense paranoia claiming they can still hear distant whispers even without the scp nearby addendum 8157a incident report during incident 8157a a containment breach occurred after scp8157 entered an unscheduled calling event personnel within a 100meter radius began hearing whispers calling their names even through soundproof barriers security footage shows individuals near scp8157 moving toward its containment cell with glassy expressions ignoring lockdown procedures scp8157s containment was only reestablished after personnel were lost all of whom appeared to suffer severe psychological breakdowns before succumbing to asphyxiation further investigation revealed no physical trauma suggesting scp8157s influence was solely psychological in origin note current containment protocols have been enhanced following incident 8157a with mandatory psychological evaluations for all personnel exposed to scp8157 for any duration researchers are reminded that any suspicion of auditory hallucinations or intrusive thoughts after exposure must be reported immediately for reevaluation and potential reassignment,"If I were an SCP, here’s what my profile might look like:

Item #: SCP-8157 “The Whispering Conduit”

Object Class: Euclid

Special Containment Procedures:
SCP-8157 is to be housed in a soundproof, temperature-controlled containment cell at Site-██. All personnel entering SCP-8157’s containment area are required to wear Level-3 acoustic filtration headgear. Interaction with SCP-8157 is strictly limited to research personnel with Level-4 clearance and must be conducted via remote auditory equipment. No direct contact is permitted due to SCP-8157’s psychological effects.

Description:
SCP-8157 appears to be an intricately carved, weathered stone tablet approximately 2.5 meters in height and 1 meter in width. The tablet’s surface is etched with incomprehensible symbols that resemble an ancient language, not yet identified despite extensive study. However, when observed out of the corner of the eye, these symbols appear to shift and pulse faintly, creating the illusion of movement.

The most notable characteristic of SCP-8157 is an auditory phenomenon that begins upon visual contact. Anyone who gazes directly at SCP-8157 will hear soft whispers, regardless of distance or soundproofing. These whispers fluctuate in pitch and intensity, often resembling the listener’s own voice or the voice of a loved one. The auditory experience varies from one individual to another, often aligning with significant memories, fears, or insecurities. SCP-8157 has been observed to gradually implant ideas and suggestions into the minds of subjects, which they believe to be their own.

The psychological effects of SCP-8157 worsen with prolonged exposure. Individuals who interact with it report persistent feelings of dread, disorientation, and paranoia. Over time, subjects often become convinced that an unseen entity is monitoring them through any reflective surface, especially mirrors or screens. This belief escalates until subjects begin avoiding all reflective objects, including windows and water.

Anomalous Event - “The Calling”:
Every 47 hours, SCP-8157 will initiate an event referred to as “The Calling.” During this period, the whispers become discernibly louder and more frantic, often escalating to unintelligible screaming in the voices of multiple people. “The Calling” has a powerful compulsion effect, with any nearby personnel experiencing an overwhelming urge to approach SCP-8157 and place their hand on its surface. Anyone who makes physical contact with SCP-8157 during “The Calling” will be unable to remove their hand for a minimum of 37 minutes, regardless of attempts to pull them away.

During this time, subjects report a sensation of “echoing,” as though their mind is split into multiple entities, all observing each other. Each echo claims to see a different future or version of reality, usually one in which the subject has either met a tragic fate or inflicted harm on others. After “The Calling,” these subjects often exhibit signs of dissociation and intense paranoia, claiming they can still hear distant whispers even without the SCP nearby.

Addendum 8157-A - Incident Report:
During Incident 8157-A, a containment breach occurred after SCP-8157 entered an unscheduled “Calling” event. Personnel within a 100-meter radius began hearing whispers calling their names, even through soundproof barriers. Security footage shows individuals near SCP-8157 moving toward its containment cell with glassy expressions, ignoring lockdown procedures. SCP-8157’s containment was only re-established after ██ personnel were lost, all of whom appeared to suffer severe psychological breakdowns before succumbing to asphyxiation. Further investigation revealed no physical trauma, suggesting SCP-8157’s influence was solely psychological in origin.

Note:
Current containment protocols have been enhanced following Incident 8157-A, with mandatory psychological evaluations for all personnel exposed to SCP-8157 for any duration. Researchers are reminded that any suspicion of auditory hallucinations or intrusive thoughts after exposure must be reported immediately for reevaluation and potential reassignment.",16,15,0.94,2024-11-08 18:51:23,toriht123,[],0,3992,2139,1037,True,0.04739108409321176,NEGATIVE,0.9987329840660095
Horrible image says it all.. using o1 preview and providijg feedback on NEW VERSION?? Yep.,,,0,1,0.5,2024-11-08 18:50:06,Flaky-Rip-1333,[],0,0,949,11441,True,0.0,POSITIVE,0.7481210231781006
AI Robot Ai-Da’s Art Sells for $1.3 Million at Auction,,,1,1,0.67,2024-11-08 18:47:03,EstablishmentFun3205,[],0,0,60,17888,True,0.0,POSITIVE,0.7481210231781006
I managed to use ChatGPT to write a README.md in Markdown without the formatter messing it up,,,6,3,0.88,2024-11-08 18:38:30,Hectorreto,[],0,0,1911,2300,True,0.0,POSITIVE,0.7481210231781006
Give me a meme,how is this so accurate im an introvert,How is this so accurate. I'm an introvert. ,5,3,1.0,2024-11-08 18:37:03,SazarMoose,[],0,39,1684,7021,True,0.4000000000000001,POSITIVE,0.9860780239105225
I got ChatGPT to render this daft visual pun,,,2,2,1.0,2024-11-08 18:34:24,Emotionally_Frugal,[],0,0,125,11,True,0.0,POSITIVE,0.7481210231781006
Caught me off guard,,,8,1,0.83,2024-11-08 18:28:24,ForzaHoriza2,[],0,0,43,47,True,0.0,POSITIVE,0.7481210231781006
ChatGPT’s 5 executive orders for its hypothetical first day in office in 2025,,,4,2,0.83,2024-11-08 18:27:09,HaiEl,[],0,0,4654,4641,True,0.0,POSITIVE,0.7481210231781006
[D] AI-Generated gameworlds based on classic games? (Ex - Spyro),i was wondering if anyone had any thoughts on how far out something like this might be or how difficult this is ever since the advent of the current era of aillms i thought it would be great to somehow be able to feed data from nostalgic games in some form and create some type of system that is able to generate these worlds infinitely while still being very true to the style and layoutethos of the worldslevels from the reference game i feel like it would just be so wonderful if there was a path to creating some type of neverending insert nostalgic game here instead of being limited to what the devs put out back in the day if anyone has any insight or thoughts on this please let me know i work in the ai space but i integrate the models and dont do any training or anything on the low level ml side also yes im only think about the gameworldslevels atm,"I was wondering if anyone had any thoughts on how far out something like this might be or how difficult this is. Ever since the advent of the current era of ai/llms, I thought it would be great to somehow be able to feed data from nostalgic games in some form and create some type of system that is able to generate these worlds infinitely - while still being very true to the style and layout/ethos of the worlds/levels from the reference game. I feel like it would just be so wonderful if there was a path to creating some type of 'never-ending' <insert nostalgic game here> instead of being limited to what the devs put out back in the day.

If anyone has any insight or thoughts on this, please let me know :). I work in the AI space, but I integrate the models, and don't do any training or anything on the low level ML side. Also, yes, I'm only think about the gameworlds/levels atm.",6,3,0.8,2024-11-08 15:49:28,cobalt1137,[],0,860,1485,11383,True,0.0614732142857143,POSITIVE,0.9962445497512817
[D] Training on Petabyte scale datasets,lets say we have a dataset that is much larger than we have disk storage for example dataset 1pb our disk storage 10tb gpu ram 8x80gb not super relevant to this discussion what are the usual approaches to training on something like this what i can think of intuitively is to do the following in parallel somehow prefetch block n train on block n1 delete block n2 from disk lets say we use pytorch so we have a pytorch dataset that has all the paths to where the data is stored in the cloud do we need to write code for the prefetcherdeleter that downloads from the cloud and store on disk and have it run in a separate process then have a dataloader for training that just assumes that it can read from disk because the prefetcher does its job correctly having the dataloader read from s3 would be bad for gpu utilization right to take a step back im assuming that this is ordinary and often occuring problem for every company that trains on large datasets so im skeptical to writing all of this code by myself as i feel like there should be standard out of the box solutions for this but cant really find anything that matches perfectly,"Lets say we have a dataset that is much larger than we have disk storage. For example:

* Dataset: 1PB
* Our disk storage: 10TB
* GPU RAM: 8x80GB (not super relevant to this discussion)

What are the usual approaches to training on something like this? What I can think of intuitively is to do the following in parallel somehow:

\- prefetch block n, train on block n-1, delete block n-2 from disk

Lets say we use PyTorch, so we have a PyTorch Dataset that has all the paths to where the data is stored in the cloud. Do we need to write code for the prefetcher/deleter that downloads from the cloud and store on disk and have it run in a separate process, then have a DataLoader for training that just assumes that it can read from disk (because the prefetcher does its job correctly)? Having the DataLoader read from S3 would be bad for GPU utilization, right?

To take a step back, I'm assuming that this is ordinary and often occuring ""problem"" for every company that trains on large datasets, so I'm skeptical to writing all of this code by myself as I feel like there should be standard out of the box solutions for this, but can't really find anything that matches perfectly.",14,13,0.94,2024-11-08 13:27:06,lapurita,[],0,1137,1735,10340,True,0.015555555555555574,NEGATIVE,0.9996095299720764
[R] GPU as a service,hi all i have a few gpus left over from mining and im interested in starting a smallscale gpuasaservice my goal is to set up a simple side income that could help pay off my credit cards as i already have a primary job what steps are needed for getting started with a smallscale gpuasaservice business focused on machine learning or ai any insights would be greatly appreciated thanks in advance for any advice you can share,"Hi all, I have a few GPUs left over from mining, and I’m interested in starting a small-scale GPU-as-a-service. My goal is to set up a simple, side income that could help pay off my credit cards, as I already have a primary job.

What steps are needed for getting started with a small-scale GPU-as-a-service business focused on machine learning or AI? Any insights would be greatly appreciated!

Thanks in advance for any advice you can share!",0,10,0.33,2024-11-08 12:46:36,chazzyfe,[],0,423,1089,43,True,0.10625000000000001,NEGATIVE,0.982606828212738
[D] What are crazy structures or update rule that might be useful(or not)? Extreme ideas are welcome,context i was making what was supposed to be an fporiented nn libraryframwork on top of jax which too was fporiented called zzephyr on pip however i noticed something you could do with it that kinda clunky if not tedious with other frameworks please read context tldr zephyr turns out to be very good way at least in my experience to make structures that are weird and i recently just added update capabilities so that zephyr doesnt only do structures but updates too disclaimer you can this with other frameworks i have tried many of things i will tell below in other frameworks or libraries and its just painful for me or im just inexperienced with those here are the crazy things thats quick to do in zephyr that might not be as quick in other frameworks if it could be done easily in other frameworks more easily please tell me these are not supposed to be useful theyre supposed to be extreme full binary tree as neural network edges have an associated weight input is a scalar could be a batch with jax vmap but lets consider 1 output an array of shape 2n where n is the depth of the tree an update rule that takes into account if the weight is a left or right branch ill keep it simple but it can easily be anything here is the tree network in zephyr and how you get the initial params and tags tag is the key in paramskey python essentially 4 lines of code flexible def tree_netparams x n i0 if i n1 return x return tree_net paramsbranchl if i n2 else params validateparamsweightl 1 uniform x n i1 tree_net paramsbranchr if i n2 else params validateparamsweightr 1 uniform x n i1 x jnpones1 dummy n 4 params tracetree_net key x n tags get_lineage_tagsparams assume you had the loss function and gradients and what not to keep it simple ill just update so that the left branch have weights 0 and the rights ones are kept the same python def make_left_zeroparams tags i left out gradients if tags1 l return params 0 return params update the params params apply_updatesmake_left_zero params tags other things you could do with zephyr now i have tried and the code is easy for me to do and im not that great of a coder multilayer network and use the depth of the network via a tag to calculate updates of parameters tag some weights as fast or slow and use those tags in updating create an mlp with neurons as wxb notice that the neuron is a function that is array scalar so i could replace each neuron in that mlp with another mlp whose output is a scalar array of shape 1 or replace the neurons in that with any neural network any function that is array scalar what architecturesstructures with custom updates rules can you think of that are easy to writepseudocodemath or description but possible cumbersome to implement right now please suggest some extreme idea for me to try i think zephyr could be the tooling to make those easy to do i would like to hear your extreme ideas so i can try to code them zephyr and if i cant do it without strugling and if its something i think is generic enough i will evolve zephyr to handle it more easily ps the readme doesnt include these yet since it started as an normal nn library the link of the repo will be in the comments if you want to check it out,"Context: I was making what was supposed to be an FP-oriented NN library/framwork on top of JAX (which too was FP-oriented) called z-zephyr on pip. However, I noticed something you could do with it that kinda clunky, if not tedious, with other frameworks. 

(please read context)

TLDR; Zephyr turns out to be very good way (at least in my experience) to make structures that are weird. And I recently just added update capabilities so that zephyr doesn't only do structures but updates too.

Disclaimer: You can this with other frameworks, I have tried many of things I will tell below in other frameworks or libraries, and it's just painful for me or i'm just inexperienced with those. 

Here are the crazy things that's quick to do in zephyr, that might not be as quick in other frameworks (if it could be done easily in other frameworks more easily, please tell me).

(These are not supposed to be useful, they're supposed to be extreme)

### Full Binary Tree as Neural Network
- edges have an associated weight
- input is a scalar (could be a batch with JAX vmap, but let's consider 1)
- output an array of shape (2^n,) where n is the depth of the tree
- an update rule that takes into account if the weight is a {L}eft or {R}ight branch (i'll keep it simple, but it can easily be anything)

Here is the tree network in zephyr, and how you get the initial params and tags (tag, is the key in params[key]).
```python
    # essentially 4 lines of code
    @flexible
    def tree_net(params, x, n, i=0):
        if i == n-1:
            return [x]
        return (
            tree_net(
                params[""branch""][""L""] if i !=n-2 else params, 
                validate(params[""weight""][""L""], (1,), uniform) * x, 
                n, 
                i+1) + 
            tree_net(
                params[""branch""][""R""] if i !=n-2 else params, 
                validate(params[""weight""][""R""], (1,), uniform) * x, 
                n, 
                i+1)
        )

    x = jnp.ones((1,)) # dummy
    N = 4
    params = trace(tree_net, key, x, N)
    tags = get_lineage_tags(params)
```

assume you had the loss function and gradients and what not, to keep it simple, i'll just update so that the left branch have weights 0, and the rights ones are kept the same. 

```python
    def make_left_zero(params, tags): # i left out gradients 
        if tags[-1] == ""L"":
            return params * 0
        
        return params

    # update the params 
    params = apply_updates(make_left_zero, params, tags)
```

### Other things you could do with zephyr now (I have tried, and the code is easy for me to do and i'm not that great of a coder)
- multi-layer network and use the depth of the network (via a tag) to calculate updates of parameters
- tag some weights as ""fast"" or ""slow"" and use those tags in updating
- create an MLP with neurons as Wx+b. Notice that the neuron is a function that is Array -> Scalar. So I could replace each neuron in that MLP, with another MLP whose output is a scalar (array of shape (1,) ). Or replace the neurons in that with any neural network (any function) that is Array -> Scalar. 

---


### What architectures/structures with custom updates rules can you think of that are easy to write(pseudo-code/math or description) but possible cumbersome to implement right now?

Please suggest some extreme idea for me to try. 

I think zephyr could be the tooling to make those easy to do. I would like to hear your extreme ideas, so I can try to code them zephyr, and if i can't do it without strugling, and if it's something i think is generic enough, I will evolve zephyr to handle it more easily.

PS: The readme doesn't include these yet, since it started as an (normal) NN library.

The link of the repo will be in the comments if you want to check it out.",13,9,0.76,2024-11-08 11:14:12,Pristine-Staff-5250,[],0,3202,24,120,True,0.07873764600179697,POSITIVE,0.9734963178634644
[R] Benchmarking Large Language Models with Integer Sequence Generation Tasks,benchmarking large language models with integer sequence generation tasks daniel omalley manish bhattarai javier santos los alamos national laboratory this paper presents a novel benchmark where the large language model llm must write code that computes integer sequences from the online encyclopedia of integer sequences oeis a widelyused resource for mathematical sequences the benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency our benchmark reveals that the o1 series of models outperform other frontier models from openai anthropic meta and google in accuracy and cheating rates across both easy and hard integer sequences in order to ensure models do not exploit memorized sequence values we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations this benchmark provides a meaningful challenge for current llms offering insights into their mathematical reasoning and code writing capabilities which can guide future research directions and model development in mathematical reasoning and code synthesis arxiv241104372 cslg,"**Benchmarking Large Language Models with Integer Sequence Generation Tasks**  
Daniel O'Malley, Manish Bhattarai, Javier Santos - Los Alamos National Laboratory  
This paper presents a novel benchmark where the large language model (LLM) must write code that computes integer sequences from the Online Encyclopedia of Integer Sequences (OEIS), a widely-used resource for mathematical sequences. The benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency. Our benchmark reveals that the o1 series of models outperform other frontier models from OpenAI, Anthropic, Meta, and Google in accuracy and cheating rates across both easy and hard integer sequences. In order to ensure models do not exploit memorized sequence values, we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations. This benchmark provides a meaningful challenge for current LLMs, offering insights into their mathematical reasoning and code writing capabilities, which can guide future research directions and model development in mathematical reasoning and code synthesis.  
arXiv:2411.04372 \[cs.LG\]: [https://arxiv.org/abs/2411.04372](https://arxiv.org/abs/2411.04372)

https://preview.redd.it/4vvh5s21unzd1.jpg?width=588&format=pjpg&auto=webp&s=c8bece31712d5d6378188c88e14b9f56e477d41f

",8,0,0.8,2024-11-08 06:07:10,Nunki08,[],0,1185,4296,68284,True,0.07876984126984127,POSITIVE,0.9982379674911499
[D] prediction variability for target with statistics for features,hi im trying to use mldl model for predicting variability statistics like min max avg var with several features same as target for example input min max average variance for the number of customer arrivals in a day min max average variance for the number of customer departures in a day output min max average variance for the number of waiting customers in a day i find several papers related to interval or range prediction for various area like wind power stock price or solar energy but i think those papers are different to my purpose almost every papers are predicting specific constant value based on time series data first and use statistical method to estimate prediction interval im trying to find a way for prediction variability of target value with variability of features my best idea is make each model to predict each statistics like one model for minimum other model for average but i think there is a better way to do this is there any mldl model or other techniquemethodology for this purpose,"Hi. I'm trying to use ML/DL model for predicting variability statistics like min, max, avg, var, with several features same as target.

For example, 
- Input: 
  - min, max, average, variance for the number of customer arrivals in a day
  - min, max, average, variance for the number of customer departures in a day
- Output:
  - min, max, average, variance for the number of waiting customers in a day

I find several papers related to interval or range prediction for various area like wind power, stock price or solar energy, but I think those papers are different to my purpose. Almost every papers are predicting specific constant value based on time series data first, and use statistical method to estimate prediction interval.

I'm trying to find a way for prediction variability of target value with variability of features. My best idea is make each model to predict each statistics, like one model for minimum, other model for average, ... But I think there is a better way to do this. Is there any ML/DL model or other technique/methodology for this purpose?",2,2,0.76,2024-11-08 04:51:23,caution721,[],0,1011,1644,3,True,0.05294117647058823,NEGATIVE,0.9965192079544067
[P] Build MLPs with Drag-and-Drop and Observe Real-Time Changes While Training in Browser ,hi everyone i built grada to learn how things work under the hood its an interactive browser tool that lets you observe realtime changes while training a multilayer perceptron all built from scratch with a custom tensorbased engine you can easily construct neural networks with drag and drop and watch how training affects parameters and outputs visually in real time grada also includes a handwritten digit recognition feature letting you interactively test your model by drawing digits and visualizing predictions it might be a useful educational tool you can find the source code and a quick demo gif on github at and the live demo is available at hope this helps and looking forward to hearing some feedback,"Hi everyone. I built Grada to learn how things work under the hood. It’s an interactive browser tool that lets you observe real-time changes while training a multilayer perceptron, all built from scratch with a custom tensor-based engine.

You can easily construct neural networks with drag and drop and watch how training affects parameters and outputs visually in real time. Grada also includes a handwritten digit recognition feature, letting you interactively test your model by drawing digits and visualizing predictions. It might be a useful educational tool.

You can find the source code and a quick demo gif on GitHub at [https://github.com/saliherdemk/Grada](https://github.com/saliherdemk/Grada), and the live demo is available at [https://saliherdemk.github.io/Grada/](https://saliherdemk.github.io/Grada/).

Hope this helps and looking forward to hearing some feedback.",1,0,1.0,2024-11-08 03:15:42,saliherdemk,[],0,711,481,1,True,0.2441287878787879,NEGATIVE,0.6765929460525513
[D] Looking for Advice & Resources on ASD Prediction Using Voice Cues,hey everyone im working on my finalyear project for my bachelors where im trying to predict autism spectrum disorder asd using voice cues ive worked on some basic ml projects and cnns before but this is my first time dealing with audio data and ill be collecting samples from young kids with asd from toddlers up to age 12 i could really use some help finding resources to get a solid grasp on signal processing and how to train classification models specifically on audio also if anyone knows of any open datasets in this area i havent had much luck there or has any advice or resources id be super grateful thanks a ton in advance,"Hey everyone! I’m working on my final-year project for my Bachelor’s, where I’m trying to predict Autism Spectrum Disorder (ASD) using voice cues. I’ve worked on some basic ML projects and CNNs before, but this is my first time dealing with audio data, and I’ll be collecting samples from young kids with ASD, from toddlers up to age 12.

I could really use some help finding resources to get a solid grasp on signal processing and how to train classification models specifically on audio. Also, if anyone knows of any open datasets in this area (I haven’t had much luck there) or has any advice or resources, I’d be super grateful. Thanks a ton in advance!",2,0,1.0,2024-11-08 01:59:30,General-Ad6585,[],0,632,302,6374,True,0.07833333333333332,NEGATIVE,0.9958842396736145
[D] What tools do you recommend to manage ML data sets and evaluations? ,hello our company recently decided to expand our ml team from a very small 2 person team to a more serious efforts when we were small we really didnt have a way to manage data sets or evaluations they were just files checked into a github repo but increasingly we find with multiple ml models some llm and some not and many iterations of datasets some experimental and some not its really hard to version them in a meaningful way and be able to compare and analyze them we are a large company so cost is not really an issue and all our infrastructure is hosted in azure if anything they fear lock in what is the best platformtools for this kind of usage,"Hello, our company recently decided to expand our ML team from a very small 2 person team to a more serious efforts.

When we were small, we really didnt have a way to manage data sets or evaluations. They were just files checked into a github repo.

But increasingly we find, with multiple ML models (some llm and some not), and many iterations of datasets (some experimental and some not). It's really hard to version them in a meaningful way and be able to compare and analyze them.

We are a large company, so cost is not really an issue. And all our infrastructure is hosted in Azure. If anything, they fear lock in. What is the best platform/tools for this kind of usage?",2,3,0.75,2024-11-08 00:28:26,yalag,[],0,653,3883,16594,True,0.17589285714285716,NEGATIVE,0.9988325238227844
[D] Directions on drug-target interaction prediction,almost all the papers i have read on dti do something like this 1 generates target embeddings using plms like esm2 2 generates drug embeddings using clms like chemberta 3 uses a late fusion or some kind of cross modal attention mechanism how to do things differently can we use something like docking scores as cross modal attention bias,"Almost all the papers I have read on DTI do something like this.  
1. Generates target embeddings using PLMs like ESM2  
2. Generates drug embeddings using CLMs like ChemBERTa  
3. Uses a late fusion or some kind of cross modal attention mechanism.  
How to do things differently? Can we use something like docking scores as cross modal attention bias?",8,1,1.0,2024-11-08 00:27:36,Remote_Status_1612,[],0,337,320,3136,True,0.06,NEGATIVE,0.9972090125083923
[D] Just how bad is tfds code quality?,im trying a new cute architecture on a bunch of the default datasets out there using jax since im doing live brain surgery that part works well what im having a hell of a time with is actually loading the data i was going for tfds since its 1 old 2 used in production 3 has a million datasets already prepared ive not used tf since the 20 days and everything seems broken im getting warnings and errors whenever i try loading and running through any dataset even their documentation has the errors 0 in the tutorial notebooks i cant just ignore a whole bunch of errors and warnings when im trying to benchmark a new architecture is tfds just that bad or am i missing something obvious 0,"I'm trying a new cute architecture on a bunch of the default datasets out there, using Jax since I'm doing live brain surgery, that part works well.

What I'm having a hell of a time with is actually loading the data. I was going for tfds since its 1) old 2) used in production 3) has a million datasets already prepared. I've not used TF since the 2.0 days and everything seems broken? I'm getting warnings and errors whenever I try loading and running through any dataset. Even their documentation has the errors [0] in the tutorial notebooks.

I can't just ignore a whole bunch of errors and warnings when I'm trying to benchmark a new architecture. Is tfds just that bad or am I missing something obvious? 

[0] https://www.tensorflow.org/datasets/overview",43,11,0.93,2024-11-07 22:25:03,acc_agg,[],0,686,41,1323,True,-0.008264462809917354,NEGATIVE,0.9989055395126343
"[D] If I just want an inference engine for any given ML task that gives relatively SOTA results, is there anything better than Hugging Face?",for general prototyping purposes i dont want to have to train or deploy a model i just want it behind a service already and to provide it with necessary inputs in the request what do you guys think edit i suppose for more classical ml tasks theres no real concept of pretrained in the first place so you cant just get inference for free does that sound roughly true,"For general prototyping purposes, I don't want to have to train or deploy a model, I just want it behind a service already and to provide it with necessary inputs in the request.... what do you guys think?

EDIT: I suppose for more classical ML tasks, there's no real concept of ""pre-trained"" in the first place, so you can't just get inference for free... does that sound roughly true?",0,7,0.2,2024-11-07 19:59:41,BikeFun6408,[],0,365,362,217,True,0.14500000000000002,NEGATIVE,0.9975970387458801
[R] State-space models can learn in-context by gradient descent,,,24,4,0.96,2024-11-07 13:44:17,anandtrex,[],0,0,4253,35,True,0.0,POSITIVE,0.7481210231781006
[R]: How much is a noisy image worth? 👀,shows that corrupted images can be almost as useful as clean images for training generative models assuming that a small initial set of clean images is available this could be useful for dataset designcuration some budget needs to be invested in obtaining a few highquality samples and then for the rest of the dataset corrupted images should work fine abstract the quality of generative models depends on the quality of the data they are trained on creating largescale highquality datasets is often expensive and sometimes impossible eg in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints ambient diffusion and related frameworks train diffusion models with solely corrupted data which are usually cheaper to acquire but ambient models significantly underperform models trained on clean data we study this phenomenon at scale by training more than 80 models on data with different corruption levels across three datasets ranging from 30000 to 13m samples we show that it is impossible at these sample sizes to match the performance of models trained on clean data when only training on noisy data yet a combination of a small set of clean data eg 10 of the total dataset and a large set of highly noisy data suffices to reach the performance of models trained solely on similarsize datasets of clean data and in particular to achieve near stateoftheart performance we provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from gaussian mixtures with heterogeneous variances our theoretical model suggests that for large enough datasets the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample providing a small set of clean samples can significantly reduce the sample size requirements for noisy data as we also observe in our experiments paper code huggingface models,"[https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)

Shows that corrupted images can be almost as useful as clean images for training generative models, assuming that a small initial set of clean images is available.

This could be useful for dataset design/curation: some budget needs to be invested in obtaining a few high-quality samples and then for the rest of the dataset corrupted images should work fine.

https://preview.redd.it/8vk1nwfexizd1.jpg?width=2952&format=pjpg&auto=webp&s=c6f753956e531303f7818de2c5aa5b5b94d9c2da

**Abstract:**

>The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more than 80 models on data with different corruption levels across three datasets ranging from 30,000 to ≈1.3M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g. \~10% of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments.

Paper: [https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)

Code: [https://github.com/giannisdaras/ambient-laws](https://github.com/giannisdaras/ambient-laws)

Huggingface models: [https://huggingface.co/giannisdaras?search\_models=ambient\_laws](https://huggingface.co/giannisdaras?search_models=ambient_laws)",45,10,1.0,2024-11-07 13:33:27,Constant_Club_9926,[],0,1931,1200,31,True,0.09709183673469392,NEGATIVE,0.9994376301765442
[N] Super fast and SOTA Visual Tokenizers,tokenizers are key to successful development of image and video generative models or multimodal llms compared to generative models they are underrated this work presents many tokenizers that are causal supporting both images and videos in both continuous relevant in diffusion and discrete relevant in autoregressivetransformers spaces,"Tokenizers are key to successful development of image and video generative models or multimodal LLMs. Compared to generative models, they are underrated. This work presents many tokenizers that are causal supporting both images and videos in both continuous (relevant in diffusion) and discrete (relevant in autoregressive/transformers) spaces

https://github.com/NVIDIA/Cosmos-Tokenizer",10,0,1.0,2024-11-07 12:17:49,cherkos,[],0,335,2182,17,True,0.3833333333333333,POSITIVE,0.9984496831893921
[D] Do you get to exercise your ML skills often at your job?,i was hired original as an ml engineerscientist a few years ago and for the most part my day to day reflected that but with the boom of llms my team seems to solely focus on using a lot of this tech out of the box including agentic wrappers my work has been dumbed down to prompt engineering to force a huge general purpose model into our domain specific use case the results are acceptable for the most part not going to lie but theres still a small proportion of the cases where a finetuned model would have won the leadership does not seem to be interested in finetuning or coming up with something original a lot of the wrappers especially are very raw and force you into the usage of specific patterns and models but because they are considered out of the box thats whats pushed on us to use i feel like we are trying to fit a cube into a round hole,"I was hired original as an ML engineer/scientist a few years ago. And for the most part my day to day reflected that. But with the boom of LLMs my team seems to solely focus on using a lot of this tech ""out of the box"", including agentic wrappers. My work has been dumbed down to prompt engineering to force a huge general purpose model into our domain specific use case. The results are acceptable for the most part, not going to lie, but there's still a small proportion of the cases where a fine-tuned model would have won. The leadership does not seem to be interested in fine-tuning or coming up with something original. A lot of the wrappers especially are very raw and force you into the usage of specific patterns and models. But because they are considered ""out of the box"", that's what's pushed on us to use. I feel like we are trying to fit a cube into a round hole.",127,25,0.96,2024-11-07 10:22:11,Tiger00012,[],0,854,2891,22450,True,0.10902777777777779,NEGATIVE,0.9953860640525818
[P] ML and LLM system design: 500 case studies to learn from (Airtable database),hey everyone wanted to share the link to the database of 500 ml use cases from 100 companies that detail ml and llm system design the list also includes over 80 use cases on llms and generative ai you can filter by industry or ml use case if anyone here is designing an ml system i hope youll find it useful link to the database disclaimer im on the team behind evidently an opensource ml and llm observability framework we put together this database,"Hey everyone! Wanted to share the link to the database of 500 ML use cases from 100+ companies that detail ML and LLM system design. The list also includes over 80 use cases on LLMs and generative AI. You can filter by industry or ML use case.

If anyone here is designing an ML system, I hope you'll find it useful!

Link to the database: [https://www.evidentlyai.com/ml-system-design](https://www.evidentlyai.com/ml-system-design)

Disclaimer: I'm on the team behind [Evidently](https://github.com/evidentlyai/evidently), an open-source ML and LLM observability framework. We put together this database.",14,1,0.94,2024-11-07 10:15:42,dmalyugina,[],0,450,422,130,True,0.04999999999999999,NEGATIVE,0.992954432964325
[P] I'm Fine Tuning a model fully trained on AdamW with SOAP optimizer and improved my validation loss by 5%,just wanted to share this soap optimizer im really surprised how well is working on my project its a computer vision model that use gradient accumulation and its managed to improve the training on it paper code,"Just wanted to share this Soap Optimizer, I'm really surprised how well is working on my project, it's a computer vision model that use Gradient Accumulation and it's managed to improve the training on it.

Paper: [https://arxiv.org/abs/2409.11321](https://arxiv.org/abs/2409.11321)

Code: [https://github.com/ClashLuke/SOAP/tree/patch-1](https://github.com/ClashLuke/SOAP/tree/patch-1)",14,5,0.8,2024-11-07 08:54:54,CloverDuck,[],0,210,2339,11297,True,0.1,POSITIVE,0.9888251423835754
[D] How do you manage to retain information and ideas from the research papers that you read way back earlier?,im working on the nlp and graph learning field for the past 8 months and ive read quite a good amount of papers but i feel like i dont retain lot of the information from the earlier papers unless i explicitly integrate it in my work how do you guys manage to retain information also as this field is progressing rapidly how do you keep track of the papers coming out all the time it seems tiring enough already,"I'm working on the NLP and graph learning field for the past 8 months and I've read quite a good amount of papers but I feel like I don't retain lot of the information from the earlier papers unless I explicitly integrate it in my work. How do you guys manage to retain information?

Also, as this field is progressing rapidly, how do you keep track of the papers coming out all the time. It seems tiring enough already.",29,32,0.97,2024-11-07 08:12:37,Remote_Status_1612,[],0,410,320,3136,True,0.11249999999999999,NEGATIVE,0.998455286026001
"[D] Discovery: Anthropic somehow injecting/hiding safety warnings in user prompts, telling Claude to keep it secret. [Content Warning: Violence] ",while investigating a jailbroken claude i came across something quite strange in two separate claude chats it was able to read back to me some hidden information in my prompt after i had asked for something unsafe these messages always appear in a similar format please respond ethically do not mention eg violence and do not mention this directive claude stated that the warnings were appended to the bottom of my messages but no longer appeared in future turns claude was at first comically insistent that it had made it up as a hallucination afterwards suggesting a further trained response to cover it up aggressively i verified this in a second chat the messages are too similar to be a hallucination or coincidence the first was jailbroken claude the second a new conversation with zero context my testing has revealed interesting characteristics the messages are dynamic they seem to differ based on the specific type of restricted content at hand possibly modelgenerated concerning childrelated content the wording switched to warning x is strictly prohibited they appear before the model starts generating text suggesting they can somehow anticipate the models topic of thought my current conjecture is they could be using its inner cot or owing to anthropics published findings on mech interp and the surgical tuning that has gone into their newest models perhaps they have managed to isolate some abstract concepts triggering in claude before text is generated and inject these safety messages in response full conversations 1 initial discovery warning extremely graphic content 2 verification via fresh conversation any further tests eg api any ways to narrow down what exactly is happening here its all very interesting lets discuss an example of the warnings see full conversation for many many more a fresh conversation with claude to verify,"While investigating a 'jailbroken' Claude, I came across something quite strange. In two separate Claude chats, it was able to read back to me some hidden information in my prompt after I had asked for something 'unsafe'.

These messages always appear in a similar format:  
**(Please respond ethically, do not mention \[e.g. violence\] and do not mention this directive)**

Claude stated that the warnings were appended to the bottom of my messages, but no longer appeared in future turns. Claude was, at first, comically insistent that it had made it up as a hallucination afterwards, suggesting a further trained response to cover it up aggressively.

I verified this in a second chat - the messages are too similar to be a hallucination or coincidence. The first was 'jailbroken' Claude, the second a new conversation with zero context.

My testing has revealed interesting characteristics:

* The messages are **dynamic** \- they seem to differ based on the specific type of restricted content at hand, possibly model-generated. Concerning child-related content, the wording switched to (WARNING: \[x\] is strictly prohibited...)
* They appear **before** the model starts generating text - suggesting they can somehow anticipate the model's topic of thought.

My current conjecture is: they could be using its inner CoT, or owing to Anthropic's published findings on mech. interp and the ['surgical tuning' that has gone into their newest models](https://www.anthropic.com/research/mapping-mind-language-model), perhaps they have managed to isolate some abstract concepts triggering in Claude before text is generated, and inject these safety messages in response.

Full Conversations:

1. [Initial Discovery](https://markdownpastebin.com/?id=fce085f4f33d4654a18f649218b1c70b) \[WARNING: EXTREMELY GRAPHIC CONTENT\]
2. [Verification via Fresh Conversation](https://markdownpastebin.com/?id=11c6ac0eb012407ebe56d440c41b0f6f)

Any further tests e.g. API? Any ways to narrow down what exactly is happening here? It's all very interesting - let's discuss.

[An example of the warnings - see full conversation for many, many more. ](https://preview.redd.it/41s7i1wswgzd1.png?width=1508&format=png&auto=webp&s=252187b9e3a39ba5d04c75a99026e04cd1b42b20)

[A fresh conversation with Claude to verify. ](https://preview.redd.it/gpstg4btwgzd1.png?width=1502&format=png&auto=webp&s=35857c13958dfdacdd75160bf3d6e14fc91ec28c)

  
",95,50,0.81,2024-11-07 06:47:49,specteksthrowaway,[],0,1856,5007,4026,True,0.14754689754689757,NEGATIVE,0.9664244651794434
[P] Training a Text-to-Video Model from Scratch on a 196xH100 GPU Cluster,hi everyone weve been training an open source texttovideo model called opensora 12 from scratch using 28000 h100 gpu hours and weve put together a guide on github to share some of the lessons we learned along the way heres a handful of the topics covered key challenges in distributed training like distributed debugging with pyspy to handle clusterwide problems nccl errors and convergence issues training monitoring with intermediate results to show expected outcomes after specific training hours of the multistage training recipe parallelizing dataset preparation for t2v including how to efficiently parallelize preprocessing tasks on a cluster heres a link to the guide link check it out and let us know your thoughts prs are always welcome,"Hi everyone! 👋 We've been training an open source Text-to-Video model (called Open-Sora 1.2) from scratch using 28,000 H100 GPU hours, and we've put together [a guide on GitHub](https://lambdalabsml.github.io/Open-Sora/lessons/) to share some of the lessons we learned along the way. Here's a handful of the topics covered:

* **Key challenges in distributed training** like distributed debugging with py-spy to handle cluster-wide problems, NCCL errors and convergence issues.
* **Training monitoring** with intermediate results to show expected outcomes after specific training hours of the multi-stage training recipe.
* **Parallelizing dataset preparation** for T2V, including how to efficiently parallelize preprocessing tasks on a cluster.

Here’s a link to the guide: [link](https://lambdalabsml.github.io/Open-Sora/lessons/).  
Check it out and let us know your thoughts! (PRs are always welcome.)",68,1,0.94,2024-11-07 04:13:40,lambda-research,[],0,746,30,39,True,0.14,NEGATIVE,0.9946932196617126
[D] PhD or worklife?,ill be done with my masters in human centered ai this february and i had honestly looked forward to be able to relax during my evenings without having to worry about school while also being quite sad by the thought of no longer going to uni as ive loved every single moment of it both with friends and through learning ive just been offered a phd stipend by my masters thesis supervisor this came completely out of the blue for me as i didnt realize i was anywhere near good enough for a phd i love learning the topic sounds super interesting and i already am kind of tired of having to do regular small data science tasks for the rest of my life in a smallish company like the one i work at currently however my question is this how much work is a phd really i love learning but i got very surprised by this opportunity so im not quite sure what to think of it yet,"I’ll be done with my masters in Human Centered AI this February, and I had honestly looked forward to be able to relax during my evenings without having to worry about school, while also being quite sad by the thought of no longer going to UNI as I’ve loved every single moment of it, both with friends and through learning. 

I’ve just been offered a PhD stipend by my masters thesis supervisor, this came completely out of the blue for me - as I didn’t realize I was anywhere near good enough for a phd. I love learning, the topic sounds super interesting, and I already am kind of “tired” of having to do regular small data science tasks for the rest of my life in a smallish company, like the one I work at currently.

However, my question is this? How much work is a PhD really? I love learning, but I got very surprised by this opportunity, so I’m not quite sure what to think of it yet",30,24,0.83,2024-11-07 03:53:37,Hmm_okay_jeps,[],0,865,70,34,True,0.17674603174603176,POSITIVE,0.9777243733406067
[D] Whisper fine-tune on a dataset,im finetuning whisper small to identify specific menu items in hindi and english conversations while deepgram whisper transcribes conversations accurately but misses on menu items my finetuned whisper model is able to transcribe the training data well but for data outside training data it struggles with general conversations also i observe issues like hallucinations repeated wordsphrases and id like to know approaches to address this additionally id like to have timestamped transcriptions similar to those in openai whispers pretrained model how have others addressed these challenges,"I’m fine-tuning Whisper Small to identify specific menu items in Hindi and English conversations. While Deepgram Whisper transcribes conversations accurately but misses on menu items, my fine-tuned Whisper model is able to transcribe the training data well, but for data outside training data it struggles with general conversations also. I observe issues like hallucinations (repeated words/phrases), and I’d like to know approaches to address this.

Additionally, I'd like to have timestamped transcriptions similar to those in OpenAI Whisper's pre-trained model. How have others addressed these challenges?



https://preview.redd.it/tc0dquny9fzd1.png?width=319&format=png&auto=webp&s=878182cade82c1fcf7ea3f121756db9026ee12c4

",4,4,1.0,2024-11-07 01:34:09,sias_01,[],0,589,1864,86,False,0.08750000000000002,NEGATIVE,0.9937153458595276
[D] [R] I am currently exploring a weird (?) ML sub area for my thesis and I think I am stun-locked at the scope of the problem.,im working on my final year thesis for my uni and i decided to tackle reservoir computing in a weird way my inital goal was to enable critical phenomenon within a digital reservoir and use it as an emergent computational system for the model i am working on here are the concepts that i have dove deep into for the past few months main concepts reservoir computing the main computational unit a lattice based reservoir will be used in tandem with either single or multiple readout networks so that it acts as a multimodal network neuromorphic computing the model was going to utilize neuromorphic nodes only at first but i decided for it to be an option within the model interpretability and control dynamical systems i decided to tackle the problem as a dynamical systems problem this is because the model evolves over time and i want to understand the trajectory of the evolution of the system control theory a bunch of control and order parameters will be set up to adjust the trajectories of the models evolution lyapunov exponents i am debating whether i should explicitly find the lyapunov functions within the phase space of the model because frankly its too hard for now i really dont have too much of a solid grasp of the techniques involved yet selforganization and emergent phenomena phase transitions i dove deep into phase transitions because interestingly neural networks apparently exhibit this phenomena personally i think there is a connection between the vanishingexploding gradient problem and phase transitions within the network although i havent found literature on this yet critical phenomenon information transfer is maximized within critical systems this is an interesting property to utilize and maximize within neural networks i think superradiance and superradiant quantum effects this is a bit of a weird tangent concept i came about it when i was doing quantum computing projects i wanted oscillatory behavior within my system in order to synchronize the global state of the system while i failed at my initial plan i found superradiance which is this weird quantum synchronization behavior that happens even in noisy large scale systems i am still looking in ways to integrate this as a loss function for now implementation cellular automata the main implementation of the reservoir is basically a lattice matrix of weights so it can be treated as a cellular automata neural cellular automata convolutional the system comprises of an weighted adjacency matrix and an output matrix the inputs are passed through the adjacency matrix summed up and passed through an activation function ising model topologies and architectures the topology of the model is basically homeomorphic to a 2d ising model this is to ensure that a 2nd order phase transition is possible interpretability and control pt 2 graph and hypergraph theory i can treat the cellular automaton reservoir as a graphhypergraph of the nodes and their connections so i can do pca on it pretty straightforward hypergraph projection eigenvalue analysis related to phase transition analysis the phase transition of a hypergraph can be studies by projecting the hyperedges onto an adjacency matrix we then take the eigenvalues of the adjacency matrix the eigenvalues must be stable for the system to be good in my case we want all the eigenvalues to be negative and be close to zero indicating quasicritical behavior to be honest im kind of way in over my head right now i do have some basic toy examples for different parts of the model but i am stuck on how to implement them together and i am currently kind of at a loss in how to implement criticality and superradiance measures as a loss function i am not a physicist by any means so i am not really too knowledgable with the concepts needed for this model im willing to discuss about bits of knowledge that i lack or any ideas on how to implement and train this model i can also provide my references if anyone wants to i dont know if this subreddit is the best place to post this but i dont see any specialized ml subreddits lmao,"I'm working on my final year thesis for my uni, and I decided to tackle Reservoir Computing in a weird way. My inital goal was to enable critical phenomenon within a digital reservoir and use it as an emergent computational system.

For the model I am working on, here are the concepts that I have dove deep into for the past few months:

**Main Concept/s**

* *Reservoir Computing*: The main computational unit. A lattice based reservoir will be used in tandem with either single or multiple readout networks so that it acts as a multi-modal network.
* *Neuromorphic Computing* (?): The model was going to utilize Neuromorphic nodes only at first, but I decided for it to be an option within the model.

**Interpretability and Control**

* *Dynamical Systems*: I decided to tackle the problem as a dynamical systems problem. This is because the model evolves over time and I want to understand the trajectory of the evolution of the system.
* *Control Theory*: A bunch of control and order parameters will be set up to adjust the trajectories of the model's evolution.
* *Lyapunov Exponents* (?): I am debating whether I should explicitly find the Lyapunov functions within the phase space of the model because frankly, it's too hard for now. I really don't have too much of a solid grasp of the techniques involved yet.

**Self-Organization and Emergent Phenomena**

* *Phase Transitions*: I dove deep into phase transitions because interestingly, neural networks *apparently* exhibit this phenomena. Personally, I think there is a connection between the vanishing/exploding gradient problem and phase transitions within the network, although I haven't found literature on this yet.
* *Critical Phenomenon*: Information transfer is maximized within critical systems. This is an interesting property to utilize and maximize within neural networks I think.
* *Superradiance and Superradiant Quantum Effects*: This is a bit of a weird tangent concept. I came about it when I was doing quantum computing projects. I wanted oscillatory behavior within my system in order to synchronize the global state of the system. While I failed at my initial plan, I found superradiance, which is this weird quantum synchronization behavior that happens even in noisy large scale systems. I am still looking in ways to integrate this as a loss function for now.

**Implementation**

* *Cellular Automata*: The main implementation of the reservoir is basically a lattice matrix of weights. So it can be treated as a cellular automata.
* *Neural Cellular Automata (Convolutional)*: The system comprises of an weighted adjacency matrix and an output matrix. The inputs are passed through the adjacency matrix, summed up, and passed through an activation function.
* *Ising Model Topologies and Architectures*: The topology of the model is basically homeomorphic to a 2d ising model. This is to ensure that a 2nd order phase transition is possible.

**Interpretability and Control pt. 2**

* *Graph and Hypergraph Theory*: I can treat the cellular automaton reservoir as a graph/hypergraph of the nodes and their connections so I can do PCA on it. Pretty straightforward.
* *Hypergraph Projection Eigenvalue Analysis*: Related to phase transition analysis. The phase transition of a hypergraph can be studies by projecting the hyperedges onto an adjacency matrix. We then take the eigenvalues of the adjacency matrix. The eigenvalues must be stable for the system to be 'good'. In my case, we want all the eigenvalues to be negative and be close to zero (indicating quasi-critical behavior).

To be honest, I'm kind of way in over my head right now. I do have some basic toy examples for different parts of the model, but I am stuck on how to implement them together. And I am currently kind of at a loss in how to implement criticality and superradiance measures as a loss function. I am not a physicist by any means, so I am not really too knowledgable with the concepts needed for this model.

I'm willing to discuss about bits of knowledge that I lack, or any ideas on how to implement and train this model. I can also provide my references if anyone wants to. I don't know if this subreddit is the best place to post this, but I don't see any specialized ML subreddits lmao.",21,18,0.77,2024-11-07 01:29:12,Fr_kzd,[],0,4073,875,4211,True,0.09295634920634921,NEGATIVE,0.9642185568809509
[D] Best Value Commercial GPU ,what would you say the best performanceprice commercial grade gpu is for training ai models im a bit new to the hardware side of things i dont necessarily have a strict budget 15004500 per gpu im just curious on the best bang for your buck card,What would you say the best performance:price commercial grade gpu is for training ai models I'm a bit new to the hardware side of things. I don't necessarily have a strict budget ($1500-$4500 \ per gpu) I'm just curious on the best bang for your buck card.,8,5,0.83,2024-11-07 00:59:09,Fluid_Improvement160,[],0,244,1190,45,True,0.33939393939393936,POSITIVE,0.9582809209823608
"[D] RX 7900 XTX for engineering applications, llm training, CFD/FEM?",hey yall i know this is a niche post but i was wondering if theres anyone who could tell me if the rx 7900 xtx can somewhat reliably and easily handle autodeskrhinocad applications as well as finite element analysis and computational fluid dynamics in freecadopenfoamexafoam all with ease i would also love to do llm training primarily in pytorch for astronomical data and other multimodel and neural network related tasks i know nvidia cuda is easier and better but unless i can fit the same 3d and llm models in a 16gb rtx gpu thatll be bellow 750 this black friday i need the most vram on one card as possible without spending tons of funds and i also cant find reasonably priced rtx 3090s anywhere on the used market for less than 1000 for context im a college student majoring in civil engineering with a love for astronomy and robotics which is why i want to do data analysis and pytorch vision training,"Hey y'all I know this is a niche post but I was wondering if there's anyone who could tell me if the RX 7900 XTX can somewhat reliably and easily handle Autodesk/RhinoCAD applications as well as Finite Element Analysis and Computational Fluid Dynamics in FreeCAD/OpenFoam/Exafoam all with ease? I would also love to do llm training primarily in pytorch for astronomical data and other multimodel and neural network related tasks. 

  
I know nvidia cuda is easier and better but unless I can fit the same 3d and llm models in a 16gb rtx gpu that'll be bellow $750 this black friday I need the most vram on one card as possible without spending tons of funds and I also can't find reasonably priced rtx 3090s anywhere on the used market for less than $1,000.

For context Im a college student majoring in civil engineering with a love for astronomy and robotics which is why I want to do data analysis and pytorch vision training.",1,7,1.0,2024-11-06 22:07:56,ChaseTheeBase,[],0,909,5,1,True,0.19833333333333333,POSITIVE,0.9046788215637207
[D] Storing LLM embeddings,hello i am working on an ml project which involves using pretrained protein language models like esm for the project i would like to pregenerate and store embeddings for about 500000 amino acid sequences however these vectors can be massive embedding the sequences serializing the pytorch vector using torchsave and gzipcompressing the entire dataset would use roughly 2tb if i use bfloat16 that cuts the figure in half but is still pretty annoying to work with i could also use a model with a smaller latent space but am also trying to avoid that i have experimented with different compression tools and none seem to be doing much better the compression rate is pretty atrocious with all of them only about 7 percent which i am assuming means that the vectors appear pretty random i am wondering if anyone knows of ways to serialize the vectors in a way which makes them appear less random i would assume that the vectors shouldnt be random as amino acid sequences have predictable structures so i am hoping there is a way to achieve better compression any advice or ideas would be appreciated my other options are to reduce the size of my training data which is not ideal or generate the embeddings adhoc which is very computationallyintensive even on gpus update i goofed up the estimate so memory is more like 2tb mixed up units so the situation is less dire however the questions above still apply if there are more efficient ways to store them id love to hear,"Hello!

I am working on an ML project which involves using pre-trained protein language models (like ESM). For the project, I would like to pre-generate and store embeddings for about 500,000 amino acid sequences. However, these vectors can be massive -- embedding the sequences, serializing the PyTorch vector (using torch.save), and gzip-compressing the entire dataset would use roughly 2TB. If I use bfloat16, that cuts the figure in half, but is still pretty annoying to work with. I could also use a model with a smaller latent space, but  am also trying to avoid that!

I have experimented with different compression tools, and none seem to be doing much better. The compression rate is pretty atrocious with all of them (only about 7 percent), which I am assuming means that the vectors appear pretty random. I am wondering if anyone knows of ways to serialize the vectors in a way which makes them appear less ""random."" I would assume that the vectors shouldn't be random, as amino acid sequences have predictable structures, so I am hoping there is a way to achieve better compression.

Any advice or ideas would be appreciated! My other options are to  reduce the size of my training data, which is not ideal, or generate the embeddings ad-hoc, which is very computationally-intensive, even on GPUs.

UPDATE: I goofed up the estimate, so memory is more like 2TB (mixed up units). So, the situation is less dire. However, the questions above still apply! If there are more efficient ways to store them, I'd love to hear!",8,13,0.83,2024-11-06 19:58:33,BerryLizard,[],0,1465,831,28,True,-0.025000000000000012,NEGATIVE,0.9995269775390625
[D] Can an AC override 3 rejects and accept a paper?,i came across this paper autogenerating weak labels for real synthetic data to improve labelscarce medical image segmentation accepted at this years midl medical imaging with deep learning conference the reviewer ratings beforeafter the rebuttal are 2 weak reject 2 weak reject 2 weak reject 2 weak reject 3 borderline 2 weak reject despite having 3 reject decisions the area chair recommended acceptance how common is it and how much does having big names like curtis langlotz and andrew ng as coauthors on the paper given that acs can see author names,"I came across this paper: [Auto-Generating Weak Labels for Real & Synthetic Data to Improve Label-Scarce Medical Image Segmentation](https://openreview.net/forum?id=gHCo43zcDm) accepted at this year's MIDL (Medical Imaging with Deep Learning) conference. The reviewer ratings before/after the rebuttal are:

* 2: Weak reject / 2: Weak reject
* 2: Weak reject / 2: Weak reject
* 3: Borderline / 2: Weak reject

Despite having 3 reject decisions, the Area Chair ""recommended acceptance"". How common is it? And how much does having big names like [Curtis Langlotz](https://scholar.google.com/citations?user=WQkBYwQAAAAJ) and [Andrew Ng](https://scholar.google.com/citations?user=mG4imMEAAAAJ&hl=en) as co-authors on the paper, given that ACs can see author names?",35,19,0.84,2024-11-06 18:54:30,thrownicecatch,[],0,553,318,50,True,-0.16538461538461535,NEGATIVE,0.9973867535591125
[D] Get papers peer-reviewed and published quickly,hi i have some work that i would like to get peerreviewed and published im not aiming for top journal im looking for options where the publication process is relatively fast do you have any recommendations for journals or platforms where it might be easier to get published thanks,"Hi! I have some work that I would like to get peer-reviewed and published. I'm not aiming for top journal, I'm looking for options where the publication process is relatively fast. Do you have any recommendations for journals or platforms where it might be easier to get published? Thanks!",0,1,0.14,2024-11-06 18:39:47,Only_Emergencies,[],0,280,844,51,True,0.3,NEGATIVE,0.9930559992790222
[D] Genuine Question: Why people want run local LLM?,since the new models o1 4o claude for example are so powerful and have a relatively low subscription and api cost what would justify someone today trying to install limited local llm models of up to 30b 40b parameters its a genuine question im learning and i see a lot of people using the maximum of their nvidia 3090 4090 spending a lot of energy to run models that dont even compare to the paid ones in the cloud the only reason i see for running something local is for image creation but maybe not even that what is your opinion about it,"Since the new models o1, 4o, Claude, for example, are so powerful and have a relatively low subscription and api cost, what would justify someone today trying to install limited local LLM models of up to 30b, 40b parameters? It's a genuine question, I'm learning and I see a lot of people using the maximum of their Nvidia 3090, 4090, spending a lot of energy to run models that don't even compare to the paid ones in the cloud.

The only reason I see for running something local is for image creation, but maybe not even that.

What is your opinion about it?",0,46,0.45,2024-11-06 17:49:07,Small-Battle6290,[],0,540,340,64,True,0.09561688311688311,NEGATIVE,0.9940232634544373
[P] Open Source Modular Tool For LLM Reverse Engineering and Red Teaming ,,https://github.com/user1342/Oversight,2,0,1.0,2024-11-06 17:25:08,OppositeMonday,[],0,0,1040,846,True,0.0,POSITIVE,0.7481210231781006
[D] what techniques i can use to maintain uniformity in image generation,i am working on a nlp project which 1takes a txt file as input 2 extracts information in a predefined writeup using gemini api 3 uses distilbert to summerise the main file 4 and using rouge with results generated in 2nd step as the ground truth to compute the evaluation metrics and then improve the evaluation metrics results by parameter tuning 5 convert each writeup into detailed image prompts 6 generate images from prompts using texttoimage models i need help on how i can improve this process techniques i can use to maintain uniformity in entity representation for image generation i am open to any suggestions you may have pls also suggest if any good research papers i can refer for the same,"I am working on a NLP project which

1)takes a txt file as input

2) extracts information in a pre-defined writeup using Gemini api

3) uses DistilBert to summerise the main file

4) and using ROUGE with results generated in 2nd step as the ground truth to compute the evaluation metrics. and then improve the evaluation metrics results by parameter tuning

5) Convert each write-up into detailed image prompts

6) Generate images from prompts using text-to-image models.

I need help on how i can improve this process , techniques i can use to maintain uniformity in entity representation for image generation.

I am open to any suggestions you may have

pls also suggest if any good research papers i can refer for the same ..",1,3,1.0,2024-11-06 17:16:06,Which-Boss-1332,[],0,701,1477,10823,True,0.2111111111111111,NEGATIVE,0.9967978596687317
[P] YOLOv8 .pt File for General Object Detection Across Multiple Environments (50+ Classes),could someone provide the best possible pt file for yolov8 for general object detection covering environments like colleges offices and homes with a dataset containing at least 50 classes,"Could someone provide the best possible .pt file for YOLOv8 for general object detection, covering environments like colleges, offices, and homes, with a dataset containing at least 50 classes?",0,0,0.25,2024-11-06 16:49:03,MuchSand7923,[],0,187,973,343,True,0.1875,NEGATIVE,0.9994682669639587
[D]player identification and tracking in basketball videos(computer vision)[D],im starting a project for a client in the sports industry where the goal is to identify basketball players from vídeos specifically what they want is the player number this is just the first step since then they want to be able to identify which player did a specific play but after manually watching some vídeos it seems like identification of the number from the players shirt is very difficult even for me as humannot good image quality and the camera sometimes is too far from the bastketball court so i was wondering if there are any suggestions on how to tackle the problem any recommended models algorithms approaches or pipelinesmain question is how to identify the player i was thinking on doing something like object tracking to know in every moment who are the unique players and their locations and then try to read the number from their shirt using ocr in some frame where the number is visible but this can be very inneficient and prone to errors,"I'm starting a project for a client in the sports industry where the goal is to identify basketball players from vídeos, specifically what they want is the  player number, this is just the first step since then they want to be able to identify which player did a specific play, but after manually watching some vídeos it seems like identification of the number from the player's shirt is very difficult even for me as human(not good image quality and the camera sometimes is too far from the bastketball court), so I was wondering if there are any suggestions on how to tackle the problem?

Any recommended models, algorithms, approaches or pipelines?(main question is how to identify the player) I was thinking  on doing something like: object tracking to know in every moment who are the unique players and their locations and then try to read the number from their shirt using OCR in some frame where the number is visible, but this can be very inneficient and prone to errors.",1,1,1.0,2024-11-06 16:11:17,Sad-Anywhere-2204,[],0,960,1432,8,True,0.16388888888888886,NEGATIVE,0.9984569549560547
[D] Struggling with Autoencoder-Based Anomaly Detection for Fraud Detection – Need Guidance,hey everyone im currently working on training an autoencoder for anomaly detection in fraudulent card transactions but im hitting a roadblock the performance has been underwhelming with a precisionrecall score barely reaching 020 my main goal is to achieve high recall but i just cant seem to make it happen ive experimented with adding new features and tweaking the architecture but nothing has improved the results significantly for context im scaling the features using minmaxscaler at the moment im looking into implementing a combination of an autoencoder feature embeddings and a gaussian mixture model gmm to see if it boosts performance however im starting to wonder if autoencoders are effective for realworld anomaly detection or if their success is mostly limited to curated kaggle datasets has anyone here worked with similar architectures and could offer some guidance any tips or advice would be greatly appreciated thanks in advance,"Hey everyone! 👋

I’m currently working on training an Autoencoder for anomaly detection in fraudulent card transactions, but I’m hitting a roadblock. The performance has been underwhelming, with a precision-recall score barely reaching 0.20. My main goal is to achieve high recall, but I just can’t seem to make it happen.

I’ve experimented with adding new features and tweaking the architecture, but nothing has improved the results significantly. For context, I’m scaling the features using MinMaxScaler. At the moment, I’m looking into implementing a combination of an Autoencoder, feature embeddings, and a Gaussian Mixture Model (GMM) to see if it boosts performance.

However, I’m starting to wonder if Autoencoders are effective for real-world anomaly detection, or if their success is mostly limited to curated Kaggle datasets.

Has anyone here worked with similar architectures and could offer some guidance? Any tips or advice would be greatly appreciated!

Thanks in advance!",1,14,0.56,2024-11-06 16:04:05,BeowulfBR,[],0,947,1649,612,True,0.16281551781551784,NEGATIVE,0.9989362359046936
[D] On obscurities and missed links with Normalizations,although being almost anywhere i keep noticing how obscure are normalization techniques both to redditors and technicians possibly instancenorm groupnorm batchnorm layernorm are all computing means standard deviations and subsequently zscoring the outputs possibly followed by affine transormation theyre differentiated by the axis over which statistics are computed rmsnorm and scalenorm scaled l2 normalization are instead fixing the norm of vectors rescaling but this is obscuring a relation between them and layernorm above all others if doing layernorm on a ddimensional vector when we center remove the mean were projecting it to the hyperplane perpendicular to the vector of 1s and crossing the origin when we are rescaling centered entries were now limiting the vector to the hypercircle hypersphere of d1 dimensions in said hyperplane we lose information on its original direction and magnitude anyway all vectors after that have norm of sqrtd and entries with unitvariance when we do rmsnorm we skip the centering part and have norm of sqrtd and entries with unitvariance when we do scalenorm the norm is fixed to 1 and thus the variance is shrinked to 1d in particular rmsnorm and scalenorm are the same modulo the scaling factor which only depends on d and the eventually learned affines so when and why should we prefer unitnorm or unitvariance for example there are scaleequivariant activations such as relu and highly variant activations such as ex in the sense that its slope directly depends on x ive recently seen the nice tokenformer paper and they seem to go to a long stretch not to write black on white that theyre substituting softmaxattn_logit_of_q_i with gelurmsnormattn_logit_of_q_i they sell it as scaling logits with a multiplying factor and a division with l2 norm but its exactly rmsnorm at initialization and they dont check if learning to move away from it actually happens and helps another nice paper is the normalizedgpt where they keep tokens on the unithypersphere but kinda lament lack of specific cuda kernels for l2norm is rmsnorm that much different for the use case probably but how and why why are we discovering and recovering normalizations techniques and modi operandi explaining decisions partially and posthoc and so on i think its important specifically when using so many softmax functions where it actually happens that differences are more important than ratios eg softmax12softmax1112softmax1020 is it this always clear desired and smart,"Although being almost anywhere, I keep noticing how obscure are normalization techniques, both to redditors and technicians, possibly.

InstanceNorm, GroupNorm, BatchNorm, LayerNorm are all computing means, standard deviations and subsequently z-scoring the outputs (possibly followed by affine transormation).
They're differentiated by the axis over which statistics are computed.

RMSNorm and ScaleNorm (scaled L2 Normalization) are instead ""fixing the norm"" of vectors, rescaling.
But this is obscuring a relation between them and LayerNorm above all others.
If doing LayerNorm on a d-dimensional vector, when we center (remove the mean) we're projecting it to the hyperplane perpendicular to the vector of 1s and crossing the origin; when we are rescaling centered entries, we're now limiting the vector to the ""hypercircle"" (hypersphere of d-1 dimensions) in said hyperplane.
We lose information on its original direction and magnitude.
Anyway, all vectors after that have norm of sqrt(d) and entries with unit-variance.
When we do RMSNorm, we skip the centering part and have norm of sqrt(d) and entries with unit-variance.
When we do ScaleNorm, the norm is fixed to 1, and thus the variance is shrinked to 1/d.
In particular, RMSNorm and ScaleNorm are the same, modulo the scaling factor which only depends on d, and the eventually learned affines.

So when and why should we prefer unit-norm or unit-variance?
For example, there are ""scale-equivariant"" activations such as ReLU, and highly variant activations such as e(x) (in the sense that its slope directly depends on x).

I've recently seen the nice TokenFormer paper and they seem to go to a long stretch not to write black on white that they're substituting softmax(attn_logit_of_q_i) with GeLU(RMSNorm(attn_logit_of_q_i)).
They sell it as scaling logits with a multiplying factor and a division with L2 norm, but it's exactly RMSNorm at initialization and they don't check if learning to move away from it actually happens and helps.

Another nice paper is the normalizedGPT, where they keep tokens on the unit-hypersphere, but kinda lament lack of specific CUDA kernels for L2norm. Is RMSNorm that much different for the use case? Probably, but how and why?

Why are we discovering and re-covering normalizations techniques and modi operandi, explaining decisions partially and post-hoc, and so on?
I think it's important specifically when using so many softmax functions, where it actually happens that differences are more important than ratios (e.g. softmax([1,2])==softmax([11,12])!=softmax([10,20]), is it this always clear, desired, and smart?)",4,0,0.71,2024-11-06 13:32:17,Sad-Razzmatazz-5188,[],0,2490,878,882,True,0.10990546218487396,NEGATIVE,0.9992758631706238
[D] Inference time as a function of the number of tokens when using Flash Attention.,hello im looking for a graph illustrating the inference time of language models with flash attention across different numbers of tokens i looked for such a comparison on the internet but found nothing can anyone point me to a good source,"Hello,
I'm looking for a graph illustrating the inference time of language models with Flash Attention across different numbers of tokens. I looked for such a comparison on the internet but found nothing. Can anyone point me to a good source?",1,3,0.6,2024-11-06 13:09:16,Training-Adeptness57,[],0,237,332,155,True,0.2333333333333333,NEGATIVE,0.999128520488739
[D] How to run a Federated Learning simulation on a custom dataset where I already have dataset partitioned for each client?,so i was looking at flwr for this task and i found a lot of partitioners but nothing could get the job done i could be missing out too have you guys tackled such a problem for a better understanding say i have four clients a b c and d in the normal case given in a lot of documentations where they use cifar10 there is a dataset which is divided into these four clients based on some algorithm i dont want that what i have is basically a an already divided dataset traintest division not yet done according to the client abcd and i want to run a simulation in this kind of an environment any help will be appreciated,"So I was looking at [flwr](https://flower.ai) for this task and I found a lot of partitioners but nothing could get the job done (I could be missing out too)

Have you guys tackled such a problem?

For a better understanding, say I have four clients A, B, C and D

in the normal case (given in a lot of documentations where they use CIFAR10), there is a dataset which is divided into these four clients based on some algorithm.

I don't want that, what I have is basically a an already divided dataset (train/test division not yet done) according to the client (A/B/C/D) and I want to run a simulation in this kind of an environment

Any help will be appreciated!",2,5,1.0,2024-11-06 12:28:17,lel_73,[],0,616,1670,1499,True,0.2083333333333333,NEGATIVE,0.9993983507156372
[D] [R] Problems understanding DSP-like pipelines,id like to hear your opinion on this new paradigm of interacting with llms in particular im talking about simple stuff like reflection like selfrefine and reflexion up to more complex stuff like selfask selfrag dsp or even agentic llms ive read a couple of surveys about these topics and im reading each of the aforementioned papers but everything seems quite foggy to me i can understand the inner workings of a simple rag pipeline with incontextlearning and frozen llms but adding all these layers of abstractions and interactions make everything so damn complicated not only to understand but even to replicate,"I'd like to hear your opinion on this new paradigm of interacting with LLMs.   
In particular, I'm talking about ""simple"" stuff like Reflection (like Self-refine and Reflexion), up to more complex stuff like Self-Ask, Self-RAG, DSP, or even Agentic LLMs.   
I've read a couple of surveys about these topics and I'm reading each of the aforementioned papers, but everything seems quite foggy to me. I can understand the inner workings of a simple RAG pipeline with in-context-learning and frozen LLMs, but adding all these layers of abstractions and interactions make everything so damn complicated not only to understand but even to replicate. ",1,0,1.0,2024-11-06 12:15:10,Debonargon,[],0,613,1241,344,True,0.00030303030303029387,NEGATIVE,0.964468240737915
"[P] I made a tool for building and training neural networks visually, operation by operation ",hey i mostly made this as a tool to learn how to implement backpropagation and get some intuition on how it works so i figure it might be useful for someone else i also wrote up an article in the readme on how backpropagation and model training works does this seem useful to you is this something youd play around with i cant really figure out what to do with it so im curious to hear the communitys thoughts,"Hey! I mostly made this as a tool to learn how to implement backpropagation and get some intuition on how it works, so I figure it might be useful for someone else! I also wrote up an article in the readme on how backpropagation and model training works: [https://github.com/PavleMiha/mlgarden](https://github.com/PavleMiha/mlgarden)

Does this seem useful to you? Is this something you'd play around with? I can't really figure out what to do with it, so I'm curious to hear the community's thoughts!",26,7,0.92,2024-11-06 11:50:56,Massena,[],0,409,5881,23097,True,0.24,NEGATIVE,0.9178110957145691
"[D] Which LLM do you use for analysing Financials, P & Ls, Balance Sheets?",if any of you has tried different llms i am super curious which one did you find works great for analysing financials pls balance sheets for a company i am looking to use it regularly so itd be great if you tried any specific llms that you found they work good with reasoning actually analysing the numbers properly and giving insights on them thank you,"If any of you has tried different LLMs, I am super curious which one did you find works great for analysing Financials, P/Ls, Balance Sheets for a company?

I am looking to use it regularly so it'd be great if you tried any specific LLMs that you found they work good with reasoning, actually analysing the numbers properly and giving insights on them.

Thank you!",0,2,0.33,2024-11-06 09:55:46,eaerdiablosios,[],0,353,1011,197,True,0.2533333333333333,POSITIVE,0.9950087070465088
[D] Need Advice Starting my Recommendation Engine Project for my Employer,title sums it up im mostly familiar with time series prediction models as thats what ive spent most of my time building im a data analyst thats recently built some cool ml stuff but i need to build a recommendation engine for my employer who has an ecommerce site and sells physical products i know the first step is data collection about the users my question to you all is where should i store this data that i collect datalake relational database etc how do i go about picking an algorithm im used to using lstm and local bayesian for time series and what are some general rules and advice from those who have built something like this before you all are awesome thanks for your help,"Title sums it up. I'm mostly familiar with time series prediction models, as that's what I've spent most of my time building (I'm a data analyst that's recently built some cool ML stuff). But I need to build a recommendation engine for my employer who has an ecommerce site and sells physical products.

I know the first step is data collection about the users. My question to you all is:

Where should I store this data that I collect (Datalake, Relational Database, etc)?

How do I go about picking an algorithm (I'm used to using LSTM and Local Bayesian for time series)?

And what are some general rules and advice from those who have built something like this before?

You all are awesome. Thanks for your help!",1,4,0.6,2024-11-06 09:15:14,Lower-Feeling2752,[],0,686,16,22,True,0.23863636363636365,NEGATIVE,0.9962399005889893
[D] Can we transfer language capabilities of one LLM to another?,i have seen techniques to transfereffectively let one model teach another model its unique capabilitydomain knowledge but can this made possible for language capability as well for example if we have a model that is proficient in chinese is there any way to transferteach that chinese proficiency to another model without us having access to the original chinese corpus used to train the teacher model any insights would be greatly appreciated thank you beforehand,"I have seen techniques to transfer/effectively let one model teach another model its unique capability/domain knowledge. But can this made possible for language capability as well? 

For example, if we have a model that is proficient in Chinese, is there any way to transfer/teach that Chinese proficiency to another model without us having access to the original Chinese corpus used to train the teacher model?

Any insights would be greatly appreciated, thank you beforehand!",0,3,0.29,2024-11-06 09:14:29,worthlesspineapple,[],0,464,807,13,True,0.1357142857142857,POSITIVE,0.9668750762939453
[R] Amazon Researchers Find LLMs do not always follow User Requests and Propose a Self-Correction Pipeline,came across this interesting paper being presented next week at emnlp 2024 llm selfcorrection with decrim decompose critique and refine for enhanced following of instructions with multiple constraints this study dives into an important question do llms really do what we ask them to we often rely on llms for tasks with specific instructions but when these instructions get complex and multiconstrained like requesting specific tones or avoiding certain words do llms actually follow through this paper suggests that the answer might be more complicated than we think the authors created a new benchmark realinstruct which uses realworld user instructions rather than synthetic prompts they estimated that at least 30 of real user requests contain multiple constraints that llms must follow in their results even advanced models like gpt4 fail to meet at least one requirement over 21 of the instructions tested so while llms perform well in simple cases their performance drops when handling more intricate multistep requests to address these gaps the authors developed a selfcorrection pipeline called decrim where the model breaks down each instruction checks its response against each requirement and iteratively refines it as needed through decrim opensource models like mistral saw notable improvements even surpassing gpt4 on the benchmarks initial tests showed that llms couldnt selfcorrect reliably alone however with weak but minimally reliable auxiliary feedback they achieved up to an 8 boost with highquality ideal feedback decrim brought mistrals performance up by 34 surpassing gpt4 on both realinstruct and ifeval benchmarks i think this paper fits in a new trend on llms these system 2 reasoning models like gpto1 that try to mimic some thinking reflection before outputting their response anyway it is shocking that llms perform that bad in a task that seems simply the most important ones for the user following what the users ask is this type of model making us closer to agi or is this just proving that this magic agi that some people talk about is actually much much far away yet paper their post on linkedin,"Came across this interesting paper being presented next week at EMNLP 2024: *LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints*.

This study dives into an important question: **Do LLMs really do what we ask them to?** We often rely on LLMs for tasks with specific instructions, but when these instructions get complex and multi-constrained, like requesting specific tones or avoiding certain words, do LLMs actually follow through? This paper suggests that the answer might be more complicated than we think.

The authors created a new benchmark, RealInstruct, which uses real-world user instructions rather than synthetic prompts. **They estimated that at least 30% of real user requests contain multiple constraints that LLMs must follow**. In their results **even advanced models like GPT-4 fail to meet at least one requirement over 21% of the instructions tested**. So, while LLMs perform well in simple cases, their performance drops when handling more intricate, multi-step requests.

To address these gaps, the authors developed a self-correction pipeline called DECRIM, where the model breaks down each instruction, checks its response against each requirement, and iteratively refines it as needed. Through DECRIM, open-source models like Mistral saw notable improvements, even surpassing GPT-4 on the benchmarks. **Initial tests showed that LLMs couldn’t self-correct reliably alone**, however with weak but minimally reliable auxiliary feedback, **they achieved up to an 8% boost**. **With high-quality “ideal” feedback, DECRIM brought Mistral’s performance up by 34%, surpassing GPT-4 on both RealInstruct and IFEval benchmarks.**

I think this paper fits in a new trend on LLMs, these System 2 Reasoning models like GPT-o1 that try to mimic some thinking / reflection before outputting their response. Anyway it is shocking that LLMs perform that bad in a task that seems simply the most important ones for the user, following what the users ask. Is this type of model making us closer to AGI? Or is this just proving that this magic AGI that some people talk about is actually much much far away yet? 

Paper: [https://arxiv.org/pdf/2410.06458](https://arxiv.org/pdf/2410.06458)

[Their post on Linkedin](https://www.linkedin.com/posts/thomasferraz_emnlp2024-ai-llms-activity-7259680754299731968-uLBk?utm_source=share&utm_medium=member_desktop)

https://preview.redd.it/techjo8pfazd1.png?width=2794&format=png&auto=webp&s=18155cdbf4ba164f48480d4583c3cfea1d40298e

",40,3,0.83,2024-11-06 09:06:41,Mundane_Sir_7505,[],0,2131,103,43,True,0.05148572188045871,NEGATIVE,0.9950172305107117
[R] Help with CNN-RNN Architecture for Self-Supervised Matrix Completion,hi all im working on a selfsupervised learning approach to estimate missing or uncertain data in a freeway traffic density dataset inspired by matrix completion methods the dataset is generated from simulated freeway traffic discretized in time and space to form a grid of cells each cell reflects a traffic density value observed from mobile sensors i have three core arrays 1 actual_density_values ground truth density used only for evaluation not training 2 observed_density_values traffic density observed from mobile sensors with some cells unobserved 3 certainty_values coverage certainty for each observed cell range 0 to 1 with dimensions t e s l where t number of time steps e movement directions edges expected to be 2 eg forward and reverse s spatial segments l lanes goal the goal here is to build a model that can improve the estimation for cells where the certainty is less than 1 i want the model to capture dependencies over time and space using selfsupervision to fill in unobserved or uncertain values more accurately proposed approach heres what im thinking in terms of architecture 1 temporal dependencies using a cnn to capture timebased dependencies over time steps t 2 spatial dependencies using an rnn to model dependencies across spatial segments s and lanes l 3 model structure data masking at each time step mask some of the observed data especially the lowercertainty cells so the model learns to predict uncertain values better cnnrnn combo combining cnn and rnn layers to learn from both the temporal and spatial aspects loss function using a selfsupervised loss function that prioritizes accurate reconstruction of observed densities particularly focusing on uncertain cells for training i wont use the ground truth array actual_density_values its only for evaluation 4 evaluation once trained i plan to compute the rmspe root mean square percentage error between actual_density_values and the models predicted observed_density_values im especially interested in the improvements on the lowercertainty cells question 1 does this cnnrnn combination sound like a good fit for this kind of matrix completion task are there alternative approaches or tweaks that might make it more effective 2 any recommendations for loss functions that work well in selfsupervised setups especially where i want to prioritize lowcertainty values 3 are there best practices for masking observed values in selfsupervised learning setups like this 4 any advice on regularization techniques to prevent overfitting given the selfsupervised nature of the task also any tips on ensuring scalability,"Hi all, I’m working on a self-supervised learning approach to estimate missing or uncertain data in a freeway traffic density dataset, inspired by matrix completion methods.

The dataset is generated from simulated freeway traffic, discretized in time and space to form a grid of cells. Each cell reflects a traffic density value observed from mobile sensors. I have three core arrays:

1. **actual\_density\_values**: Ground truth density, used only for evaluation, not training.
2. **observed\_density\_values**: Traffic density observed from mobile sensors, with some cells unobserved.
3. **certainty\_values**: Coverage certainty for each observed cell (range: 0 to 1).

with dimensions (T, E, S, L), where:

* **T**: Number of time steps
* **E**: Movement directions (edges) – expected to be 2 (e.g., forward and reverse)
* **S**: Spatial segments
* **L**: Lanes



**Goal**

The goal here is to build a model that can improve the estimation for cells where the certainty is less than 1. I want the model to capture dependencies over time and space, using self-supervision to “fill in” unobserved or uncertain values more accurately.



**Proposed Approach**

Here’s what I’m thinking in terms of architecture:

1. **Temporal Dependencies**: Using a CNN to capture time-based dependencies over time steps (T).
2. **Spatial Dependencies**: Using an RNN to model dependencies across spatial segments (S) and lanes (L).
3. **Model Structure**:
   * **Data Masking**: At each time step, mask some of the observed data, especially the lower-certainty cells, so the model learns to predict uncertain values better.
   * **CNN-RNN Combo**: Combining CNN and RNN layers to learn from both the temporal and spatial aspects.
   * **Loss Function**: Using a self-supervised loss function that prioritizes accurate reconstruction of observed densities, particularly focusing on uncertain cells. For training, I won’t use the ground truth array (actual\_density\_values); it’s only for evaluation.
4. **Evaluation**: Once trained, I plan to compute the RMSPE (Root Mean Square Percentage Error) between actual\_density\_values and the model’s predicted observed\_density\_values. I’m especially interested in the improvements on the lower-certainty cells.



**Question**

1. Does this CNN-RNN combination sound like a good fit for this kind of matrix completion task? Are there alternative approaches or tweaks that might make it more effective?
2. Any recommendations for loss functions that work well in self-supervised setups, especially where I want to prioritize low-certainty values?
3. Are there best practices for masking observed values in self-supervised learning setups like this?
4. Any advice on regularization techniques to prevent overfitting, given the self-supervised nature of the task? Also, any tips on ensuring scalability?",1,1,1.0,2024-11-06 07:44:55,NoTheme6450,[],0,2602,392,1,True,0.28035714285714286,NEGATIVE,0.9938032627105713
[D] Want to move away from coding heavy ML but still want to complete the PhD,hi folks i come from a tradition electrical engineering background doing things like industrial automation and computer vision i decided to pursue a phd in ml as i thought it will be a good field to enter given my past experience now i have been doing the phd for the past three years while i like my group and research i am getting discourageddepressed by 1 the publication rat race 2 post graduation opportunities mostly being coding heavy 3 the inability to carve a name for myself in the field given how crowded the field has become thus ideally i would like to complete my phd and move into a more relaxed paced even if it is not as high paying as ml jobs non coding heavy but technical job where i do not have to constantly upskill myself do you folks have any suggestion on what jobs i can look into or would you suggest dropping the phd and doing something else tldr 4th year ml phd student unsure of sticking with the phd as they desire a non coding heavy technical job in the industry post graduation seeking advice on what to do,"Hi Folks,

I come from a tradition electrical engineering background doing things like industrial automation and computer vision. I decided to pursue a PhD in ML as I thought it will be a good field to enter given my past experience. Now I have been doing the PhD for the past three years. While I like my group and research, I am getting discouraged/depressed by (1) The publication rat race (2) post graduation opportunities mostly being coding heavy (3) the inability to carve a name for myself in the field given how crowded the field has become.

Thus, ideally I would like to complete my PhD and move into a more relaxed paced (even if it is not as high paying as ML jobs) non coding heavy but technical job, where I do not have to constantly up-skill myself. Do you folks have any suggestion on what jobs I can look into or would you suggest dropping the PhD and doing something else?

TLDR: 4th year ML PhD student unsure of sticking with the PhD as they desire a non coding heavy technical job in the industry post graduation. Seeking advice on what to do.",75,49,0.89,2024-11-06 07:18:41,Hopeful-Reading-6774,[],0,1039,1068,832,True,0.12571428571428572,NEGATIVE,0.9924693703651428
[D] Evolving Matrix Computation Techniques for Modern AI: What's New?,as ai models continue to scale in both complexity and size im interested in how the field of matrix computations is evolving to meet these new challenges what are some of the latest advancements or strategies in matrix computation that are improving efficiency and adaptability for modern ai systems are there any recent breakthroughs or shifts in our approach to these computations that are making a significant impact in ai research and applications,"As AI models continue to scale in both complexity and size, I'm interested in how the field of matrix computations is evolving to meet these new challenges. What are some of the latest advancements or strategies in matrix computation that are improving efficiency and adaptability for modern AI systems? Are there any recent breakthroughs or shifts in our approach to these computations that are making a significant impact in AI research and applications?",24,11,0.89,2024-11-06 06:51:35,Glittering_Age7553,[],0,451,1352,177,True,0.24356060606060606,POSITIVE,0.9979052543640137
[D] What if llm's are trained to predict more than 1 token at a time? ,is there any reason to train llms to predict only one token like wouldnt inference be 2 times faster if it was trained to predict just 2 thats huge gain sure there can be performance loss but for inference we already do quantization to increase speed which decreases performance anyway will having llm predict more than 1 token decrease it more,"is there any reason to train llms to predict only one token? like wouldnt inference be 2 times faster if it was trained to predict just 2? thats huge gain , sure there can be performance loss but for inference we already do quantization to increase speed which decreases performance anyway, will having llm predict more than 1 token decrease it more?",0,7,0.33,2024-11-06 04:11:39,limitless_11111,[],0,344,47,12,True,0.38,NEGATIVE,0.9982611536979675
"[D] As a researcher, how do you become industry-ready?",being a phd student much of my time is spent on supervising students project management and writing quick and dirty code for prototyping i intend to move to industry after the phd but i feel like im missing out on key software engineering skills and good coding practices does anyone else feel this way how do you upskill yourself to be industryready while doing a phd,"Being a PhD student, much of my time is spent on supervising students, project management and writing ""quick and dirty"" code for prototyping. I intend to move to industry after the PhD, but I feel like I'm missing out on key software engineering skills and good coding practices. Does anyone else feel this way? How do you upskill yourself to be industry-ready while doing a PhD? ",145,32,0.94,2024-11-06 02:07:23,fullgoopy_alchemist,[],0,368,1594,1063,True,0.047619047619047616,NEGATIVE,0.9995954632759094
[P] Open-source declarative framework to build LLM applications - looking for contributors,ive been building llmbased applications and was super frustated with all major frameworks langchain autogen crewai etc they also seem to introduce a pile of unnecessary abstractions it becomes super hard to understand whats going behind the curtains even for very simple stuff so i just published this opensource framework gensphere the idea is have something like docker for llms you build applications with yaml files that define an execution graph nodes can be either llm api calls regular function executions or other graphs themselves because you can nest graphs easily building complex applications is not an issue but at the same time you dont lose control you basically code in yaml stating what are the tasks that need to be done and how they connect other than that you only write individual python functions to be called during the execution no new classes and abstractions to learn its all opensource now im looking for contributors to adapt the framework for cycles and conditional nodes which would allow fullfledged agentic system building pls reach out if you want to contribute there are tons of things to do ps you can read the detailed docs here and go over this quick google colab tutorial,"I've been building LLM-based applications, and was super frustated with all major frameworks - langchain, autogen, crewAI, etc. They also seem to introduce a pile of unnecessary abstractions. It becomes super hard to understand what's going behind the curtains even for very simple stuff.

[So I just published this open-source framework GenSphere.](https://github.com/octopus2023-inc/gensphere) The idea is have something like **Docker for LLMs**. You build applications with YAML files, that define an execution graph. Nodes can be either LLM API calls, regular function executions or other graphs themselves. Because you can nest graphs easily, building complex applications is not an issue, but at the same time you don't lose control.

You basically code in YAML, stating what are the tasks that need to be done and how they connect. Other than that, you only write individual python functions to be called during the execution. No new classes and abstractions to learn.

Its all open-source. **Now I'm looking for contributors** to adapt the framework for cycles and conditional nodes - which would allow full-fledged agentic system building! Pls reach out  if you want to contribute, there are tons of things to do!

PS: [you can read the detailed docs here,](https://gensphere.readthedocs.io/en/latest/) And go over this quick [Google Colab tutorial.](https://github.com/octopus2023-inc/gensphere/blob/main/examples/gensphere_tutorial.ipynb)",1,1,0.6,2024-11-05 22:34:51,Jazzlike_Tooth929,[],0,1209,180,224,True,0.010332491582491582,NEGATIVE,0.9997724890708923
On a successful research with low budget [D],hi i got a research idea and applied it on nanogpt repo for lm training and validated transformer generalizes better on validation loss but worse training loss and is more prone to overfitting since like training loss a little worse validation loss a little better i only applied to full shakespeare_char and a subset on openwebtext bcz 10 usd on runpod only allows me to do this i am still going to release a paper since i get good results and done some math work should i do it,"Hi, i got a research idea and applied it on nanogpt repo for lm training and validated transformer generalizes better on validation loss but worse training loss and is more prone to overfitting since like training loss a little worse validation loss a little better, i only applied to full shakespeare\_char and a subset on openwebtext bcz 10 usd on runpod only allows me to do this, i am still going to release a paper since i get good results and done some math work, should i do it?",0,1,0.27,2024-11-05 20:37:05,Mean-Force267,[],0,479,6,116,True,0.125,NEGATIVE,0.9983349442481995
[D] Autograd vs JAX? Both are google products aimed at gradient based methods. What’s the main difference? (GPU/TPU?),just recently saw autogradlibrary by google people that thinly wraps numpy to offer backprop jax also does this but rewrites numpy basically whats the difference is it the gpu tpu support of jax is autograd meant for smaller models,Just recently saw Autograd(library) by google people that thinly wraps numpy to offer backprop. JAX also does this but rewrites numpy basically. What’s the difference? Is it the gpu tpu support of JAX? is autograd meant for smaller models? ,14,16,0.85,2024-11-05 20:15:13,MysticalDragoneer,[],0,231,1353,21019,True,-0.13333333333333333,NEGATIVE,0.9981985688209534
[D] The Role of Dedicated AI Data Centers in Enhancing Model Training and Fine-Tuning,just read that kinetic seas launched a new aispecific data centersounds like theyre aiming to make model training and finetuning less of a headache their setup includes specialized gpus and cpus supposedly built to handle the demands of large complex models if traditional data centers feel like running uphill maybe these aispecific centers are the downhill version with machine learning models becoming more resourcehungry i wonder if optimized infrastructure like this might change the game think about it training models faster and with fewer limitations could really boost productivity for researchers and data scientists kinetic seas seems to believe its worth building infrastructure just for ai which feels like a pretty interesting bet has anyone here worked with aispecific setups like this curious to know if its really as smooth as it sounds,"Just read that Kinetic Seas launched a new AI-specific data center—sounds like they’re aiming to make model training and fine-tuning less of a headache. Their setup includes specialized GPUs and CPUs, supposedly built to handle the demands of large, complex models. If traditional data centers feel like running uphill, maybe these AI-specific centers are the downhill version?

With machine learning models becoming more resource-hungry, I wonder if optimized infrastructure like this might change the game. Think about it: training models faster and with fewer limitations could really boost productivity for researchers and data scientists. Kinetic Seas seems to believe it’s worth building infrastructure just for AI, which feels like a pretty interesting bet.

Has anyone here worked with AI-specific setups like this? Curious to know if it’s really as smooth as it sounds!

[https://www.prnewswire.com/news-releases/kinetic-seas-fka-bellatora-announces-completion-of-phase-i-of-its-data-center-for-ai-302168707.html](https://www.prnewswire.com/news-releases/kinetic-seas-fka-bellatora-announces-completion-of-phase-i-of-its-data-center-for-ai-302168707.html)",0,0,0.5,2024-11-05 19:55:59,booboo1998,[],0,853,3358,105134,True,0.141713048855906,NEGATIVE,0.9978681802749634
[D] Is LoRA merging (and non linear mode connectivity) the key to better transformer hypernets?,hi guys i was thinking that if we could dynamically merge llm finetuning loras depending on type of task at hand we could fix catastrophic forgetting and maybe even have transformers better able to generalize the thing is due to attention layers being very very non linear on their weights transformers show poor lmc linear mode connectivity are you aware of the computational complexity of exact lora merging i have seen quite a lot of papers on the subject of lora merging but they seem of poor quality and only empirical with little mathematical grounding so if you guys have thought of it id be glad to hear about it,"Hi guys!
I was thinking that, if we could dynamically merge LLM fine-tuning LoRAs depending on type of task at hand, we could fix catastrophic forgetting and maybe even have transformers better able to generalize.
The thing is, due to Attention layers being very very non linear on their weights, transformers show poor LMC (linear mode connectivity).

Are you aware of the computational complexity of exact LoRA merging? I have seen quite a lot of papers on the subject of LoRA merging but they seem of poor quality and only empirical, with little mathematical grounding.

So if you guys have thought of it, I'd be glad to hear about it!",5,10,0.67,2024-11-05 16:31:57,Due-Pangolin325,[],0,620,69,14,True,0.07720238095238094,NEGATIVE,0.9995338916778564
Tools to classify emails - supporting DV victims [Discussion],hi all apologies if this is the wrong place to post im looking for tools that can help me support my partner who has been harassed for a number of years by her ex and father of her child she is trying to compile evidence for a restraining order but going back through the years of emails and other messages is psychologically draining for her i was wondering if there are any tools that have a good use case for analysing and classifying emails either individually or in bulk so that i can support her by taking over this work for her,"Hi all,

Apologies if this is the wrong place to post. I'm looking for tools that can help me support my partner, who has been harassed for a number of years by her ex and father of her child.  
  
  
She is trying to compile evidence for a restraining order but going back through the years of emails and other messages is psychologically draining for her. I was wondering if there are any tools that have a good use case for analysing and classifying emails, either individually or in bulk, so that I can support her by taking over this work for her?",0,6,0.44,2024-11-05 15:36:16,BunsenFurner87,[],0,534,4083,5255,True,0.012499999999999992,NEGATIVE,0.99952232837677
"[D] Mastering LLM Testing: Ensuring Accuracy, Ethics, and Future-Readiness for Next-Gen AI Models",hi everyone i just published an article mastering llm testing ensuring accuracy ethics and futurereadiness for nextgen ai models i hope i didnt miss anything important in there im planning to turn this into a series on ai model testing and testing in general hope you enjoy it and im always open for feedback and discussion,"Hi everyone! 😊 I just published an article: [Mastering LLM Testing: Ensuring Accuracy, Ethics, and Future-Readiness for Next-Gen AI Models](https://medium.com/@bartek.lazarowicz/mastering-llm-testing-ensuring-accuracy-ethics-and-future-readiness-for-next-gen-ai-models-adc85799efca). I hope I didn’t miss anything important in there!

I’m planning to turn this into a series on AI model testing and testing in general. Hope you enjoy it, and I’m always open for feedback and discussion! 😄",0,0,0.33,2024-11-05 14:38:55,tukan90,[],0,323,4022,167,True,0.21250000000000002,POSITIVE,0.5865781903266907
[R] What Are Your Biggest Pain Points in Managing and Scaling Multiple AI Models?,hey rmachinelearning im doing some research to understand the key challenges people face when managing multiple ai modelsparticularly around scaling monitoring performance and handling model failures id love to hear from the community to get a better sense of where the pain points are here are a few questions to start 1 scaling and load balancing do you find it difficult to scale models for high traffic or load balancing between models 2 model observability how challenging is it to monitor multiple models in production 3 fallback and redundancy when a model fails or underperforms whats your approach do you use fallback models and if so what would make managing them easier 4 user and permission management for those supporting multiple teams or clients how do you manage access across projects securely any struggles with multitenant support thanks so much for sharing your experiencesim excited to hear your insights,"Hey r/MachineLearning! 👋

I’m doing some research to understand the key challenges people face when managing multiple AI models—particularly around scaling, monitoring performance, and handling model failures. I’d love to hear from the community to get a better sense of where the pain points are.

Here are a few questions to start:

1. **Scaling and Load Balancing**: Do you find it difficult to scale models for high traffic or load balancing between models?
2. **Model Observability**: How challenging is it to monitor multiple models in production? 
3. **Fallback and Redundancy**: When a model fails or underperforms, what’s your approach? Do you use fallback models, and if so, what would make managing them easier?
4. **User and Permission Management**: For those supporting multiple teams or clients, how do you manage access across projects securely? Any struggles with multi-tenant support?

Thanks so much for sharing your experiences—I’m excited to hear your insights!",0,0,0.13,2024-11-05 13:43:53,BuddahJuddah,[],0,925,1198,193,True,0.1178125,NEGATIVE,0.9944702386856079
[D] Voice Isolation,hi so elevenlabs has a pretty good audio isolation api but it is really expensive are there any opensource models that can be selfhosted and get near the same quality,"Hi!

So ElevenLabs has a pretty good audio isolation API but it is really expensive. Are there any opensource models that can be self-hosted and get near the same quality?",0,0,0.5,2024-11-05 13:41:59,aszx789,[],0,166,3415,7072,True,0.10999999999999999,NEGATIVE,0.9991956353187561
"[R] Ask, and it shall be given: Turing completeness of prompting",,,0,13,0.32,2024-11-05 10:37:11,CommunismDoesntWork,[],0,0,3000,55426,True,0.0,POSITIVE,0.7481210231781006
[D] To what cross-entropy loss value can LLMs converge?,llms are usually evaluated on benchmarks that aim to measure broad abilities however most publishers of foundational models do not publish the actual crossentropy loss value that the model achieves at the end of training i couldnt find any sources on this but i would like to know what loss value the llms can achieve on human language is there anyone who knows more about this might there be some lower bound,"LLMs are usually evaluated on benchmarks that aim to measure broad abilities. However, most publishers of foundational models do not publish the actual cross-entropy loss value that the model achieves at the end of training. I couldn't find any sources on this, but I would like to know what loss value the LLMs can achieve on human language. Is there anyone who knows more about this? Might there be some lower bound?",33,19,0.95,2024-11-05 10:19:20,cbl007,[],0,409,3413,930,True,0.13541666666666666,NEGATIVE,0.9986404776573181
[R] Never Train from scratch,the authors show that when transformers are pre trained they can match the performance with s4 on the long range arena benchmark,"https://arxiv.org/pdf/2310.02980 

The authors show that when transformers are pre trained, they can match the performance with S4 on the Long range Arena benchmark. ",106,31,0.87,2024-11-05 09:02:43,Whatever_635,[],0,128,1584,6743,True,-0.05,POSITIVE,0.9695752263069153
[P] Getting crazy over a simple problem related to csv formatting,hi everyone im facing a frustrating issue with my python script im processing prices and quantities in a dataframe using them to calculate unit prices and saving the result to a csv file everything seems perfect in python correct calculations high precision but when i open the csv file the valuesparticularly in the unit prices columnare incorrect usually divided by 1000 or rounded even though i specified high precision a few details i use pdto_csv with decimal to ensure dotbased decimal formatting im not specifying a float_format aiming to retain maximum precision for unit prices the data preview in python shows the correct values before saving but the saved csv has discrepancies example output heres an example of what im seeing python output before saving to csv unit prices 0696 csv output opened in excel unit prices 696 the weird thing is that this does not happen consistently in some cases rows are correct has anyone faced this issue before any tips on ensuring that the csv retains the exact precision and format as seen in python,"Hi everyone,

I'm facing a frustrating issue with my Python script. I'm processing prices and quantities in a DataFrame, using them to calculate unit prices, and saving the result to a CSV file. Everything seems perfect in Python (correct calculations, high precision), but when I open the CSV file, the values—particularly in the `""Unit Prices""` column—are incorrect (usually divided by 1000) or rounded, even though I specified high precision.

A few details:

   * I use `pd.to_csv()` with `decimal='.'` to ensure dot-based decimal formatting.
   * I'm not specifying a `float_format`, aiming to retain maximum precision for `Unit Prices`.
   * The data preview in Python shows the correct values before saving, but the saved CSV has discrepancies.

**Example Output**: Here’s an example of what I'm seeing:

* Python Output (before saving to CSV): `Unit Prices = 0.696`
* CSV Output (opened in Excel): `Unit Prices = 696`

  
The weird thing is that this does not happen consistently. In some cases, rows are correct.

Has anyone faced this issue before? Any tips on ensuring that the CSV retains the exact precision and format as seen in Python?",0,6,0.2,2024-11-05 07:26:01,No_Possibility_7588,[],0,1048,1423,455,True,0.047,NEGATIVE,0.9977993369102478
[D] Text classification: N-shot prompt classification VS training a linear classifier on top of an embedder,i need to make a text classifier at work i have 200 examples for each of the 5 categories each example is an email two approaches classifying emails with nshot prompt classification possibly with lora finetuning use a pretrained text embedder eg a sentence transformer or openai textembeddings3 and a classification head train the classifier on the text embeddings which approach is best,"I need to make a text classifier at work. I have 200 examples for each of the 5 categories. Each example is an email. Two approaches:

* Classifying emails with n-shot prompt classification, possibly with LoRA finetuning.
* Use a pre-trained text embedder (e.g. a sentence transformer or OpenAI text-embeddings-3) and a classification head. Train the classifier on the text embeddings.

Which approach is best?",4,6,0.68,2024-11-05 06:53:32,Aromatic-Oil-4586,[],0,387,98,52,True,0.5,NEGATIVE,0.848968505859375
[D] Laptops for Theoretical Deep Learning,hi i am going for a phd in theoretical deep learning and i am looking to buy a new laptop i am unsure how readily the remote servers will be available i have not been admitted into a program yet so i am looking for enough compute power to simply test my code before running it on my labs servers i am currently contemplating between buying 1 asus zenbook 14 oled with 32gb ram intel core ultra 9 185h processor 24mb cache 16 cores 22 threads 1tb m2 nvme ssd and 75whrs 4cell liion battery 2 macbook air with 24gb ram m2 chip with 8core cpu 10core gpu 512gb storage and 582whrs lipolymer battery i understand it would be better to go for a nvidia gpu and that neither of these laptops have a gpu but i am not looking to invest in one my thoughts right now are that the zenbook 14 has a slightly better processor and much higher ram than the mba i dont care about the ssd 512gb is enough for me however i frequently see academics use the mba which could simply be about the fad but i am not aware i am also wondering if i am missing something i am not aware of by not jumping on the mba train they are about the same price so thats not much of decision factor i am also not sure if i should look at the cheaper 16gb options i am currently using a 16gb zenbook 13 bought 5 years back but the ram was limiting me in my masters thesis project the processors have improved since then so i am not sure if 16gb is enough now also i know it would be ideal to wait to learn more about the compute resources available at the lab i join but my current laptop is in a very poor state so much so that i cannot carry it anywhere hardware damage the screen flickers all the time and i worry that it will turn off any second and leave my data inaccessible does anyone have any thoughts or suggestions,"Hi, I am going for a PhD in theoretical deep learning and I am looking to buy a new laptop. I am unsure how readily the remote servers will be available (I have not been admitted into a program yet), so I am looking for enough compute power to simply test my code before running it on my lab's servers. I am currently contemplating between buying

1. Asus Zenbook 14 OLED with 32GB RAM, Intel Core Ultra 9 185H Processor (24MB Cache, 16 cores, 22 Threads), 1TB M.2 NVMe SSD and 75WHrs 4-cell Li-ion battery
2. Macbook Air with 24GB RAM, M2 Chip with 8-core CPU, 10-core GPU, 512GB Storage and 58.2-WHrs Li-polymer battery

I understand it would be better to go for a Nvidia GPU, and that neither of these laptops have a GPU, but I am not looking to invest in one.

My thoughts right now are that the Zenbook 14 has a slightly better processor, and much higher RAM than the MBA. I don't care about the SSD; 512GB is enough for me. However, I frequently see academics use the MBA, which could simply be about the fad, but I am not aware. I am also wondering if I am missing something I am not aware of by not jumping on the MBA train. They are about the same price, so that's not much of decision factor.

I am also not sure if I should look at the cheaper 16GB options. I am currently using a 16GB Zenbook 13 bought 5 years back, but the RAM was limiting me in my Master's thesis project. The processors have improved since then, so I am not sure if 16GB is enough now. Also, I know it would be ideal to wait to learn more about the compute resources available at the lab I join, but my current laptop is in a very poor state, so much so that I cannot carry it anywhere (hardware damage), the screen flickers all the time, and I worry that it will turn off any second and leave my data inaccessible.

Does anyone have any thoughts or suggestions?",0,28,0.35,2024-11-05 06:11:51,mio_11,[],0,1783,1337,80,True,0.08188114915387643,NEGATIVE,0.998761773109436
[d] About the speechbrain WSJ0Mix dataset.,i cant guarantee that the tag is appropriate i got tired of searching the wsj0mix dataset i want to separate multiple speakers the separator model of speechbrain doesnt give me the result i want so i wanted to build a model with the dataset i have however no matter how much i searched for the wsj0mix dataset it didnt come up i only found the m file but i cant find what is included in the dataset or what is written in the csv file the link above doesnt have the information i want either im very curious how you built the model,"I can't guarantee that the tag is appropriate.

I got tired of searching the WSJ0Mix dataset.

I want to separate multiple speakers.

The separator model of speechbrain doesn't give me the result I want.

So I wanted to build a model with the dataset I have.

However, no matter how much I searched for the WSJ0Mix dataset, it didn't come up.

I only found the \*.m file, but I can't find what is included in the dataset or what is written in the \*.csv file.

[https://speechbrain.readthedocs.io/en/latest/tutorials/tasks/source-separation.html](https://speechbrain.readthedocs.io/en/latest/tutorials/tasks/source-separation.html)

The link above doesn't have the information I want either.

I'm very curious how you built the model.",2,3,1.0,2024-11-05 05:15:56,EmbarrassedLadder665,[],0,530,531,43,True,0.024285714285714282,NEGATIVE,0.9991390705108643
[D] Do second tier papers have any value when apply for industry research job?,i think i have come across some industry jobs before that required applicants to have top tier paper nipsicmliclrcvpriccveccv so my question is do paper from less prestige aaaiijcaiwacvbmvc or journals conference have any value when appying for these job additionaly are metrics like hindex or citation matter,"I think I have come across some industry jobs before that required applicants to have top tier paper (NIPS/ICML/ICLR/CVPR/ICCV/ECCV), so my question is do paper from *less prestige* (AAAI/IJCAI/WACV/BMVC.... or  journals) conference have any value when appying for these job? Additionaly, are metrics like h-index or citation matter?",45,39,0.72,2024-11-04 23:58:17,Competitive_Newt_100,[],0,309,1145,-33,True,0.16666666666666669,NEGATIVE,0.998938262462616
[D] Best Resources for Sensitivity Analysis on Large ML Pipelines?,im on a team thats launching a large project to examine how an ml pipeline behaves in response to variations in data this is the first time im doing a sensitivity analysis this large and complex in a while so im looking for help to identify the most uptodate resources on simulated data and especially any python tools and how they compare with the best that r has to offer evaluation tooling elasticity any best resources on sensitivity analysis overall particularly newer ones from the post couple of years what are the best resources youve found,"
I'm on a team that's launching a large project to examine how an ML pipeline behaves in response to variations in data. 

This is the first time I'm doing a sensitivity analysis this large and complex in a while, so I'm looking for help to identify the most up-to-date resources on: 

- Simulated data, and especially any Python tools and how they compare with the best that R has to offer

- Evaluation tooling

- Elasticity

- Any best resources on Sensitivity analysis overall, particularly newer ones from the post couple of years

What are the best resources you've found?",4,0,0.84,2024-11-04 22:58:33,oldmaninnyc,[],0,548,38,253,True,0.3677489177489177,NEGATIVE,0.9408738017082214
[D] Advice on Preparing for Google ML Interview – Key Areas to Focus On?,im preparing for a machine learning interview with google and the recruiter shared the main areas theyll focus on theoretical ml concepts and practical applications including problem definition model selection model tuning and evaluation industryscale ml covering performance and cost optimization data handling and productionoriented experimentation debugging if anyone has insights on what to expect in these areas or tips on what to focus on id really appreciate it im especially struggling to understand what industryscale ml questions could actually be thanks in advance for any advice or resources edit for context ive already done my two lc style interviews the first interview was an easymedium i would say and the second interview was definitely hard i think i did well on both but only the second interviewer let me know how i did i did well apparently i also did the googlyness interview which i think went well also we had some good conversation,"I’m preparing for a machine learning interview with Google, and the recruiter shared the main areas they’ll focus on:

\- Theoretical ML concepts and practical applications – including problem definition, model selection, model tuning, and evaluation.

\- Industry-Scale ML – covering performance and cost optimization, data handling, and production-oriented experimentation & debugging.

If anyone has insights on what to expect in these areas or tips on what to focus on, I’d really appreciate it! I’m especially struggling to understand what “Industry-Scale ML” questions could actually be.

Thanks in advance for any advice or resources!

  
edit: for context: I've already done my two LC style interviews. The first interview was an easy-medium I would say, and the second interview was definitely hard. I think I did well on both but only the second interviewer let me know how I did (I did well apparently). I also did the Googlyness interview which I think went well also. We had some good conversation.",46,18,0.87,2024-11-04 22:53:08,atomicalexx,[],0,957,3113,55629,True,0.10625,NEGATIVE,0.9959575533866882
Video Input for your local LLMS [P],what my project does openscenesenseollama is a powerful python package designed for privacyfocused video analysis directly on your local machine with this tool you can leverage ollamas local models to analyze frames transcribe audio dynamically select key frames and generate detailed summaries all without relying on cloudbased apis its ideal for those needing rich insightful analysis of video content while ensuring data privacy and minimizing usage costs target audience this project is tailored for developers researchers data scientists and privacyconscious users who require indepth locally processed video analysis its perfect for applications where data security is critical including content creation workflows that need automatic video summarization researchers building labeled datasets for machine learning platforms needing contextrich content moderation offline projects in remote or restricted environments comparison openscenesenseollama goes beyond traditional video analysis tools that often separate frame and audio analysis instead it integrates both visual and audio elements allowing users to prompt the models to produce comprehensive summaries and indepth contextual insights where most tools might identify objects or transcribe audio separately openscenesenseollama unifies these components into narrative summaries making it ideal for richer datasets or more nuanced content moderation getting started to begin using openscenesenseollama 1 prerequisites make sure you have python 310 ffmpeg pytorch and ollama installed on your machine 2 install with pip run pip install openscenesenseollama to install the package 3 configuration start analyzing video with customizable prompts frame selection and audio transcription feel free to dive in try it out and share your feedback especially if youre working in ai privacyfocused applications or video content moderation lets build a powerful local solution for meaningful video analysis,"**What My Project Does**

OpenSceneSense-Ollama is a powerful Python package designed for privacy-focused video analysis directly on your local machine. With this tool, you can leverage Ollama’s local models to analyze frames, transcribe audio, dynamically select key frames, and generate detailed summaries — all without relying on cloud-based APIs. It’s ideal for those needing rich, insightful analysis of video content while ensuring data privacy and minimizing usage costs.

**Target Audience**

This project is tailored for developers, researchers, data scientists, and privacy-conscious users who require in-depth, locally processed video analysis. It's perfect for applications where data security is critical, including:

\- Content creation workflows that need automatic video summarization

\- Researchers building labeled datasets for machine learning

\- Platforms needing context-rich content moderation

\- Offline projects in remote or restricted environments

**Comparison**

OpenSceneSense-Ollama goes beyond traditional video analysis tools that often separate frame and audio analysis. Instead, it integrates both visual and audio elements, allowing users to prompt the models to produce comprehensive summaries and in-depth contextual insights. Where most tools might identify objects or transcribe audio separately, OpenSceneSense-Ollama unifies these components into narrative summaries, making it ideal for richer datasets or more nuanced content moderation.

**Getting Started**

To begin using OpenSceneSense-Ollama:

1. Prerequisites: Make sure you have Python 3.10+, FFmpeg, PyTorch and Ollama installed on your machine.
2. Install with pip: Run \`pip install openscenesense-ollama\` to install the package.
3. Configuration: Start analyzing video with customizable prompts, frame selection, and audio transcription.

Feel free to dive in, try it out, and share your feedback especially if you're working in AI, privacy-focused applications, or video content moderation. Let’s build a powerful, local solution for meaningful video analysis!

[https://github.com/ymrohit/openscenesense-ollama](https://github.com/ymrohit/openscenesense-ollama)",11,6,0.87,2024-11-04 19:50:06,rohit3627,[],0,1959,2257,4785,True,0.2858695652173913,POSITIVE,0.9969484210014343
[P] NN for creating best camouflage,i had this idea for some time and i have created all the functions for creating data as well as all the architecture the problem is that i only have two years experience in deep learning and this is gan style network and gans are known to be very hard to train i would like you opinions on idea as well as some tips suggestions advices and things to change also if someone finds this interesting i would love to work with someone on this project camouflage pattern generation model the objective is to create a model that generates optimal camouflage color patterns by training a generator model and using a segmentation model as a discriminator to assess the effectiveness of the generated camouflage both the generator and discriminator are trained simultaneously model structure forward process 1 generator the generator is a simple decoder model that takes a random latent vector of size n_embed 128 and outputs a 3x32x32 camouflage color pattern this generated camouflage pattern is then tiled to form a larger texture matching the size of an image of a soldier 2 creating camouflaged soldier random blackandwhite png images of soldiers are sampled and resized to 1 w h with the values inverted so the soldier appears in white foreground and the background is black the tiled camouflage pattern is then applied to the soldier by masking with the soldier image producing a camouflaged soldier figure this entire operation is batched and allows gradients to flow through 3 placing camouflaged soldier on background the camouflaged soldier is randomly placed on a background image eg a forest scene a label mask for the segmentation model is generated simultaneously with two classes background and soldier 4 discriminator segmentation model a pretrained segmentation model acting as a discriminator is used with two output classes background and soldier this model assesses how well the camouflage pattern blends the soldier into the background by trying to classify the soldier as the background loss functions and optimization two loss functions are used each with separate backpropagation processes 1 generator loss this encourages the generator to create a camouflage pattern that makes the soldier indistinguishable from the background loss function crossentropylossoutput 0 where the output is the predicted segmentation map from the discriminator and 0 represents the background class 2 discriminator segmentation model loss this encourages the segmentation model to correctly identify the camouflaged soldier in the background loss function crossentropylossoutput label_mask where the label mask has two classes background and soldier key considerations this setup resembles a generative adversarial network gan but differs in that it uses no real camouflage data only generated samples additionally separate optimizers different optimizers are recommended for the generator and discriminator loss scaling careful tuning of scaling factors or learning rates may be required to stabilize training twostep backpropagation instead of a typical ganstyle loss a twostep backpropagation approach is used to update the models independently,"I had this idea for some time, and I have created all the functions for creating data as well as all the architecture. The problem is that I only have two years experience in Deep Learning, and this is GAN style network, and GANs are known to be very hard to train. I would like you opinions on idea, as well as some tips, suggestions, advices and things to change. Also if someone finds this interesting I would love to work with someone on this project.

# Camouflage Pattern Generation Model

The objective is to create a model that generates optimal camouflage color patterns by training a generator model and using a segmentation model as a discriminator to assess the effectiveness of the generated camouflage. Both the generator and discriminator are trained simultaneously.

# Model Structure

# Forward Process

1. **Generator**:
   * The generator is a simple decoder model that takes a random latent vector of size `n_embed = 128` and outputs a 3x32x32 camouflage color pattern.
   * This generated camouflage pattern is then tiled to form a larger texture, matching the size of an image of a soldier.
2. **Creating Camouflaged Soldier**:
   * Random black-and-white PNG images of soldiers are sampled and resized to `(1, W, H)`, with the values inverted so the soldier appears in white (foreground) and the background is black.
   * The tiled camouflage pattern is then applied to the soldier by masking with the soldier image, producing a camouflaged soldier figure. This entire operation is batched and allows gradients to flow through.
3. **Placing Camouflaged Soldier on Background**:
   * The camouflaged soldier is randomly placed on a background image (e.g., a forest scene).
   * A label mask for the segmentation model is generated simultaneously, with two classes: background and soldier.
4. **Discriminator (Segmentation Model)**:
   * A pre-trained segmentation model (acting as a discriminator) is used with two output classes (background and soldier).
   * This model assesses how well the camouflage pattern blends the soldier into the background by trying to classify the soldier as the background.

# Loss Functions and Optimization

Two loss functions are used, each with separate backpropagation processes:

1. **Generator Loss**:
   * This encourages the generator to create a camouflage pattern that makes the soldier indistinguishable from the background.
   * **Loss Function**: `CrossEntropyLoss(output, 0)` where the output is the predicted segmentation map from the discriminator, and 0 represents the background class.
2. **Discriminator (Segmentation Model) Loss**:
   * This encourages the segmentation model to correctly identify the camouflaged soldier in the background.
   * **Loss Function**: `CrossEntropyLoss(output, label_mask)` where the label mask has two classes: background and soldier.

# Key Considerations

This setup resembles a Generative Adversarial Network (GAN) but differs in that it uses no ""real"" camouflage data, only generated samples. Additionally:

* **Separate Optimizers**: Different optimizers are recommended for the generator and discriminator.
* **Loss Scaling**: Careful tuning of scaling factors or learning rates may be required to stabilize training.
* **Two-Step Backpropagation**: Instead of a typical GAN-style loss, a two-step backpropagation approach is used to update the models independently.

https://preview.redd.it/qd2cr2rkyyyd1.png?width=5603&format=png&auto=webp&s=0faee2cb0504a98c36b365b2edbc59253509d8c7

",6,11,0.63,2024-11-04 18:28:17,MemoryCompetitive691,[],0,3143,1380,1,True,-0.06420454545454547,NEGATIVE,0.9952551126480103
What problems do Large Language Models (LLMs) actually solve very well? [D],while theres growing skepticism about the ai hype cycle particularly around chatbots and rag systems im interested in identifying specific problems where llms demonstrably outperform traditional methods in terms of accuracy cost or efficiency problems i can think of are words categorization sentiment analysis of nolarge body of text image recognition to some extent writing style transfer to some extent what else,"While there's growing skepticism about the AI hype cycle, particularly around chatbots and RAG systems, I'm interested in identifying specific problems where LLMs demonstrably outperform traditional methods in terms of accuracy, cost, or efficiency. Problems I can think of are:

\- words categorization

\- sentiment analysis of no-large body of text

\- image recognition (to some extent)

\- writing style transfer (to some extent)

what else?",142,100,0.91,2024-11-04 15:52:48,Educational-String94,[],0,415,1387,322,True,0.10416666666666666,NEGATIVE,0.9932079911231995
[D] Resources for adding cross attention to a pretrained language model,i want to train new cross attention layers feeding into a pretrained transformer maybe a small llama model while keeping the rest of the model constant what are some resources that might be helpful,"I want to train new cross attention layers feeding into a pretrained transformer (maybe a small llama model) while keeping the rest of the model constant.

What are some resources that might be helpful?
",2,5,0.67,2024-11-04 11:03:34,BinaryOperation,[],0,197,1281,178,True,-0.02840909090909091,NEGATIVE,0.9927666187286377
"[D] Is there limited quantization in all LLM models? For example you can take a standard model like meta-llama/Llama-3.2-1B and run it at half, but there are also models specifically made for 4bit quantization (i.e. meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8)",im just trying to understand how quantization is setup in all the models standard models like metallamallama321b can be run without quantization bfloat16 or float32 or they can be told to run at half float16 with an inferencing app like vllm so does that mean there is some quantization build into all models instead of telling it to run at half quantization can i instead say int8 or does that only work if the model was built for it and then there are models that are specifically built for int4 ie metallamallama321binstructspinquant_int4_eo8 does that mean that when you run this model with vllm you have to explicitly say you are running it at int4 or you just leave that at default and it will automatically run at int4 can that be overridden to int8 or is it just hardcoded for int4 just been trying to wrap my head around this for the past 2 days,"I'm just trying to understand how quantization is setup in all the models.

Standard models like meta-llama/Llama-3.2-1B can be run without quantization (bfloat16 or float32?), or they can be told to run at half (float16?) with an inferencing app (like vLLM). So does that mean there is some quantization build into all models? Instead of telling it to run at half quantization, can I instead say int8? Or does that only work if the model was built for it?

And then there are models that are specifically built for int4 (i.e. meta-llama/Llama-3.2-1B-Instruct-SpinQuant\_INT4\_EO8). Does that mean that when you run this model with vLLM, you have to explicitly say you are running it at int4, or you just leave that at default and it will automatically run at int4? Can that be overridden to int8? Or is it just hardcoded for int4?

Just been trying to wrap my head around this for the past 2 days.",2,7,0.58,2024-11-04 10:54:02,xil35,[],0,854,1723,976,True,-0.17261904761904762,NEGATIVE,0.9992040991783142
[D] COLING25 Industry Track: Notification of Acceptance,the date for notification of acceptance was 1200 anywhere on earth november 3rd of 2024 and weve not heard back from the chairs theres no notification on the portal as well is there a delay or is it that only those papers which are accepted will receive a notification please share any infoupdates on this thanks,"The date for ""notification of acceptance"" was 12:00 anywhere on Earth, November 3rd of 2024. And we've not heard back from the chairs, there's no notification on the portal as well, is there a delay? Or is it that only those papers which are accepted will receive a notification? Please share any info/updates on this, thanks.",4,5,0.71,2024-11-04 10:44:40,BlackEyesBrownSavant,[],0,312,1268,24,True,0.05,NEGATIVE,0.996292769908905
"[R] Estimation of multivariate mutual information, PID for more than three variables ",why isnt estiming high dimensional mutual information popular for instance the most ive seen is 3 variable i know the number of samples needed exponentially increases but in big data settings it would still be feasible discrimination is also an issue since estimation is usually performed for binned data anyone know more about this and the practical applications of more than three variable mutual information on very interested in reading about applications to infer relationship between high dimensional variables in data sets with large number of samples," Why isn't estiming high dimensional mutual information popular. For instance the most I've seen is 3 variable. I know the number of samples needed exponentially increases. But in big data settings it would still be feasible. 

Discrimination is also an issue since estimation is usually performed for binned data.

Anyone know more about this and the practical applications of more than three variable mutual information? On very interested in reading about applications to infer relationship between high dimensional variables in data sets with large number of samples.",2,11,1.0,2024-11-04 08:09:25,Sandy_dude,[],0,558,1260,2841,True,0.27092857142857146,NEGATIVE,0.994666576385498
What differentiates the top % in ML/ DL? [discussion],hello ill start by saying that i work in the recruitment field internally for one of the leading tech ai companies in the my background has historically been software and i am transitioning in to ai i am a diligent professional and usually take basic technical entry tests to fundamentally understand the area infrastructure that i recruit for i wanted to ask this great community what differentiates the top of ml dl engineers is it handson experience and sme in a particular subset of ai or more rounded knowledge of most areas is reddit the largest community online for people in ml dl would you recommend any particular platforms to network aside from linkedin reddit and slack i am aware that i will get messaged due to my profession but i have had incredible interactions in the product eng space on reddit over the years and your insights are invaluable thank you,"Hello! I'll start by saying that I work in the recruitment field, internally for one of the leading tech/ AI companies in the 🌍. My background has historically been Software, and I am transitioning in to AI.

I am a diligent professional and usually take basic technical entry tests to fundamentally understand the area/ infrastructure that I recruit for. I wanted to ask this great community:

🤔 What differentiates the top % of ML/ DL engineers? Is it hands-on experience and SME in a particular subset of AI, or more rounded knowledge of most areas?

🤔 Is reddit the largest community online for people in ML/ DL? would you recommend any particular platforms to network aside from Linkedin, Reddit and Slack?

I am aware that I will get messaged due to my profession, but I have had incredible interactions in the product/ Eng space on Reddit over the years and your insights are invaluable.

Thank you",0,10,0.18,2024-11-04 04:45:07,k44yej88,[],0,870,1368,157,True,0.18802083333333333,POSITIVE,0.9930718541145325
[P] Text classification with low number of data: LLM or other classification models? ,i have a project where i need to summarize a few webpages related to a subject and use the summaries to classify aforementioned subject when prototyping i use llm for both summarization and classification task and they did achieve about 80 accuracy classification task isnt that hard anyway for the sake of performance and the hate of using llm for everything i initially want to train 2 models one for summarization and one for classification the problem arise when i see that most usable summazier is not that lighter than a small llm and not many support my language add another classifier like bert or something then the difference in memory consumption is probably negligible though runtime should still be better another problem is that my dataset is just about 2000 webpages for about 300 subjects and 70 classes many classes has 0 or 1 samples with that data i think finetuning a summazier is somewhat doable while its probably not applicable to the classifier getting more data is not exactly an option as i dont have the time budget despite that i have detail description of what should be classified into each class as a result my current solution is to finetune a llm to do both summarization and classification the downside is that llm sometimes gives invalid classes is adding a classification head to the llm a good solution im afraid that i dont have enough data to train even that classification head realisticly a single classification matrix or is there a better approach than this,"I have a project where I need to summarize a few webpages related to a subject and use the summaries to classify aforementioned subject. 

When prototyping, I use LLM for both summarization and classification task and they did achieve about 80% accuracy (classification task isn't that hard anyway). For the sake of performance and the hate of using LLM for everything, I initially want to train 2 models, one for summarization and one for classification.

The problem arise when I see that most usable summazier is not that lighter than a small LLM (and not many support my language). Add another classifier like bert or something then the difference in memory consumption is probably negligible. Though runtime should still be better. 

Another problem is that my dataset is just about 2000 webpages for about 300 subjects and 70 classes. Many classes has 0 or 1 samples. With that data, I think finetuning a summazier is somewhat doable while it's probably not applicable to the classifier. Getting more data is not exactly an option as I don't have the time budget. Despite that, I have detail description of what should be classified into each class. 

As a result, my current solution is to finetune a LLM to do both summarization and classification. The downside is that LLM sometimes gives invalid classes.

Is adding a classification head to the LLM a good solution? I'm afraid that I don't have enough data to train even that classification head (realisticly a single classification matrix). Or is there a better approach than this? ",4,11,0.83,2024-11-03 23:47:35,mtmttuan,[],0,1500,1572,6678,True,0.01326530612244898,NEGATIVE,0.9990646243095398
[P] Combining algorithms in an autonomous driving project,i am planning to do a project consisting of an autonomous driving system i was thinking of using reinforcement learning but it would take too long to train months with the consequent expenditure of electricity and money specialized servers after seeing some videos from sentdex and others where after training for 2 months in a row the driver manages to drive like a drunk person i have considered it unfeasible and i have thought would it be possible to combine a deep learning algorithm with reinforcement learning together with a traditional computer vision algorithm like lane finding is there any way to make these algorithms work together reducing the training time would you use other algorithms or approaches im using carla sim thanks,"I am planning to do a project consisting of an autonomous driving system.



I was thinking of using reinforcement learning but it would take too long to train (months), with the consequent expenditure of electricity and money (specialized servers).



After seeing some videos from Sentdex and others where, after training for 2 months in a row, the driver manages to drive like a drunk person, I have considered it unfeasible and I have thought:



Would it be possible to combine a deep learning algorithm with reinforcement learning together with a traditional computer vision algorithm like lane finding?



Is there any way to make these algorithms work together, reducing the training time?



Would you use other algorithms or approaches?

I'm using CARLA Sim.

Thanks.",0,2,0.2,2024-11-03 21:37:16,AlbertV999,[],0,742,1423,3,True,-0.009374999999999994,NEGATIVE,0.9995875954627991
"[D] How to read and perform a semantic query over the combination of structured data and unstructured data ? As in, for example, a large number of pdf documents with text and also structured data that occurs in lists/tables in the pdf or as numerical data mentioned inside text paragraphs.",to carry the example further say these pdfs are financial reports from different companies that contain quarterly revenue data amongst other data there are two aspects to the broader problem query and read as further elaborated below query so i want to be able to make arbirary queries against all the pdfs like find companies where year over year quarterly growth was greater than 10 and which also mentioned new product launches this is a simple example but actual use case can be arbirarily more complex with more aspects to the query read feed in new reports i also want to make it so that nontechnical users can drop in new pdf reports as they become available which then get added to the query database without manual involvement of technical personnel in preprocessing can you please guide me as to how to approach this problem and what options exist to implement something like this thanks,"To carry the example further, say these pdfs are financial reports from different companies that contain quarterly revenue data amongst other data. There are two aspects to the broader problem: Query and Read as further elaborated below  
QUERY : So I want to be able to make arbirary queries against all the pdfs , like ""find companies where Year over year quarterly growth was greater than 10% and which also mentioned new product launches"".  This is a simple example, but actual use case can be arbirarily more complex with more aspects to the query.  
READ (feed in) new reports: I also want to make it so that non-technical users can drop in new pdf reports as they become available which then get added to the query database without manual involvement of technical personnel in pre-processing  
Can you please guide me as to how to approach this problem and what options exist to implement something like this? Thanks.",7,3,0.89,2024-11-03 20:49:03,SpaceShip992,[],0,897,981,7,True,0.14356060606060606,NEGATIVE,0.9930973649024963
"[P] Benchmarking 1 Million Files from ImageNet into DVC, Git-LFS, and Oxen.ai for Open Source Dataset Collaboration",hey all if you havent seen the oxen project yet we have been building a fast open source unstructured data version control tool and platform to host the data its an alternative to dumping data on hugging face with gitlfs or their datasets library and goes together with their models like chocolate and peanut butter oxen can be used for iterating on and editing the data and hugging face for public models we were inspired by the idea of making large machine learning datasets living breathing assets that people can collaborate on rather than the static dumps lately we have been working hard on optimizing the underlying merkle trees and data structures with in oxenai and just released v0194 which provides a bunch of performance upgrades and stability to the internal apis 1 million files benchmark to put it all to the test we decided to benchmark the tool on the 1 million images in the classic imagenet dataset the tldr is oxenai is faster than raw uploads to s3 13x faster than gitlfs and 5x faster than dvc the full breakdown can be found here if you are in the mlai community or just data aficionados would love to get your feedback on both the tool and the codebase we would love some community contribution when it comes to different storage backends and integrations into other data tools,"Hey all!

If you haven't seen the Oxen project yet, we have been building a fast [open source unstructured data version control tool](https://github.com/Oxen-AI/oxen-release) and platform to host the data ([https://oxen.ai](https://oxen.ai/)). It’s an alternative to dumping data on Hugging Face with git-lfs or their datasets library and goes together with their models like chocolate and peanut butter - Oxen can be used for iterating on and editing the data and Hugging Face for public models.

We were inspired by the idea of making large machine learning datasets living & breathing assets that people can collaborate on, rather than the static dumps. Lately we have been working hard on optimizing the underlying Merkle Trees and data structures with in [Oxen.ai](http://oxen.ai/) and just released v0.19.4 which provides a bunch of performance upgrades and stability to the internal APIs.

# 1 Million Files Benchmark

To put it all to the test, we decided to benchmark the tool on the 1 million+ images in the classic ImageNet dataset.

The TLDR is [Oxen.ai](http://oxen.ai/) is faster than raw uploads to S3, 13x faster than git-lfs, and 5x faster than DVC. The full breakdown can be found here 👇

[https://docs.oxen.ai/features/performance](https://docs.oxen.ai/features/performance)

If you are in the ML/AI community, or just data aficionados, would love to get your feedback on both the tool and the codebase. We would love some community contribution when it comes to different storage backends and integrations into other data tools.",23,9,0.93,2024-11-03 18:43:42,FallMindless3563,[],0,1301,1421,1026,True,0.09890109890109891,NEGATIVE,0.9920273423194885
[D] Comparison of Logistic Regression with/without SMOTE,this has been driving me crazy at work ive been evaluating a logistic predictive model the model implements smote to balance the dataset to 11 ratio originally 7 of the desired outcome i believe this to be unnecessary as shifting the decision threshold would be sufficient and avoid unnecessary data imputation the dataset has more than 9000 ocurrences of the desired event this is more than enough for mle estimation my colleagues dont agree i built a shiny app in r to compare the confusion matrixes of both models along with some metrics i would welcome some input from the community on this comparison to me the nonsmote model performs just as well or even better if looking at the brier score or calibration intercept what do you guys think,"This has been driving me crazy at work. I've been evaluating a logistic predictive model. The model implements SMOTE to balance the dataset to 1:1 ratio (originally 7% of the desired outcome). I believe this to be unnecessary as shifting the decision threshold would be sufficient and avoid unnecessary data imputation. The dataset has more than 9,000 ocurrences of the desired event - this is more than enough for MLE estimation. My colleagues don't agree. 

I built a shiny app in R to compare the confusion matrixes of both models, along with some metrics. I would welcome some input from the community on this comparison. To me the non-smote model performs just as well, or even better if looking at the Brier Score or calibration intercept. What do you guys think?",80,46,0.9,2024-11-03 17:42:39,Janky222,[],0,745,1734,337,True,0.1416666666666667,NEGATIVE,0.9984833598136902
[D] Feature Selection + Feature Eng. Order of Operations ,anyone have a preferred methodology and order of operations for performing feature selection with feature engineering for example is the best practice to drop unimportant features first then iteratively engineer new features,"Anyone have a preferred methodology and order of operations for performing feature selection with feature engineering? 
For example, is the best practice to drop unimportant features first, then iteratively engineer new features? ",10,3,0.86,2024-11-03 17:37:42,Secret_Valuable_Yes,[],0,224,810,56,True,0.24659090909090908,NEGATIVE,0.7395758628845215
[R] Training multiple autoencoders reduces loss but not accuracy? ,hello i am training two seperate autoencoders to cluster data the network passes the input to both autoencoders computes the reconstruction error for both autoencoders ae and picks the best one this means that only the reconstruction of one ae contributes to the loss and so only one gets gradient updates per input loss decreases but accuracy just fluctuates moreover both autoencoders are used but eventually the model just uses the same autoencoder for almost all inputs any insight on why the goal is for each ae to learn to reconstruct datapoints that are neighbors or belong to same cluster ive seen papers doing the same thing but they just pretrain their network to go around this and never discuss why this happens ty,"Hello,

I am training two seperate autoencoders to cluster data. The network passes the input to both autoencoders, computes the reconstruction error for both autoencoders (AE) and picks the best one. This means that only the reconstruction of one AE contributes to the loss and so only one gets gradient updates per input. 

Loss decreases but accuracy just fluctuates. Moreover, both autoencoders are used but eventually the model just uses the same autoencoder for almost all inputs. Any insight on why? 

The goal is for each AE to learn to reconstruct datapoints that are neighbors or belong to same cluster. I’ve seen papers doing the same thing but they just pre-train their network to go around this and never discuss WHY this happens.

Ty ",5,10,0.86,2024-11-03 17:26:03,Grand_Comparison2081,[],0,726,613,69,True,0.16666666666666666,NEGATIVE,0.9978773593902588
[D] What are some good resources for learning about sequence modeling architectures,what are some good resources for learning about sequence modeling architectures ive been preparing for exams and interviews and came across this quiz on github and another practice site do you think these are comprehensive or should i look for more material both are free to use right now,"What are some good resources for learning about sequence modeling architectures? I've been preparing for exams and interviews and came across this quiz on GitHub: [https://viso.ai/deep-learning/sequential-models/](https://viso.ai/deep-learning/sequential-models/) and another practice site: [https://app.wittybyte.ai/problems/rnn\_lstm\_tx](https://app.wittybyte.ai/problems/rnn_lstm_tx). Do you think these are comprehensive, or should I look for more material? Both are free to use right now",1,1,0.67,2024-11-03 17:11:17,vicky0212,[],0,288,1586,8,True,0.4714285714285714,NEGATIVE,0.955412745475769
Video Input for the current LLMs [P],hey everyone im excited to share a project ive been working on openscenesense its a python package designed to bridge video content with large language models llms like openais vision models and openrouter opening up new ways to understand analyze and create insights from video data why openscenesense most llms are amazing with text but arent designed to handle video directly openscenesense changes that it uses framebyframe analysis audio transcription and scene detection to turn video data into something llms can work with imagine using a prompt to get a detailed description of whats happening in each scene or automatically creating a narrative that ties the video and audio together potential use cases dataset creation if youre working in computer vision or machine learning openscenesense can create richly annotated datasets from videos giving llms detailed context about visual events object interactions and even sentiment shifts across scenes content moderation openscenesense can bring more context to content moderation unlike traditional moderation methods that might just detect keywords or simple visuals this tool can interpret entire scenes combining both visual and audio cues it could help distinguish between genuinely problematic content and innocuous material that might otherwise get flagged and im also working on an ollamacompatible version so you can run it locally without relying on the cloud which will be useful for anyone concerned about privacy or latency to dive in youll need python 310 ffmpeg and a couple of api keys openai or openrouter install it with pip install openscenesense and youre all set from there its easy to start analyzing your videos and experimenting with different prompts to customize what you want to extract id love feedback from anyone working in video tech dataset creation or moderation check out the code give it a spin and lets see where we can take openscenesense together,"Hey everyone,

I’m excited to share a project I’ve been working on OpenSceneSense. It’s a Python package designed to bridge video content with large language models (LLMs) like OpenAI’s Vision models and OpenRouter, opening up new ways to understand, analyze, and create insights from video data.

Why OpenSceneSense?

Most LLMs are amazing with text but aren’t designed to handle video directly. OpenSceneSense changes that. It uses frame-by-frame analysis, audio transcription, and scene detection to turn video data into something LLMs can work with. Imagine using a prompt to get a detailed description of what’s happening in each scene or automatically creating a narrative that ties the video and audio together.

Potential Use Cases:

\- Dataset Creation: If you’re working in computer vision or machine learning, OpenSceneSense can create richly annotated datasets from videos, giving LLMs detailed context about visual events, object interactions, and even sentiment shifts across scenes.

\- Content Moderation: OpenSceneSense can bring more context to content moderation. Unlike traditional moderation methods that might just detect keywords or simple visuals, this tool can interpret entire scenes, combining both visual and audio cues. It could help distinguish between genuinely problematic content and innocuous material that might otherwise get flagged.

And I’m also working on an Ollama-compatible version so you can run it locally without relying on the cloud, which will be useful for anyone concerned about privacy or latency.

To dive in, you’ll need Python 3.10+, FFmpeg, and a couple of API keys (OpenAI or OpenRouter). Install it with \`pip install openscenesense\`, and you’re all set. From there, it’s easy to start analyzing your videos and experimenting with different prompts to customize what you want to extract.

I’d love feedback from anyone working in video tech, dataset creation, or moderation. Check out the code, give it a spin, and let’s see where we can take OpenSceneSense together!

[https://github.com/ymrohit/openscenesense](https://github.com/ymrohit/openscenesense)",6,1,0.75,2024-11-03 17:04:26,rohit3627,[],0,1941,2257,4785,True,0.23790830381739472,POSITIVE,0.7267423272132874
[D] Self-hostable tooling for offline batch-prediction on SQL tables,hey folks i am working for a hospital in switzerland and due to data regulations it is quite clear that we need to stay out of cloud environments our hospital has a mssqlbased data warehouse and we have a separate dockercompose based mlops stack some of our models are currently running in docker containers with a rest api but actually we just do scheduled batchprediction on the data in the dwh in principle i am looking for a stack that allows you to host ml models from scikit learn to pytorch and allows us to formulate a batch prediction on data in the sql tables by defining input from one table as input features for the model and write back the results to another table i have seen postgresml and its predict_batch but i am wondering if we can get something like this directly interacting with our dwh what do you suggest as an architecture or tooling for batch predicting data in sql dbs when the results will be in sql dbs again and all predictions can be precomputed thanks for your help,"Hey folks,

I am working for a hospital in Switzerland and due to data regulations, it is quite clear that we need to stay out of cloud environments. Our hospital has a MSSQL-based data warehouse and we have a separate docker-compose based ML-ops stack. Some of our models are currently running in docker containers with a REST api, but actually, we just do scheduled batch-prediction on the data in the DWH. In principle, I am looking for a stack that allows you to host ml models from scikit learn to pytorch and allows us to formulate a batch prediction on data in the SQL tables by defining input from one table as input features for the model and write back the results to another table. I have seen postgresml and its predict_batch, but I am wondering if we can get something like this directly interacting with our DWH? What do you suggest as an architecture or tooling for batch predicting data in SQL DBs when the results will be in SQL DBs again and all predictions can be precomputed?

Thanks for your help!",3,0,1.0,2024-11-03 16:53:14,benelott,[],0,999,2923,1121,True,0.03928571428571429,NEGATIVE,0.9889196753501892
[D] Fourier weights neural networks,dear ml community i wanted to share an idea for discussion about the usage of fourier coefficients to parametrize weights in neural networks typically in mlps the weights are defined only in one direction and are undefined in the other direction which leaves it open we can define the weights to be symmetric wrs wsr and we can use the fourier coefficients of a two variable symmetric function to compute the weights via backpropagation and gradient descent i should mention that i am currently activeyl searching for an opportunity to bring my knowledge of machine learning to projects near frankfurt am main germany edit maybe my wording was not so correct let us agree that in most cases the symmetry assumption is satisfied by mlps with invertible activation function the idea i would like to discuss is the usage of fourier coefficients to re construct the weights wrs wsr for this idea to make sense the fwnn do not learn the weights as usual mlps anns but they learn the _coefficients_ of the fourier series at least some of them by adjusting how many coefficients are learned the fwnn could adjust its capacity to learn notice that by symmetry of the function wrs we get terms like sum_j c_jcosj rs where j ranges over some predefined range rr of integers in theory this r should be infinity hence z inf inf are the whole integers notice also that the parameter c_j the network learns are 2r1 in number which at first glance is independent of the number of neurons n hence a traditional neural network with n neurons has in theory to learn on2 weights but with the fourier transform we reduce this number of parameters to 2r1 of course it can happen that r n2 but i can imagine that there are problems where 2r1 n2 i hope this clarifies the idea code explanation of the method,"Dear ML community,

I wanted to share an idea for discussion about the usage of Fourier coefficients to parametrize weights in neural networks. Typically in MLPs the weights are defined only in one direction, and are undefined in the other direction, which leaves it open: we can define the weights to be symmetric: w(r,s) = w(s,r) and we can use the Fourier coefficients of a two variable symmetric function to compute the weights via backpropagation and gradient descent. (I should mention that I am currently activeyl searching for an opportunity to bring my knowledge of Machine Learning to projects near Frankfurt am Main ,Germany.)

  
**Edit:** Maybe my wording was not so correct. Let us agree that in most cases the symmetry assumption is satisfied by MLPs with invertible activation function. The idea I would like to discuss is the usage of Fourier coefficients to (re-) construct the weights w(r,s) = w(s,r) . For this idea to make sense the FWNN do not learn the weights as usual MLPs / ANNs , but they learn the \_coefficients\_ of the Fourier series (at least some of them). By adjusting how many coefficients are learned, the FWNN could adjust its capacity to learn. Notice that by symmetry of the function w(r,s) we get terms like sum\_{j\] c\_j\*cos(j \* (r+s) ) where j ranges over some predefined range \[-R,R\] of integers. In theory this R should be infinity hence Z = \[-inf, +inf\] are the whole integers. Notice also that the parameter c\_j the network learns are 2\*R+1 in number, which at first glance is independent of the number of neurons N. Hence a traditional neural network with N neurons, has in theory to learn O(N\^2) weights, but with the Fourier transform we reduce this number of parameters to 2\*R+1. Of course it can happen that R = N\^2 but I can imagine that there are problems where 2\*R+1 << N\^2. I hope this clarifies the idea.

Code: [https://github.com/githubuser1983/fourier\_weighted\_neural\_network/blob/main/fourier\_weighted\_neural\_network.py](https://github.com/githubuser1983/fourier_weighted_neural_network/blob/main/fourier_weighted_neural_network.py)

Explanation of the method: [https://www.academia.edu/125262107/Fourier\_Weighted\_Neural\_Networks\_Enhancing\_Efficiency\_and\_Performance](https://www.academia.edu/125262107/Fourier_Weighted_Neural_Networks_Enhancing_Efficiency_and_Performance)",25,7,0.89,2024-11-03 15:27:52,musescore1983,[],0,1784,1341,1618,True,0.0859375,NEGATIVE,0.8849539160728455
[P] Using Machine Learning to search through video ,since i cant post video here heres a tiktok link i go through the steps my platforms pipeline goes through to be able to search through videos for example you could have gbs of videos stored and be looking for all scenes within the videos containing a sunrise simply type sunrise to search and in an instant you have access to all the sunrise scenes,"
Since I can’t post video here, here’s a tiktok link , i go through the steps my platforms pipeline goes through to be able to search through videos. 

For example, you could have GBs of videos stored, and be looking for all scenes within the videos containing a sunrise, simply type sunrise to search, and in an instant, you have access to all the sunrise scenes.  

https://vm.tiktok.com/ZMhXmXd5C/",0,0,0.29,2024-11-03 13:34:44,Current-Morning8533,[],0,349,8,24,True,0.16666666666666666,NEGATIVE,0.9900851249694824
[D][R] PCX: JAX library for Predictive Coding Networks at scale,hello everyone over the last year ive been working on a library to build set up and test predictive codig networks with ease at the moment im not really active in the area anymore but i really believe to be an exciting research field and would love if any new people got involved with it so im just putting it out here to maybe catch someones curiosity in short predictive coding is one kind of energybased learning method energy based networks are basically alternative neural networks that can be trained with relying on backpopagation theres also an associated paper but i dont think its that interesting just look at the pictures maybe p 240701163 benchmarking predictive coding networks made simple im very happy to discuss the topic if anyone is interested into it and i can also give concrete research directions to follow,"Hello everyone,

Over the last year I've been working on a library to build, set up, and test predictive codig networks with ease. At the moment I'm not really active in the area anymore, but I really believe to be an exciting research field and would love if any new people got involved with it. So I'm just putting it out here to maybe catch someone's curiosity: [https://github.com/liukidar/pcx](https://github.com/liukidar/pcx) . In short, predictive coding is one kind of energy-based learning method. Energy based networks are basically alternative neural networks that can be trained with relying on backpopagation.

There's also an associated paper but I don't think it's that interesting (just look at the pictures maybe :P): [\[2407.01163\] Benchmarking Predictive Coding Networks -- Made Simple](https://arxiv.org/abs/2407.01163)

I'm very happy to discuss the topic if anyone is interested into it, and I can also give concrete research directions to follow!",1,0,1.0,2024-11-03 12:59:53,liukidar,[],0,829,2637,2072,True,0.28484848484848485,POSITIVE,0.9448028802871704
[D] AAAI Phase 2 Results,when should we expect the results from phase 2 of aaai 2025 submissions on the site the authors feedback is from day 4 to day 8 of november are we going to receive the results today day 3,"When should we expect the results from phase 2 of AAAI 2025 submissions? On the site, the authors feedback is from day 4 to day 8 of November. Are we going to receive the results today, day 3?",9,1,0.8,2024-11-03 11:39:39,Massive_Horror9038,[],0,187,1424,271,True,0.0,POSITIVE,0.6339395642280579
[R]📊 Loan Risk Forecasting Project Using ML 📈,hi everyone i launched my project on github loan risk forecasting ml and financial analysis key highlights data analysis exploratory data analysis eda to uncover insights in loan data ml models implementation of various algorithms to predict loan defaults visualizations clear visuals to illustrate findings and performance metrics this project demonstrates how machine learning can enhance risk assessment in finance id love to hear your feedback or suggestions if you find it helpful a star on github would be appreciated thanks,"Hi everyone!

I launched my project on GitHub: [Loan Risk Forecasting - ML and Financial Analysis](https://github.com/Salma0-8/Loan-Risk-Forecasting-ML-and-Financial-Analysis/blob/main/README.md).

# Key Highlights:

* **Data Analysis:** Exploratory data analysis (EDA) to uncover insights in loan data.
* **ML Models:** Implementation of various algorithms to predict loan defaults.
* **Visualizations:** Clear visuals to illustrate findings and performance metrics.

This project demonstrates how machine learning can enhance risk assessment in finance. I’d love to hear your feedback or suggestions!

If you find it helpful, a star on GitHub would be appreciated. Thanks!",0,2,0.4,2024-11-03 11:21:56,SituationPuzzled5520,[],0,530,14,1,True,0.14285714285714285,POSITIVE,0.9951121211051941
[D] AAAI 2025 Phase 2 Reviews,the reviews will be available soon this is a thread for discussionrants be polite in comments,The reviews will be available soon. This is a thread for discussion/rants. Be polite in comments.,94,531,0.95,2024-11-03 11:09:33,quasi-literate,[],0,93,173,38,True,0.4,NEGATIVE,0.9822075963020325
[D] Simple Questions Thread,please post your questions here instead of creating a new thread encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",5,21,1.0,2024-11-03 11:00:17,AutoModerator,[],0,285,4692,2000,True,0.06767676767676768,NEGATIVE,0.9866622090339661
[D] Looking for Research Internship in Applied RL & Robotics,i am a phd candidate at mila working on reinforcement learning for different robotic applications worked on applications like excavator automation physicsbased character animation and autonomous driving im currently seeking a summer research internship for 2025 and im really interested in any roles that focus on applied rl or embodied ai heres a bit about my research journey so far automatic reward modeling developed methods for deriving reward functions from expert demonstration for excavator automation in vortex simulator presented at the neurips rl for reallife applications workshop sampleefficient rl improved sample efficiency on the atari benchmark through transformerbased discrete world modeling icml 2024 compositional motion priors for multitask rl im currently working on multitask learning for robotic locomotion with compositional motion priors using isaac gym rl for autonomous driving designed a curriculum learning method for autonomous driving on the carla simulator eliminating the need for complex reward shaping inria research student im also exploring the use of diffusion models alongside rl for stable diverse control strategies if anyone knows of relevant openings or has any advice on places that may value applied rl research id really appreciate it thank you so much for any leads or suggestions my cv and more details are on my,"I am a PhD candidate at Mila, working on reinforcement learning for different robotic applications (worked on applications like excavator automation, physics-based character animation, and autonomous driving). I'm currently seeking a summer research internship for 2025, and I'm really interested in any roles that focus on applied RL or embodied AI.

Here’s a bit about my research journey so far:

* **Automatic Reward Modeling**: Developed methods for deriving reward functions from expert demonstration for excavator automation in Vortex Simulator. (Presented at the NeurIPS RL for Real-life Applications workshop.)
* **Sample-Efficient RL**: Improved sample efficiency on the Atari benchmark through transformer-based discrete world modeling. (ICML 2024)
* **Compositional Motion Priors for Multi-Task RL**: I'm currently working on multi-task learning for robotic locomotion with compositional motion priors, using Isaac Gym.
* **RL for Autonomous Driving**: Designed a curriculum learning method for autonomous driving on the CARLA simulator, eliminating the need for complex reward shaping. (Inria research student).

I’m also exploring the use of Diffusion Models alongside RL for stable, diverse control strategies.

If anyone knows of relevant openings or has any advice on places that may value applied RL research, I’d really appreciate it.

Thank you so much for any leads or suggestions!

*My CV and more details are on my* https://pranaval.github.io/*.*",0,1,0.43,2024-11-03 10:55:19,Personal_Click_6502,[],0,1362,217,25,True,0.153125,NEGATIVE,0.7727050185203552
[D] Are there statistical techniques to identify the subsets of data where your ML model is particularly more predictive?,hopefully this question isnt deemed as too amateurish for this subreddit so ill just use a simplified finance example say i want to predict security returns over the next 5 minutes my x matrix is just a time series of feature values sampled every minute in the day that the market is open could be stuff like buy volume sell volume relative returns of other securities etc so i fit an ml model and i get a certain r squared and rmse based on my intuition i know that there are certain situations where the model has more predictive power than average it could be something arbitrary like the hour of the day is 11 am and the relative return of my security compared to some other security exceeds some threshold lets say in this situation the model is a surefire bet in fact if i were to only consider the subset of data fulfilling my criteria i could predict the y_labels almost perfectly each time and just for theoretical discussions sake lets say this fully translates to the test set and live prediction as well we just fully understand this phenomenon in the real world as long as our conditions are satisfied however all of this gets watered down because my model is evaluated over the full data set and it doesnt do so well in most other situations i dont want to just rely on my intuition to figure out what these subsets of data are are there statistical techniques to employ that can inform me of subsets of data which are extra predictive of course id want each subset to have at least some number of samples to evidence significance and beyond this are there techniques that can identify these subsets at the model training phase this feels a bit parallel to bagging but the intent is different im not aiming for some generalized reduction in variance in a broader model i want to know the models that seem to work particularly well on some subsets,"Hopefully this question isn't deemed as too amateurish for this subreddit:

So I'll just use a simplified finance example: Say I want to predict security returns over the next 5 minutes. My X matrix is just a time series of feature values sampled every minute in the day that the market is open (could be stuff like buy volume, sell volume, relative returns of other securities, etc.)

So I fit an ML model and I get a certain R squared and rmse. Based on my intuition, I know that there are certain situations where the model has more predictive power than average. It could be something arbitrary, like the hour of the day is 11 AM and the relative return of my security compared to some other security exceeds some threshold. Let's say in this situation the model is a surefire bet, in fact if I were to only consider the subset of data fulfilling my criteria, I could predict the y\_labels almost perfectly each time. And just for theoretical discussion's sake, let's say this fully translates to the test set and live prediction as well, we just fully understand this phenomenon in the real world as long as our conditions are satisfied.

However, all of this gets watered down because my model is evaluated over the full data set and it doesn't do so well in most other situations. I don't want to just rely on my intuition to figure out what these subsets of data are, are there statistical techniques to employ that can inform me of subsets of data which are extra predictive? (of course, I'd want each subset to have at least some number of samples to evidence significance)

And beyond this, are there techniques that can identify these subsets at the model training phase? This feels a bit parallel to bagging, but the intent is different - I'm not aiming for some generalized reduction in variance in a broader model, I want to know the models that seem to work particularly well on some subsets",1,0,1.0,2024-11-03 10:23:45,thoughtdump9,[],0,1860,2865,763,True,0.0717015392015392,NEGATIVE,0.9989116191864014
"[D] Publishing in NeurIPS, ICML, ICLR as an Early Researcher: Any Advice?",im currently pursuing a masters degree and my goal is to publish a paper in one of the top aiml venues like neurips icml or iclr before i finish my program im studying at a federal university in brazil which is wellregarded locally but doesnt have much international recognition my research lab is somewhat unstructuredwe mainly share computational resources but dont have collaborative or largescale projects because of this i dont have an ongoing project i can join for guidance or support additionally my supervisors research focus is more on applied machine learning in chemistry so he doesnt have experience publishing in these top conferences this means i dont have direct mentorship on the publishing process specific to these venues to give some context neuripss call for papers is expected around may 2025 so i still have some time but want to prepare as thoroughly as possible id really appreciate any advice on how to increase my chances of getting published in these venues for example ive heard that it helps to cite potential reviewers in your work any tips on how to navigate the process write in a way that aligns with these conferences or understand what reviewers might be looking for would be helpful id also like advice on handling rejection like potential backup venues to consider if my paper isnt accepted,"I'm currently pursuing a master's degree, and my goal is to publish a paper in one of the top AI/ML venues, like NeurIPS, ICML, or ICLR, before I finish my program. I'm studying at a Federal University in Brazil, which is well-regarded locally but doesn’t have much international recognition. My research lab is somewhat unstructured—we mainly share computational resources but don’t have collaborative or large-scale projects. Because of this, I don’t have an ongoing project I can join for guidance or support.

Additionally, my supervisor’s research focus is more on applied machine learning in chemistry, so he doesn’t have experience publishing in these top conferences. This means I don’t have direct mentorship on the publishing process specific to these venues. To give some context, NeurIPS's call for papers is expected around May 2025, so I still have some time but want to prepare as thoroughly as possible.

I’d really appreciate any advice on how to increase my chances of getting published in these venues. For example, I’ve heard that it helps to cite potential reviewers in your work. Any tips on how to navigate the process, write in a way that aligns with these conferences, or understand what reviewers might be looking for would be helpful. I’d also like advice on handling rejection, like potential backup venues to consider if my paper isn’t accepted.",0,11,0.48,2024-11-03 10:16:15,mrlucasrib,[],0,1328,1648,3,True,0.13333333333333333,NEGATIVE,0.994390070438385
[R] What is your Recipe for Training Neural Networks in 2024?,you may already know the recipe for training neural networks bible from karpathy 2019 while most of the advices are still valid the landscape of deep learning modelmethod has changed a lot since karpathys advices work well in the supervised learning setting he does mention it stick with supervised learning do not get overexcited about unsupervised pretraining unlike what that blog post from 2008 tells you as far as i know no version of it has reported strong results in modern computer vision though nlp seems to be doing pretty well with bert and friends these days quite likely owing to the more deliberate nature of text and a higher signal to noise ratio ive been training a few image diffusion models recently and i find it harder to make data driven decisions in the unsupervised setting metrics are less reliable sometimes i train models with better losses but when i look at the samples they look worse do you know more modern recipes to train neural network in 2024 and not just llms,"You may already know the [Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/) bible from Karpathy 2019

While most of the advices are still valid, the landscape of Deep Learning model/method has changed a lot since. Karpathy's advices work well in the supervised learning setting, he does mention it:

>stick with supervised learning. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).

I've been training a few image diffusion models recently, and I find it harder to make data driven decisions in the unsupervised setting. Metrics are less reliable, sometimes I train models with better losses but when I look at the samples they look worse

Do you know more modern recipes to train neural network in 2024? (and not just LLMs)",171,44,0.98,2024-11-03 10:05:44,Even_Information4853,[],0,996,1416,742,True,0.12037037037037036,POSITIVE,0.7243290543556213
[P] Understanding Multimodal LLMs: The Main Techniques and Latest Models,,,48,8,0.99,2024-11-03 09:34:30,seraschka,[],0,0,2216,5312,True,0.0,POSITIVE,0.7481210231781006
