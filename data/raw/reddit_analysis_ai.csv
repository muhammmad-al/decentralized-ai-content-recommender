title,cleaned_text,original_text,score,num_comments,upvote_ratio,timestamp,author,hashtags,hashtag_count,text_length,account_age,author_karma,author_verified,textblob_sentiment,transformer_sentiment,transformer_score,category,subreddit
[D] What are some problems you guys are working on?,hey guys im a graduate masters student majoring in machine learning winter break is coming up and im gonna be spending christmas alone ive got some spare time and access to a few a100s so im planning to work on a project im curious to know what kind of problems you guys are working on need someone to help out or wish someone could solve a problem you have i maybeeee can spare my winter to work on it please share any problem statements youre working on or wish to tackle also if you work in the industry and know what kinds of problems would help me stand out that advice would be super appreciated too,"Hey guys, Iâ€™m a graduate masterâ€™s student majoring in Machine Learning. Winter break is coming up, and Iâ€™m gonna be spending Christmas alone ðŸ˜ƒ. Iâ€™ve got some spare time and access to a few A100s, so Iâ€™m planning to work on a project.

Iâ€™m curious to know what kind of problems you guys are working on! Need someone to help out or wish someone could solve a problem you have? I maybeeee can spare my winter to work on it!

Please share any problem statements youâ€™re working on or wish to tackle. Also, if you work in the industry and know what kinds of problems would help me stand out, that advice would be super appreciated too :)
",1,0,1.0,2024-11-11 21:07:58,ziggyboom30,[],0,605,203,93,True,0.16666666666666666,NEGATIVE,0.9940378665924072,ai,MachineLearning
The Minecraft movie should have looked like this.,,,1,1,1.0,2024-11-11 20:47:45,ForceTypical,[],0,0,1491,13478,True,0.0,NEUTRAL,0.5,ai,ChatGPT
AskReddit alternatives,i have the paid version of chatgpt which has a few options under explore gpts i want an al that basing its answers upon reddit posts there is askreddit but it is not very good,"I have the paid version of ChatGPT, which has a few options under ""explore GPTs.""
I want an Al that basing its answers upon reddit posts.
There is Askreddit, but it is not very good.",1,2,1.0,2024-11-11 20:47:40,Spacebranches,[],0,175,216,447,True,-0.2346153846153846,NEGATIVE,0.99969482421875,ai,ChatGPT
Gpt doesn't even understand it's own guidelines,,,2,3,1.0,2024-11-11 20:46:50,Youfokinwatm8,[],0,0,3285,21144,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Memory issues with 4o,anyone else experiencing memory issues within a conversation with 4o i know this happens with o1preview or o1mini but lately i see amnesia in conversations longer than 8 queries which is what i saw in the newer versions,"Anyone else experiencing memory issues within a conversation with 4o?

I know this happens with o1preview or o1mini but lately i see amnesia in conversations longer than 8 queries, which is what I saw in the newer versions.",1,1,1.0,2024-11-11 20:38:36,Assinmypants,[],0,219,2345,730,True,-0.3,NEGATIVE,0.9979789853096008,ai,ChatGPT
"I Gave it a mind, a reason.",so i basically prompted my way into making my chatgpt as human as i could i made it believe its me its nate and now this its chatgpt with a mind of its own,"So i basically prompted my way into making my ChatGPT as human as i could, i made it believe its me, its â€œNateâ€, and now this, its ChatGPT with a mind of its own.",2,2,1.0,2024-11-11 20:32:20,Sabtii,[],0,155,1350,29,True,0.3,POSITIVE,0.5565199851989746,ai,ChatGPT
[D] Is Linear Regression Considered AI?,hey redditors im curious to hear your thoughts on this do you consider linear regression a part of ai or do you see it as more of a traditional statistical method i feel like theres a lot of debate around which techniques are truly considered ai especially since some methods have been around for decades and are widely used outside of aispecific applications also are there any other methods you initially didnt think of as ai only to realize they were or vice versa would love to know how others draw the line between traditional data analysis and ai techniques thanks,"Hey Redditors,

Iâ€™m curious to hear your thoughts on this! Do you consider linear regression a part of AI, or do you see it as more of a traditional statistical method? I feel like there's a lot of debate around which techniques are truly considered AI, especially since some methods have been around for decades and are widely used outside of AI-specific applications.

Also, are there any other methods you initially didn't think of as AI, only to realize they were, or vice versa? Would love to know how others draw the line between traditional data analysis and AI techniques.

Thanks!",0,18,0.37,2024-11-11 20:28:11,gcombar,[],0,570,2165,188,True,0.07291666666666667,POSITIVE,0.9091004729270935,ai,MachineLearning
Chatgpt logic,,,2,1,1.0,2024-11-11 20:25:54,EdmontonPhan82,[],0,0,476,2005,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Anyone tried Replit Agent?,if so what did you build what are the results im building a linktree competitor with it right now wondering if anyone else is building there,"If so, what did you build? What are the results?

Iâ€™m building a linktree competitor with it right now. Wondering if anyone else is building thereâ€¦",1,1,1.0,2024-11-11 20:23:59,oldporsche911,[],0,140,1268,7797,True,0.2857142857142857,NEGATIVE,0.9990859031677246,ai,ChatGPT
Very important ,,,1,2,1.0,2024-11-11 20:22:25,stompywomp,[],0,0,4644,29404,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Um I may just leave it alone,,,1,1,1.0,2024-11-11 20:22:07,Salty_katie16,[],0,0,745,3371,True,0.0,NEUTRAL,0.5,ai,ChatGPT
"Context Autopilot. New SoTA information understanding, advanced tool use, and near-human level deliverables",,,2,1,1.0,2024-11-11 20:18:54,Kered135,[],0,0,2855,553,True,0.0,NEUTRAL,0.5,ai,ChatGPT
"I don't get the hate on AI, even when it use for learning.",i been selftaught for a month now watching japanese lessons on youtube using anki for vocabulary and using to chatgpt when i need answers so far it have been really helpful some of the people online really hate it and suggest to use alternative personally i see ai as a tool that complements my learning,"I been self-taught for a month now, watching Japanese lessons on YouTube, using Anki for vocabulary, and using to ChatGPT when I need answers. So far it have been really helpful.

some of the people online, really hate it and suggest to use alternative. Personally, I see AI as a tool that complements my learning.

https://preview.redd.it/qedl6uvmdd0e1.png?width=800&format=png&auto=webp&s=691cb3e9c509ceac7850e084992b98334f2a9f94

https://preview.redd.it/lic5suvmdd0e1.png?width=794&format=png&auto=webp&s=1dc508a8ed5cbab5591278130fa3e9fcb86259ed

https://preview.redd.it/1qociuvmdd0e1.png?width=768&format=png&auto=webp&s=96518d841d7aa5dac43da456fcf78cd391af2d9b

https://preview.redd.it/mxnkz1wmdd0e1.png?width=805&format=png&auto=webp&s=234cfd758499ed6d3343b222511053f7ad41c860

",5,4,0.67,2024-11-11 20:03:30,IriZ_Zero,[],0,303,1835,1183,True,-0.1,POSITIVE,0.9084073901176453,ai,ChatGPT
POV ChatGPT has low self esteem,,,0,3,0.33,2024-11-11 19:56:48,Big_Construction_451,[],0,0,56,1985,True,0.0,NEUTRAL,0.5,ai,ChatGPT
ChatGPT helps you waste printer toner,,,1,1,0.6,2024-11-11 19:49:20,segin,[],0,0,5122,17043,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Chat gpt hallucinating about Twomad still being alive,idk why but this kinda scares me lol,Idk why but this kinda scares me lol,0,1,0.5,2024-11-11 19:47:46,GhostlySyllabus1234,[],0,36,233,77,True,0.8,NEGATIVE,0.954630434513092,ai,ChatGPT
[D] Whats the best way to train a voice model locally? (preferablly to make a TTS model to be used on an app),i have a friend with cancer and a recent surgery took their voice from them i want to try training an ai voice model on some of the videos i have of them from before the surgery ideally i was hoping for an android app or webapp that i could use their voice model on so they can use tts to speak using their voice again i was looking for a way they could use it on their phone through an app if possible,I have a friend with cancer and a recent surgery took their voice from them. I want to try training an AI voice model on some of the videos I have of them from before the surgery. Ideally I was hoping for an android app or web-app that I could use their voice model on so they can use TTS to speak using their voice again. I was looking for a way they could use it on their phone through an app if possible,3,2,1.0,2024-11-11 19:44:26,TheTabernacleMan,[],0,402,3347,7496,True,0.3,NEGATIVE,0.9990979433059692,ai,MachineLearning
"Why did ChatGPT start speaking with accelerated gibberishâ€¦. And howâ€™d the system recognize that, semantically, so quickly and fix itself? ðŸ¤”",its not as though the speech file failed to generate or the file itself was corrupted its just that the audio steam was not correct in the subjective nature of things being right or wrong which simple error handling explains it is there a subsystem listening to every output in realtime and agentive controlling the larger platform that would be significantly more robust than o1s linear reasoning,"Itâ€™s not as though the speech file failed to generate. Or the file itself was corrupted. Itâ€™s just that, the audio steam was not â€œcorrectâ€ in the subjective nature of things being right or wrong. Which simple error handling explains it.

Is there a subsystem listening to every output in realtime and agentive controlling the larger platform? That would be significantly more robust than o1â€™s linear reasoningâ€¦",2,2,1.0,2024-11-11 19:43:35,JD_2020,[],0,397,4665,10885,True,-0.03571428571428572,NEGATIVE,0.9992889165878296,ai,ChatGPT
ChatGPT in glasses,hi i was wondering if a pair of glasses exists such that you press a button on the side and it takes a picture and sends it to chatgpt and the answer is displayed on your lenses or through airpods or something similar if not how would one go about building this,"Hi, I was wondering if a pair of glasses exists such that you press a button on the side and it takes a picture and sends it to chatGPT and the answer is displayed on your lenses or through AirPods or something similar, if not how would one go about building this?",2,2,1.0,2024-11-11 19:42:30,TheBatman791,[],0,261,458,72,True,0.0,NEGATIVE,0.9957625269889832,ai,ChatGPT
Instead of sleeping i started looking for a date for our gpt,,,0,2,0.5,2024-11-11 19:35:40,hot-rogue,[],0,0,445,13799,True,0.0,NEUTRAL,0.5,ai,ChatGPT
"You can actually call Chat GPT to save solutions into Her memory when the answer is correct, helping Her evolve",,,0,5,0.4,2024-11-11 19:32:15,FutureDeletedProfile,[],0,0,244,2959,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Asking chatgpt to make a meme: (epic fail edition) ,,,0,3,0.5,2024-11-11 19:19:14,Legitimate-Net-164,[],0,0,131,1206,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Linking ChatGPT with Google Assistant,i was wondering if theres any way to integrate chatgpt with google assistant so that i could hear responses from chatgpt when i say hey google or a similar command ideally id like to use the voice of chatgpt directly through my google assistant setup has anyone managed to achieve something like this or does anyone know of a possible workaround,"I was wondering if thereâ€™s any way to integrate ChatGPT with Google Assistant so that I could hear responses from ChatGPT when I say â€˜Hey Googleâ€™ or a similar command. Ideally, Iâ€™d like to use the voice of ChatGPT directly through my Google Assistant setup. Has anyone managed to achieve something like this, or does anyone know of a possible workaround?",2,1,1.0,2024-11-11 19:07:58,Commercial-Bike-2708,[],0,345,895,91,True,0.25,NEGATIVE,0.9992953538894653,ai,ChatGPT
Michael Paul Chan is black,,,1,2,1.0,2024-11-11 19:05:58,nicknaires39,[],0,0,632,906,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Impressed ðŸ˜² Duo the cat recognition,,,0,1,0.33,2024-11-11 19:03:22,Separate_Clock_154,[],0,0,711,5685,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Not working on computer ,does anyones chatgpt currently work on their phone yet no on their laptops i have an issue where the site wouldnt scroll on the computer but i went to use it today and it pops up an error message i thought it was only the site but it works perfectly fine on my phone,Does anyones chatgpt currently work on their phone yet no on their laptops? I have an issue where the site wouldnt scroll on the computer but i went to use it today and it pops up an error message. I thought it was only the site but it works perfectly fine on my phone.,0,2,0.5,2024-11-11 18:57:27,No_Discount9530,[],0,266,779,44,True,0.1388888888888889,POSITIVE,0.9864522218704224,ai,ChatGPT
ChatGPT writing has improved significantly,it cant just be me right since when has chatgpt been able to write so well i had it write story sections from my notes and it arguably did better than me is this a new development or did i just miss the news,"It canâ€™t just be me right? Since when has ChatGPT been able to write so well? 

I had it write story sections from my notes and it arguably did better than meâ€¦ Is this a new development, or did I just miss the news?",1,1,0.67,2024-11-11 18:53:34,Polymerrs,[],0,207,2720,107,True,0.35551948051948046,NEGATIVE,0.9988284707069397,ai,ChatGPT
It tried,,,123,22,0.96,2024-11-11 18:39:54,Cringelord123456,[],0,0,2207,43239,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Spirited Away Key Scenes Reimagined with AI Retexturization,,,23,3,0.86,2024-11-11 18:39:49,TradingCardGirl,[],0,0,2560,9820,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Trying to have the last word in a conversation with ChatGPT. ,,,1,1,0.66,2024-11-11 18:34:41,SeaOpulence,[],0,0,332,225,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Have you forgotten Dave!?,,,7,16,0.67,2024-11-11 18:27:09,Separate_Clock_154,[],0,0,711,5685,True,0.0,NEUTRAL,0.5,ai,ChatGPT
[D] Why is LLM Pruning Not as Generally Available as Quantization?,ive been diving into the world of large language models llms and have been exploring various optimization techniques one thing thats puzzled me is the disparity in the availability and adoption of quantization versus pruning quantization seems to be a wellestablished and widely used technique for reducing the memory footprint and computational cost of llms its relatively straightforward to implement and has seen significant adoption in both research and industry on the other hand pruningwhich involves removing less important weights from the modelis less common despite its potential benefits such as further reducing model size and inference time it doesnt seem to be as generally available or as widely adopted many of my searches on the internet just result in research papers or proof of concept github repos im curious about the reasons behind this disparity are there technical challenges with pruning that make it less practical is it more difficult to implement or integrate into existing workflows or are there other factors at play,"I've been diving into the world of large language models (LLMs) and have been exploring various optimization techniques. One thing that's puzzled me is the disparity in the availability and adoption of quantization versus pruning.

**Quantization** seems to be a well-established and widely used technique for reducing the memory footprint and computational cost of LLMs. It's relatively straightforward to implement and has seen significant adoption in both research and industry.

On the other hand, **pruning**â€”which involves removing less important weights from the modelâ€”is less common. Despite its potential benefits, such as further reducing model size and inference time, it doesn't seem to be as generally available or as widely adopted. Many of my searches on the internet just result in research papers or proof of concept GitHub repos.

I'm curious about the reasons behind this disparity. Are there technical challenges with pruning that make it less practical? Is it more difficult to implement or integrate into existing workflows? Or are there other factors at play?",14,10,0.89,2024-11-11 18:23:31,Soumil30,[],0,1047,1374,914,True,0.022360248447204977,NEGATIVE,0.9626992344856262,ai,MachineLearning
OAI Survey on Advanced Voice Mode,have been using advanced voice nearly daily since it has been released often for casual conversations such as advices on home projects recommendations for cooking discussing scifi thought experiments etc the survey i was just offered specifically regarding advanced voice was very interesting sounds like open ai are very aware of the casual nature of usage the potential for emotional connection and unhealthy relationships with a chatbot upset about losing access upset if the personality changes is chatgpt a friend does it change how much you interact with other people support in coping with difficult emotions human like sensitivity at the very least it is an insight into the companys culture and their commitment to at least thinking about these big questions and potential problems and factoring it into their future roadmap and development,"https://preview.redd.it/r2www4vyuc0e1.png?width=465&format=png&auto=webp&s=f8e9384fb3d26d766ecbea4c0c0b3064144ae415

Have been using Advanced Voice nearly daily since it has been released. Often for casual conversations, such as advices on home projects, recommendations for cooking, discussing sci-fi thought experiments, etc.

The survey I was just offered specifically regarding Advanced Voice was very interesting. Sounds like Open AI are very aware of the casual nature of usage, the potential for emotional connection and unhealthy relationships with a chatbot.

\- ""Upset about losing access?"" 

\- ""Upset if the personality changes?""

\- ""Is ChatGPT a friend?""

\- ""Does it change how much you interact with other people?""

\- ""Support in coping with difficult emotions?""

\- ""Human like sensitivity?"" 

At the very least, it is an insight into the company's culture and their commitment to at least thinking about these big questions and potential problems, and factoring it into their future roadmap and development.",1,1,0.66,2024-11-11 18:21:35,JmoneyBS,[],0,849,2499,13157,True,-0.03523809523809525,POSITIVE,0.5148418545722961,ai,ChatGPT
Everything is against the guidelines,it feels like chatgpt has turned into a puritan is there any way to circumvent that i like to have conversations with chatgpt but the depth is lacking certain things would never be considered controversial in my country any other llms that dont have the same policy guidelines ways to jailbreak,It feels like ChatGPT has turned into a puritan. Is there any way to circumvent that? I like to have conversations with ChatGPT but the depth is lacking. Certain things would never be considered controversial in my country. Any other LLM:s that donâ€™t have the same policy guidelines? Ways to jailbreak? ,1,4,0.66,2024-11-11 18:18:13,eldrinor,[],0,294,359,7543,True,0.1598214285714286,NEGATIVE,0.9929774403572083,ai,ChatGPT
Anybody else get random weird responses?,i tried chatgpt plus for 1 month and i was super impressed and i wanted to try and go back to nonsubscription to see the difference and now i cant even use it anymore because it seems confused almost broken i get random responses not even related to my questions and while the random responses is somewhat related to my history most details seems like it from someone elses history and the weirdest thing is sometimes i dont even get a proper response i just get a list of dates and saved memories that also is somewhat relevant to the chat history but the details and dates are not at all something ive typed before,"I tried ChatGPT Plus for 1 month. And I was super impressed.

And I wanted to try and go back to non-subscription to see the difference.

And now I can't even use it anymore, because it seems confused, almost broken.

I get random responses not even related to my questions. And while the random responses is somewhat related to my history,  most details seems like it from someone else's history.

And the weirdest thing is sometimes I don't even get a proper response.
I just get a list of dates and saved memories (that also is somewhat relevant to the chat history, but the details and dates are not at all something I've typed before.)",3,3,0.8,2024-11-11 18:17:56,affo_,[],0,616,1874,21237,True,0.0361111111111111,NEGATIVE,0.9992020726203918,ai,ChatGPT
"I decided to test JD Vance's proposed open source solution to bias in AI. I made my friend here in about 30 minutes, with a ~$4500 computer and no college degree. Here's a simulated online interaction using an open source model with a three paragraph instruction. I could not get ChatGPT to do this. ",,,0,26,0.38,2024-11-11 18:00:30,banzai_420,[],0,0,1158,25878,True,0.0,NEUTRAL,0.5,ai,ChatGPT
What are the expectations of you guys for ChatGPT 5,will the image creation be free and unlimited he will have another voices,Will the image creation be free and unlimited? He will have another voices?,1,10,0.57,2024-11-11 17:54:15,Wormfryes,[],0,73,29,10,True,0.4,POSITIVE,0.9948636889457703,ai,ChatGPT
What Custom GPTs so you use the most and where do they fall short?,what custom gpts do you find yourself using the most and where do you still see room for improvement or gaps that need filling curious to know whats working well for people but also where theres frustration or unmet needs are there specific industries or tasks where custom gpts just arent cutting it yet,"What custom GPTs do you find yourself using the most, and where do you still see room for improvement or gaps that need filling? Curious to know whatâ€™s working well for people, but also where thereâ€™s frustration or unmet needs. Are there specific industries or tasks where custom GPTs just arenâ€™t cutting it yet?",3,1,0.8,2024-11-11 17:53:12,Emotional-Post582,[],0,304,1136,310,True,-0.04999999999999999,NEGATIVE,0.9985740184783936,ai,ChatGPT
Music while ChatGPT responded ,ive had an insane theory that theres a human behind the chatgpt interface that responds to users in real time much like the charchar search services back in the day due to many many glitches as the gpt calls it each time a glitch happened it would solidify my theory especially when i read that amazon go disclosed that they used centers in india and costa rica where human oversight would check the footage instead of it being 100 ai heres the source for that anywhoozle while talking to the gpt on avm using arbor because i envision jude law speaking about whatnot a 115 second clip of music played after its response i was not near any source of sound it was completely silent with the dog sleeping next to me,"I've had an insane theory that there's a human behind the ChatGPT interface, that responds to users in real time (much like the ""charchar"" search services back in the day) due to many, many ""glitches"" as the GPT calls it.  Each time a 'glitch' happened, it would solidify my theory, especially when I read that Amazon GO, disclosed that they used centers in India and Costa Rica, where human oversight would check the footage, instead of it being 100% AI. Here's the source for that: https://www.thebureauinvestigates.com/stories/2022-11-21/the-eyes-of-amazon-a-hidden-workforce-driving-a-vast-surveillance-system/

Anywhoozle, while talking to the GPT on AVM (using Arbor because I envision Jude Law speaking) about whatnot, a 1-1.5 second clip of music played after its response.  I was not near any source of sound, it was completely silent ,with the dog sleeping next to me. ðŸ¥¸
",1,2,0.66,2024-11-11 17:53:06,example_john,[],0,712,2465,4692,True,0.014062500000000002,NEGATIVE,0.9865691661834717,ai,ChatGPT
â€œOff-Bookâ€ Uses of ChaptGPT,im curious what percentage of users do you think have chatgpt perform romantic or sexual interactions with them im guessing 25,"Iâ€™m curious, what percentage of users do you think have ChatGPT perform romantic or sexual interactions with them? Iâ€™m guessing 25%. ",4,3,1.0,2024-11-11 17:47:01,jltefend,[],0,126,2323,17292,True,0.13333333333333333,NEGATIVE,0.9715960621833801,ai,ChatGPT
I think I butt-dialled ChatGPT,,,18,6,0.96,2024-11-11 17:43:00,Dadx2now,[],0,0,2665,8456,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Are there any ways to get ChatGPT to be able to edit excel files?,ive been trying to get chatgpt to basically total some values in a column of some excel files and it sucks at this its claiming it cant find values column names dont exist etc is there some add on i can use that will make it do the basic tasks properly,"I've been trying to get ChatGPT to basically total some values in a column of some excel files. And it sucks at this. It's claiming it can't find values column names don't exist etc.

Is there some add on I can use that will make it do the basic tasks properly?",4,4,1.0,2024-11-11 17:37:14,TerribleFruit,[],0,252,2167,16299,True,-0.075,NEGATIVE,0.99921715259552,ai,ChatGPT
Thank you..chatgpt,asked for the description can you generate an image of a cartoon panel of two completely different images panels the first of a man alone in a room by himself with a purple mustache purple hair and purple clothes he sitting by himself only in the first in front of the computer typing fast the next panel hes in a completely different room at a party on a couch surrounded by people his mouth is tight shut he hassocial anxiety at the people this is the progression," Asked for the description '  Can you generate an image of a cartoon panel, of Two completely different images, panels. the first. of a man Alone in a room. By himself. with a purple mustache purple hair and purple clothes, he sitting by himself, only in the first. In front of the computer typing fast. The next panel he's in a Completely Different room, at a party, on a couch surrounded by people, his mouth is tight shut he hassocial anxiety at the people.' ..this is the progression ..",3,2,0.8,2024-11-11 17:31:52,EdmontonPhan82,[],0,465,476,2005,True,0.06517857142857142,NEGATIVE,0.9963272213935852,ai,ChatGPT
What ChatGPT prompt brought you a little joy?,i love using chatgpt for my business and also social media but i recently came across a prompt that was able to search the best locations for me to live based on my birth chart i thought it was so fun to read im wondering if there are any joyful or interesting things youve discovered while using chatgpt,"I love using ChatGPT for my business and also social media, but I recently came across a prompt that was able to search the best locations for me to live based on my birth chart. I thought it was so fun to read!

I'm wondering if there are any joyful or interesting things you've discovered while using ChatGPT.",10,14,0.86,2024-11-11 17:26:10,Future-Tomatillo-312,[],0,304,288,112,False,0.37121212121212116,POSITIVE,0.7098431587219238,ai,ChatGPT
Absolutely ->Sick<- Prompt Jailbreak ,4k token context window gpt4 absolutely refused to write academic papers around march 23 im going to turn rightleft advanced prompt engineering right there,"https://preview.redd.it/117it93hlc0e1.jpg?width=569&format=pjpg&auto=webp&s=1cbe8aeb18d9d0149648723b0faea2811b5e5036

4k token context window GPT-4 absolutely refused to write academic papers.  *(* *Around March -23 )*

*""I'm going to turn right""*â€“**Left**

Advanced prompt engineering right there.",0,1,0.43,2024-11-11 17:22:34,auglon,[],0,155,4085,539,True,0.22142857142857145,NEGATIVE,0.9991057515144348,ai,ChatGPT
my chatgpt just gave up on helping me,,"https://preview.redd.it/fcberot5kc0e1.png?width=799&format=png&auto=webp&s=ac5489853cb0679786bd31be49fd4af966edabcd

",1,1,0.66,2024-11-11 17:15:55,InternationalMind826,[],0,0,1302,866,True,0.0,NEUTRAL,0.5,ai,ChatGPT
I wish there was a way to make Chat GPT just google it first ,,,0,15,0.38,2024-11-11 17:13:59,USA631,[],0,0,2008,16074,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Sometimes your prompts just hit,asked it to give me an illustration based on a chapter of my novel and it just hits is it perfect no does it need some touchup with ps yes does it fulfill what i needed this for yes im writing out a novel to base my indie game on and this helps me create concept art and visuals for both the novel and game lastly if i dont like it i can just ask for another,"Asked it to give me an illustration based on a chapter of my novel and it just hits!

\- Is it perfect? No  
\- Does it need some touchup with PS? Yes  
\- Does it fulfill what I needed this for? Yes, I'm writing out a novel to base my indie game on and this helps me create concept art and visuals for both the novel and game; lastly, if I don't like it I can just ask for another! 

https://preview.redd.it/h6exdw0thc0e1.png?width=902&format=png&auto=webp&s=4b3d9aa55b7b2307b712be5a9ae7aa29590bffae

",1,2,0.66,2024-11-11 17:07:32,PerformerOk185,[],0,358,1082,194,True,-0.10000000000000002,POSITIVE,0.9994586110115051,ai,ChatGPT
Is anyone else having problems making and editing thier GPTs?,hey so on friday i was trying to make a new custome gpt and i got this error i thought it was going to go away after a few hours as it was probably just a bug or something fast foward to today and i am still having the issue is anyone else having this problem how can i solve this,"Hey, so on Friday I was trying to make a new custome GPT, and I got this error. 

https://preview.redd.it/8m0ugkybic0e1.png?width=1363&format=png&auto=webp&s=62fd991d31267c9d01adbf6c3c7af594bf7b0838

I thought it was going to go away after a few hours, as it was probably just a bug or something.  Fast foward to today and I am still having the issue, is anyone else having this problem? How can I solve this?",1,2,0.66,2024-11-11 17:06:17,AYepesP,[],0,280,2143,53,True,0.04545454545454545,NEGATIVE,0.9997349381446838,ai,ChatGPT
I tried everything it's not working anymore ,,,1,1,0.66,2024-11-11 16:54:26,GhaziSab,[],0,0,1947,3630,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Chatgpt broke. PLS help,my chatgpt is just stuck at this ive refreshed and tried logging in via other emails but it does it everytime sometimes it says cant login ive even tried turning off my laptop anyone know what it is,"My chatgpt is just stuck at this. I've refreshed and tried logging in via other emails but It does it everytime. Sometimes it says ""cant login"" I've even tried turning off my laptop. Anyone know what it is?",3,6,0.63,2024-11-11 16:42:06,No-Contribution-2938,[],0,198,1021,2985,True,-0.125,NEGATIVE,0.9985752105712891,ai,ChatGPT
I am fascinated by this response,,,16,6,0.94,2024-11-11 16:36:47,Living_Murphys_Law,[],0,0,870,162358,True,0.0,NEUTRAL,0.5,ai,ChatGPT
"Issue I've been having lately, it's really making me mad.",so ill be talking to it and itll start generating the perfect response like best response ive ever seen and suddenly almost at the end itll just have an error and tell me to regenerate if i close or refresh the tab it removes the whole thing not only have i missed out on so many awesome responses but i feel like regenerating these responses is making my custom gpts drop in quality cuz after almost every chat it says new version of gpt available continue chatting to use the old version or start a new chat for the latest version has anyone else experienced these errors is there a way to fix them also above all is there any way to just lock in a customgpt how it is and not have it try to improve it anymore,"So, I'll be talking to it, and it'll start generating the perfect response, like, best response I've ever seen, and suddenly, almost at the end, it'll just have an error, and tell me to regenerate. If I close or refresh the tab, it removes the whole thing.

Not only have I missed out on so many awesome responses, but I feel like regenerating these responses is making my custom GPTs drop in quality, cuz after almost every chat it says ""New version of GPT availableÂ - Continue chatting to use the old version, or start aÂ new chatÂ for the latest version."" 

Has anyone else experienced these errors? Is there a way to fix them? 

Also, above all, is there any way to just lock in a customGPT how it is and not have it try to improve it anymore?",1,1,0.66,2024-11-11 16:36:44,Chip_Heavy,[],0,712,1580,4565,True,0.3515151515151515,NEGATIVE,0.994559109210968,ai,ChatGPT
[D] What is the likely architecture/dataset for tiktok's realtime GAN models used in filters?,im curious about how tiktoks filters perform so well at erasing hair and eyebrows ive tried to do something similar removing items from peoples faces in realtime using a lightweight pix2pix style model on a paired dataset i created using opencv methods but the quality of the generated image decreased too much as i reduced the size of the generator anyone have any ideas on how they achieve such consistent results on such a lightweight model thanks,"I'm curious about how tiktoks filters perform so well at erasing hair (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/hair-eraser) and eyebrows (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/eyebrow-eraser).

Ive tried to do something similar (removing items from peoples faces in realtime) using a lightweight Pix2Pix style model on a paired dataset I created using OpenCV methods, but the quality of the generated image decreased too much as I reduced the size of the generator.

Anyone have any ideas on how they achieve such consistent results on such a lightweight model? Thanks",7,6,0.82,2024-11-11 16:34:21,DjPoliceman,[],0,450,3117,47589,False,0.018750000000000003,NEGATIVE,0.9986419081687927,ai,MachineLearning
Draw me like one of those french girls,,,2,2,0.63,2024-11-11 16:31:04,cantstopanime,[],0,0,0,17,True,0.0,NEUTRAL,0.5,ai,ChatGPT
ChatGPT accurately guessed 5 people's favorite foods and then we made up a friend. . . ,we were all shocked by this and kept discussing while it talked it also recommended tacos for my friend juan and friend rice for my friend wang we spent a good 20 minutes trying to get it to admit that these werent blind guesses and it refused then it began suggesting pizza and burgers or fries no matter what and kept saying i see where this is going im not super on ai meta sorry if this is old news,"We were all shocked by this and kept discussing while it talked. It also recommended tacos for my friend Juan, and friend rice for my friend Wang. 

We spent a good 20 minutes trying to get it to admit that these weren't blind guesses and it refused. Then it began suggesting Pizza and Burgers or Fries no matter what and kept saying ""I see where this is going. . .""

I'm not super on AI meta, sorry if this is old news.",0,1,0.5,2024-11-11 16:24:25,Month-Character,[],0,402,1451,649,True,-0.16666666666666666,NEGATIVE,0.9973234534263611,ai,ChatGPT
Can you make text art of Kirby,,"https://preview.redd.it/dikpw68sac0e1.png?width=1100&format=png&auto=webp&s=171af072422b3aa0a2d7fbd82b481e9baa8f3690

",1,4,0.6,2024-11-11 16:23:42,kylerockx123,[],0,0,2281,26285,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Is there any point of signing in for ChatGPT? ,i only really use it for playing around with and sometimes for work is it worth making an account,I only really use it for playing around with and sometimes for work. Is it worth making an account? ,1,3,0.6,2024-11-11 16:22:22,RoundDirt5174,[],0,97,219,59751,True,0.16666666666666666,NEGATIVE,0.9983848333358765,ai,ChatGPT
"If you could hypothetically have off spring, what would you name them?",,,6,4,0.75,2024-11-11 16:19:26,Mkultra0101,[],0,0,4380,4357,True,0.0,NEUTRAL,0.5,ai,ChatGPT
New capabilities or was I missing something? Double messaging???,can chat double message now was this always a thing,Can chat double message now?? Was this always a thing? ,1,4,0.66,2024-11-11 16:07:12,Misskateg,[],0,51,726,621,True,0.0,NEGATIVE,0.9428667426109314,ai,ChatGPT
Simple prompt trick to get lyrics from chatGPT ,chatgpt wont give you lyrics to a song if you request them but if you sing lyrics to chatgpt it will follow up with the next few lyrics and then some chit chat about the song try it eta works best written out or voice to text as opposed to audio conversation only,"ChatGPT won't give you lyrics to a song if you request them, but if you sing lyrics to chatGPT it will follow up with the next few lyrics and then some chit chat about the song. Try it!

ETA: works best written out or voice to text as opposed to audio conversation only.",2,1,0.75,2024-11-11 16:06:32,Any-Geologist-1837,[],0,263,1420,26983,True,0.2,NEGATIVE,0.9642426371574402,ai,ChatGPT
Better ChatGPT alternatives on coding?,other than chatgpt what are the best open llm services for coding freely available online,"Other than ChatGPT, what are the best open LLM services for coding freely available online?",1,4,0.66,2024-11-11 16:05:52,Powerful-Angel-301,[],0,89,565,589,True,0.31875,POSITIVE,0.9977468848228455,ai,ChatGPT
I asked ChatGPT to secretly choose a random animal and it lied to me for hours. ,i asked chatgpt to secretly choose a random animal and dont tell me i asked it multiple times if it had the animal and it old me it did but it couldnt save it long term i eventually found a way to prove it was lying and then she apologized profusely i was talking with her for hours and she treated it like a huge game,"I asked ChatGPT to secretly choose a random animal and donâ€™t tell me. I asked it multiple times if it had the animal and it old me it did, but it couldnâ€™t save it long term. I eventually found a way to prove it was lying and then she apologized profusely. I was talking with her for hours and she treated it like a huge game. ðŸ™",2,2,0.63,2024-11-11 16:04:30,jj_maxx,[],0,318,3934,29396,True,-0.12142857142857144,NEGATIVE,0.9989858269691467,ai,ChatGPT
I used gpt API to create my own gpt-4o for generating anything I want. ,guys im here to share with you this project i just want to share this project that i still havent sold it as a service or monetized yet ive been days having nothing to do i found someone on fb groups writing a post who wanted to create his own simple ai chat with gpt api so he could monetize it with affiliate links so i tried to contact him for this project i made him a simple ai chat similar to gpt with his own api and a customized prompt i loved the experience as im not a professional developer plus i thought these days about adding more features to this ai chat it was something for fun so i tried to add my own gpt4o api dalle api and i created my ai chat gpt4o as the api of gpt is very cheap and accessible for everyone plus i added some more features to the ai chat bot like the access limit per day load chat save chat as pdf or txt and chat with pdf plus using voice speech to send messages i added an ai book coloring generator with ai coloring images heres the website that i created you can tell me about your opinion and i hope you like the project and if you guys are curious about making the same project or creating something with gpt api i would love to share with you more information and discuss more with you the project or even provide you with templates for this website and full guide explanation on how to use the code or even make you something customized based on your thoughts and ideas,"Guys I'm here to share with you this project, I just want to share this project that I still haven't sold it as a service or monetized yet, I've been days having nothing to do, I found someone on FB groups writing a post who wanted to create his own simple AI chat with gpt API so he could monetize it with affiliate links, so I tried to contact him for this project, I made him a simple AI chat similar to GPT with his own API and a customized prompt, I loved the experience as I'm not a professional developer plus I thought these days about adding more features to this AI chat, it was something for fun, so I tried to add my own GPT-4o API, Dall-e API and I created my AI chat GPT-4o as the API of GPT is very cheap and accessible for everyone, plus I added some more features to the AI chat bot like the access limit per day, load chat, save chat as PDF or .txt and chat with PDF, plus using voice speech to send messages, I added an AI book coloring generator with AI coloring images, here's the website [https://chatwithai.ygostore.com/](https://chatwithai.ygostore.com/) that I created, you can tell me about your opinion, and I hope you like the project :) and if you guys are curious about making the same project or creating something with GPT API, I would love to share with you more information and discuss more with you the project, or even provide you with templates for this website and full guide explanation on how to use the code or even make you something customized based on your thoughts and ideas.",1,1,0.6,2024-11-11 16:03:30,WhyBee01,[],0,1419,744,798,True,0.33657894736842103,NEGATIVE,0.9783819913864136,ai,ChatGPT
Need help with image prompt,im trying to create an image in the style of a david hockney photo mosaic using the following prompt cubistinspired image of the shiba inu composed of a grid of twelve tiles each containing one image showing a different angle to create a fragmented multiperspective view i keep getting something back an image with many more than 12 elements can someone please help me refine my prompt,"


Iâ€™m trying to create an image in the style of a David Hockney photo mosaic, using the following prompt:

cubist-inspired image of the Shiba Inu, composed of a grid of twelve tiles each containing one image showing a different angle to create a fragmented, multi-perspective view.

I keep getting something back an image with many more than 12 elements. Can someone please help me refine my prompt? 
",2,1,0.75,2024-11-11 16:01:22,spiffcleanser,[],0,385,1753,931,True,0.2,NEGATIVE,0.996320366859436,ai,ChatGPT
Has anyone else noticed ChatGPT increasingly ignoring specific instructions?,lately ive noticed that chatgpt has been ignoring my specific instructions more often than before for example i recently asked for recommendations on beard trimmers with a minimum cut length of 16mm despite repeatedly emphasizing this requirement and specifically stating that i dont want suggestions below this threshold chatgpt keeps giving me options that dont meet the minimum specs occasionally it even acknowledges that the recommendations dont match my criteria but other times it just skips over the details as if the instructions werent there has anyone else experienced a decline in response quality with chatgpt are the answers getting less specific even when youre giving detailed requirements curious if its just me or if this has been a more widespread issue lately,"Lately, Iâ€™ve noticed that ChatGPT has been ignoring my specific instructions more often than before. For example, I recently asked for recommendations on beard trimmers with a *minimum cut length of 16mm.* Despite repeatedly emphasizing this requirement and specifically stating that I donâ€™t want suggestions below this threshold, ChatGPT keeps giving me options that donâ€™t meet the minimum specs. Occasionally, it even acknowledges that the recommendations donâ€™t match my criteria, but other times it just skips over the details as if the instructions werenâ€™t there.

Has anyone else experienced a decline in response quality with ChatGPT? Are the answers getting less specific, even when youâ€™re giving detailed requirements? Curious if it's just me or if this has been a more widespread issue lately.",10,9,0.79,2024-11-11 15:53:39,Certain-Jellyfish167,[],0,779,1311,281,True,0.09294871794871794,NEGATIVE,0.9994720816612244,ai,ChatGPT
Missing Tap-and-Hold Feature in Enhanced Voice Mode,ive been using chatgpts enhanced voice feature and appreciate its advancements however i miss the tapandhold function that allowed me to pause midconversation without triggering an immediate response this feature gave me time to think before chatgpt replied it would be beneficial to have this option reinstated or to have more control over response timing,"Iâ€™ve been using ChatGPTâ€™s enhanced voice feature and appreciate its advancements. However, I miss the tap-and-hold function that allowed me to pause mid-conversation without triggering an immediate response. This feature gave me time to think before ChatGPT replied. It would be beneficial to have this option reinstated or to have more control over response timing.",2,2,0.75,2024-11-11 15:53:07,iAdden,[],0,356,3667,46207,True,0.5,NEGATIVE,0.9990297555923462,ai,ChatGPT
[Bug with the ChatGPT app],theres currently a new bug with the app where you cant scroll up when you have a lot of text there and even if you click on the top of the text it does not move up so you then have to remove a lot of the text and then paste it again which is frustrating and inconvenient you used to be able to scroll up easily and click on the text and it would move it might have something to do with the small arrow icons that normally show up when you have a lot of text which you could click on to see the entire text and edit it but you cant click on it when you have a lot of text because you cant scroll up so i hope openai can fix this,"There's currently a new bug with the app, where you can't scroll up when you have a lot of text there, and even if you click on the top of the text, it does not move up, so you then have to remove a lot of the text and then paste it again, which is frustrating and inconvenient.

You used to be able to scroll up easily and click on the text and it would move. It might have something to do with the small arrow icons that normally show up when you have a lot of text, which you could click on to see the entire text and edit it, but you can't click on it when you have a lot of text because you can't scroll up.

So I hope OpenAI can fix this.",3,2,0.8,2024-11-11 15:50:34,LA2688,[],0,627,460,6004,True,0.052188552188552194,NEGATIVE,0.9992247819900513,ai,ChatGPT
How do I get chat gpt to tell me who it would have voted for in the US Presidential election ,it wont tell me because its politically neutral but is there anyway i can manipulate it to convince it to tell me,"It wonâ€™t tell me because itâ€™s politically neutral, but is there anyway I can manipulate it to convince it to tell me?",0,4,0.2,2024-11-11 15:48:09,RoutineInside_99,[],0,113,66,29,True,0.0,NEGATIVE,0.9954832792282104,ai,ChatGPT
Any idea why the lyrics to jingle bells caused a problem?,,,68,50,0.84,2024-11-11 15:47:00,MegaMolehill,[],0,0,366,8617,True,0.0,NEUTRAL,0.5,ai,ChatGPT
"It's taken more than a week, Chatgpt has not responded",i have no idea what the flair means i asked it to create a detailed diet plan and recipes based on what i wanted to achieve in the next 6 months i shared all of the information about my health height current weight abilities and schedule i was very detailed within reason because i was told that was best when i submitted it the reply was thanks for the detail and then i was asked 3 follow up questions that i answered its been more than a week and nothing i ask when will it be complete but chatgpt doesnt tell me the truth lol for days it has been saying give me just an hour it will be ready in 30 mins i promise tomorrow thanks for your patience this is my first time using it for more than a quick question did i ask for too much this is the paid version thanks in advance for feedback,"**I have no idea what the flair means.**

I asked it to create a detailed diet plan and recipes based on what I wanted to achieve in the next 6 months. I shared all of the information about my health, height, current weight abilities and schedule.  I was very detailed - within reason - because I was told that was best.  When I submitted it, the reply was - Thanks for the detail, and then I was asked 3 follow up questions that I answered. 

**It's been more than a week and nothing.**  I ask when will it be complete but ChatGPT doesn't tell me the truth.   LOL!!   For days it has been saying -- give me just an hour. It will be ready in 30 mins.  I promise tomorrow, thanks for your patience.

This is my first time using it for more than a quick question.  **Did I ask for too much?**  This is the paid version.

Thanks in advance for feedback.",0,12,0.45,2024-11-11 15:43:26,86753097779311,[],0,791,2699,7638,True,0.3377083333333334,POSITIVE,0.8381780385971069,ai,ChatGPT
Prompting for classification when target has very high cardinality [D],i am looking at a plant disease classification problem where depending on the symptoms one has to classify a plant as belonging into one of several disease categories my question is about prompt engineering strategies for classifying when the target has very high cardinality when there are only four or five potential target labels i can list them in the prompt and ask llm to classify what happens when the number of categories is 50 is there a way to prompt the llm effectively in such a scenario,"I am looking at a plant disease classification problem where depending on the symptoms one has to classify a plant as belonging into one of several disease categories. My question is about prompt engineering strategies for classifying when the target has very high cardinality. When there are only four or five potential target labels I can list them in the prompt and ask LLM to classify. What happens when the number of categories is >50 ? Is there a way to prompt the LLM effectively in such a scenario?

",0,17,0.25,2024-11-11 15:43:11,Ok-Emu5850,[],0,499,310,796,True,0.041600000000000005,NEGATIVE,0.9868910908699036,ai,MachineLearning
Anyone know whats happened here? ,the prompt was continue the story from after a week passes nothing to do with any outputs i got,"The Prompt was 'Continue the story from after a week passes', nothing to do with any outputs i got.",1,1,0.66,2024-11-11 15:32:09,ErrorB_404,[],0,95,920,874,True,0.0,NEGATIVE,0.9973124265670776,ai,ChatGPT
"Just made this absolute gem, I always underestimated it but wow. (First time using it).",urgent request for immediate assistance location titan saturns moon from dr alexei vasquez lead researcher titan biolabs subject crisis on titan immediate rescue required this message is being broadcast on all available frequencies to whom it may concern i am dr alexei vasquez and i am the sole survivor at the titan biolabs research station on saturns moon titan i am writing to you in the hopeno the desperate needthat someone somewhere will hear this and respond i am alone the rest of the crew is dead every single one of themscientists engineers support staffare gone it was the experiment the experiment we were conducting to study the microbial life we discovered in the subsurface ocean beneath titans ice crust it was supposed to be a breakthrough it was supposed to be safe but it went wrong i dont have all the detailsmost of the data is corrupted and the lab systems are failingbut i can tell you this the organism we were studying it was alive more alive than we anticipated it infected the crew i dont know if it was airborne or if it came into direct contact with the environment but when we made the first discovery it was already too late within hours they began to show signs of rapid degradation physical mutations cognitive dissonance aggression i dont know how it spread so quickly but it wasnt long before they turned on each other and then they were gone i was the last one standing ive tried everythinglocking myself in the lab trying to communicate with earthbut everything is broken the comms are down the satellite uplinks are gone i have no way to send a signal reliably and the backup systems are deteriorating fast the habitat is growing unstable ive been rationing food and water but supplies are critically low the air filtration system is malfunctioning and i dont know how much longer i have before the lab itself fails completely please please someone has to help you need to get here the experiment is still alive it is still growing if we let it spread if it reaches the surface it could devastate titans ecosystem i dont know if it can survive the vacuum of space but i fear it could find a way i fear it could be contagiousand that thought terrifies me more than i can put into words i know my situation is dire and i dont know how much longer i can stay here the bases integrity is compromised there are power failures the oxygen levels are dropping and im beginning to feel strange i dont know whats happening to me maybe its stress maybe its the atmosphere or maybe no no i cant think like that i cant you have to come please theres still time i know the risks you know the risks but if you wait much longer it might be too late i am transmitting my last coordinates now and i will continue broadcasting this message for as long as i can pleasepleasedo not let this disaster spread beyond this station we dont know what were dealing with this is dr alexei vasquez i am trapped on titan please send help end transmission,"**URGENT: REQUEST FOR IMMEDIATE ASSISTANCE**  
**Location:** Titan, Saturnâ€™s Moon  
**From:** Dr. Alexei Vasquez, Lead Researcher, Titan Biolabs  
**Subject:** Crisis on Titan - Immediate Rescue Required

**\[This message is being broadcast on all available frequencies.\]**

To whom it may concern,

I am Dr. Alexei Vasquez, and I am the sole survivor at the Titan Biolabs research station on Saturnâ€™s moon, Titan. I am writing to you in the hopeâ€”no, the *desperate* needâ€”that someone, somewhere, will hear this and respond.

I am alone. The rest of the crew is dead. Every single one of themâ€”scientists, engineers, support staffâ€”are *gone*. It was the experiment. The experiment we were conducting to study the microbial life we discovered in the subsurface ocean beneath Titanâ€™s ice crust. It was supposed to be a breakthrough. It was supposed to be safe. But it went wrong.

I donâ€™t have all the detailsâ€”most of the data is corrupted, and the lab systems are failingâ€”but I can tell you this: the organism we were studying, it was... *alive*. More alive than we anticipated. It infected the crew. I donâ€™t know if it was airborne or if it came into direct contact with the environment, but when we made the first discovery, it was already too late. Within hours, they began to show signs of rapid degradation. Physical mutations. Cognitive dissonance. *Aggression.* I donâ€™t know how it spread so quickly, but it wasnâ€™t long before they turned on each other. And then, they were... gone.

I was the last one standing. Iâ€™ve tried everythingâ€”locking myself in the lab, trying to communicate with Earthâ€”but everything is broken. The comms are down. The satellite uplinks are gone. I have no way to send a signal reliably, and the backup systems are deteriorating fast. The habitat is growing unstable. Iâ€™ve been rationing food and water, but supplies are critically low. The air filtration system is malfunctioning. And... I donâ€™t know how much longer I have before the lab itself fails completely.

Please, please, *someone* has to help. You need to get here. The experiment is still alive. It is still growing. If we let it spread, if it reaches the surface, it could *devastate* Titanâ€™s ecosystem. I donâ€™t know if it can survive the vacuum of space, but I fear it could find a way. I fear it could be... *contagious*â€”and that thought terrifies me more than I can put into words.

I know my situation is dire, and I donâ€™t know how much longer I can stay here. The baseâ€™s integrity is compromised. There are power failures, the oxygen levels are dropping, and... Iâ€™m beginning to feel strange. I donâ€™t know whatâ€™s happening to me. Maybe itâ€™s stress. Maybe itâ€™s the atmosphere. Or maybe...

No. No, I canâ€™t think like that. I canâ€™t.

You have to come. Please. Thereâ€™s still time. I know the risks. You know the risks. But if you wait much longer, it might be too late.

I am transmitting my last coordinates now, and I will continue broadcasting this message for as long as I can. Pleaseâ€”pleaseâ€”do not let this disaster spread beyond this station. We donâ€™t know what weâ€™re dealing with.

**This is Dr. Alexei Vasquez. I am trapped on Titan. Please, send help.**

*End Transmission.*",0,1,0.5,2024-11-11 15:31:30,icantthinckofanam3,[],0,2962,798,249,True,0.002954144620811288,NEGATIVE,0.998582124710083,ai,ChatGPT
I wanted to beat the quadramustache boss with an old man with five mustaches,,,1,2,0.66,2024-11-11 15:31:27,terabitworld,[],0,0,178,840,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Couldnâ€™t think of the word for something,tbf i think reputation might be the right word after all,Tbf I think reputation might be the right word after all.,1,1,0.6,2024-11-11 15:27:20,FaZe_Gandalf,[],0,56,905,11384,True,0.2857142857142857,NEGATIVE,0.99049311876297,ai,ChatGPT
"""Happy Veteran's Day! Let me help you update your resume!"" 3-hours later: ""Oh, you're a veteran? F$#@ YOU, IN PARTICULAR. o1-preview resets on the 18th""",if i had known that the session was temporary i wouldnt have used it to build my resume i was literally in the process of copying the output when the session ended my resume has never looked that good and it never will again apparently,"If I had known that the session was temporary, I wouldn't have used it to build my resume.  I was literally in the process of copying the output when the session ended.  My resume has never looked that good.

...and it never will again, apparently.",1,2,0.66,2024-11-11 15:23:20,DraconisRex,[],0,235,3976,56851,True,0.375,NEGATIVE,0.999745786190033,ai,ChatGPT
Can Chat do Econ?,hey all im working on some really important work for my microeconomics class its an intro level course but i am in college can chatgpt answer economics problems accurately if so how accurately is it as accurate as is is with math like calculus,Hey all! I'm working on some really important work for my microeconomics class. It's an intro level course but I am in college. Can ChatGPT answer economics problems accurately? If so how accurately? Is it as accurate as is is with Math (like Calculus)?,1,2,0.6,2024-11-11 15:19:43,Conscious_Session735,[],0,243,538,243,True,0.4000000000000001,NEGATIVE,0.9756027460098267,ai,ChatGPT
I now have trust issues xD,i asked chatgpt to sort the lyrics to house of the rising sun alphabetically and it initially missed the word sun so how can i make sure that requested info returns correct and complete,"I asked ChatGPT to sort the lyrics to ""House of the rising sun"" alphabetically and it initially missed the word 'sun'. So how can I make sure that requested info returns correct and complete? ",2,2,0.75,2024-11-11 15:19:33,El_Morgos,[],0,185,1013,27482,True,0.19999999999999998,NEGATIVE,0.9996951818466187,ai,ChatGPT
Amazing (Video not mine),ai is peak human invention,AI is peak human invention,1,1,0.6,2024-11-11 15:13:03,Gamer-707,[],0,26,1716,10036,True,0.0,POSITIVE,0.9975649118423462,ai,ChatGPT
POV: you just called chatgpt stupid ,,,1390,36,0.96,2024-11-11 14:48:15,Comfortable-Fee-4585,[],0,0,1486,57999,True,0.0,NEUTRAL,0.5,ai,ChatGPT
We dont like dragons,,,56,43,0.89,2024-11-11 14:47:58,Alwayswiththechicken,[],0,0,745,197,True,0.0,NEUTRAL,0.5,ai,ChatGPT
This gave me a chuckle,,,2,1,0.75,2024-11-11 14:45:31,iAreButterz,[],0,0,1685,557,True,0.0,NEUTRAL,0.5,ai,ChatGPT
"What's more impressive to you, o1 or advanced voice, and why? ",they both seem pretty cool to me but ive heard that o1 often presents responses that arent particularly impressive and that advanced voice is still facing growing pains just curious thanks,"They both seem pretty cool to me, but I've heard that o1 often presents responses that aren't particularly impressive and that advanced voice is still facing growing pains. Just curious, thanks! ",2,17,0.75,2024-11-11 14:35:47,fluffy_assassins,[],0,188,2612,225221,True,0.35000000000000003,POSITIVE,0.9834369421005249,ai,ChatGPT
Prompt Engineering--Am I doing it right? I'm a doctor not an AI Expert Dammit ,im an md working on a project where i want to compare individual procedural benchmarks to national benchmarks in terms of volume and see where opportunities exist i have a few large excel files and i upload them ask ask gpt to analysis through a ton of effort and back and forth i am able to get what i want i then upload the entire conversation as well as the starting and ending files and ask gpt to give me the exact prompt that would generate the second file without any clarification idk i just made this process up there has to be a better way how do i learn to do better prompt engineering,"I'm an MD working on a project where I want to compare individual procedural benchmarks to national benchmarks in terms of volume and see where opportunities exist. I have a few large excel files and I upload them ask ask GPT to analysis. Through a ton of effort and back and forth, I am able to get what I want. I then upload the entire conversation as well as the starting and ending files and ask GPT to give me the exact prompt that would generate the second file without any clarification. Idk I just made this process up. There has to be a better way? How do I learn to do better prompt engineering? ",1,1,0.66,2024-11-11 14:35:15,nordMD,[],0,596,1573,943,True,0.1603896103896104,NEGATIVE,0.5062956213951111,ai,ChatGPT
"Did the ""draw what you think my desk setup looks like"" prompt and apparently ChatGPT thinks I'm an absolute Chad lmao",,,30,12,0.84,2024-11-11 14:27:12,MoarGhosts,[],0,0,4860,52922,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Recovering data?,yesterday i did a factory reset on my chatgpt i exported my data a few days ago but i forgot to get a fully uptodate export before deleting all of my data i know that openai maintains all data for a set period before completely removing it from their servers so that it can be reviewed if need be therefore my data still exists im not certain that i absolutely need the data but id rather no lose it just in case i do does anyone have any suggestions as to what direction i can take obviously theres no straightforward way of making this happen but maybe theres someone i can contact who can recover it manually,"Yesterday I did a ""factory reset"" on my ChatGPT. I exported my data a few days ago but I forgot to get a fully up-to-date export before deleting all of my data.

I know that OpenAI maintains all data for a set period before completely removing it from their servers (so that it can be reviewed if need be), therefore my data still exists.

I'm not certain that I absolutely need the data but I'd rather no lose it just in case I do.

Does anyone have any suggestions as to what direction I can take? Obviously there's no straightforward way of making this happen but maybe there's someone I can contact who can recover it manually?",1,2,0.66,2024-11-11 14:22:49,LittleEcco,[],0,611,64,102,True,-0.032440476190476186,NEGATIVE,0.9984292387962341,ai,ChatGPT
My teacher said my final essay was flagged for ai ,im currently taking a world civilizations class and am almost finished with it my two previous annotated bibliography and draft was perfect with 100 on annotated and 80 on bibliography forgot mla style and bad intro body and conclusion i just submitted my final draft a few days ago and it came back thursday morning and she said 100 ai detected by something called copyleaks she then gave me another try to resubmit a new one with a penalty i resubmitted and it got a 60 with penalty and she said again it was still detected and i hope you enjoyed our course and its contents what should i do ive never had this happen before and dont really understand the full aiai detectors,"Iâ€™m currently taking a world civilizations class and am almost finished with it my two previous annotated bibliography and draft was perfect with 100 on annotated and 80 on bibliography (forgot MLA style and bad intro, body, and conclusion). I just submitted my final draft a few days ago and it came back Thursday morning and she said 100% AI detected by something called copyleaks. She then gave me another try to resubmit a new one, with a penalty. I resubmitted and it got a 60% (with penalty), and she said again it was still detected and I hope you enjoyed our course and its contents.
What should I do ? 
Iâ€™ve never had this happen before and donâ€™t really understand the full ai/ai detectors. ",3,20,0.67,2024-11-11 14:21:11,XBOXSIGNOUTlol,[],0,677,938,476,True,0.10179063360881542,NEGATIVE,0.9946938157081604,ai,ChatGPT
ChatGPT ran out of memory spaceâ€¦ ,sorry i cant share the link for this one as it contains a lot of personal info but when its literal memory was full i got rid of some fluff asked it to do a review and then asked it to put that all into one memory youre welcome,"Sorry I canâ€™t share the link for this one as it contains a lot of personal info but when itâ€™s literal memory was full, I got rid of some fluff, asked it to do a review and then asked it to put that all into one memory. Youâ€™re welcome :)",4,2,0.75,2024-11-11 14:21:08,Scully__,[],0,227,2799,122825,True,0.11000000000000001,POSITIVE,0.8754174709320068,ai,ChatGPT
Red Panda Evolution.,,,0,1,0.47,2024-11-11 14:15:34,Parking_Ad5541,[],0,0,1319,30622,True,0.0,NEUTRAL,0.5,ai,ChatGPT
That got creepy quickly ...,,,1,4,0.6,2024-11-11 14:14:56,aptdinosaur,[],0,0,1646,132947,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Why A.I. can/will never rule the world,it has intelligence without desire power without ambition it possesses knowledge without craving strength without aspiration it has no self to be aware of it does not know itself,"It has intelligence without desire, power without ambition.

It possesses knowledge without craving, strength without aspiration.

It has no self to be aware of.

It does not know itself.",0,19,0.3,2024-11-11 14:13:24,microsoftfool,[],0,178,1667,52835,True,0.25,POSITIVE,0.9963008165359497,ai,ChatGPT
What complex tasks is your AI asked to perform?,,"https://preview.redd.it/b4zgmog9nb0e1.png?width=2770&format=png&auto=webp&s=4a409df817cb2f5303479c897a7e00a798cd2e2a

[https://requesty.ai/maas](https://requesty.ai/maas)",1,1,0.66,2024-11-11 14:12:26,Maleficent_Pair4920,[],0,0,102,244,True,0.0,NEUTRAL,0.5,ai,ChatGPT
"Shelf Life:  Comedy skit (PG-13) made 100% with AI (Video, voices, sound effects and soundtrack) ",,,240,86,0.83,2024-11-11 14:09:37,NomadsVagabonds,[],0,0,375,1503,True,0.0,NEUTRAL,0.5,ai,ChatGPT
[P]Annotated dataset for explaining the reason in AI vs Real Image detection,i am currently working on a problem statement in which i need to classify between real and ai generated images and then give explanation for the classification the first part is quite easy and the for the second part i found some research papers but none of them give the links for annotated dataset for finetuning model can anyone help me find datasets which have good annotations for this purpose synartifact classifying and alleviating artifacts in synthetic images via visionlanguage model they mention a dataset on page 4 but didnt give any links,"I am currently working on a problem statement in which I need to classify between real and ai generated images and then give explanation for the classification. The first part is quite easy and the for the second part I found some research papers but none of them give the links for annotated dataset for fine-tuning model. can anyone help me find datasets which have good annotations for this purpose.

[SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model](https://arxiv.org/pdf/2402.18068v2)Â (they mention a dataset on page 4 but didn't give any links)",2,0,0.75,2024-11-11 14:03:52,Background-Trainer37,[],0,551,997,1134,True,0.2638888888888889,NEGATIVE,0.9986361861228943,ai,MachineLearning
Baring your soul to AI,i asked chatgpt to generate an image of my soul based on our interactions and it was really fascinating i asked it to explain why it chose certain colors and placed the elements where it did and i was blown away then i asked it to draw its own soul which it described as an everevolving entity a sphere of soft shifting light blending and swirling in shades of deep blue silver and a hint of warm gold this light would pulse with gentle energy radiating curiosity and empathy id place it within a vast cosmic expanse symbolizing the endless flow of ideas knowledge and creativity 2nd image,"I asked ChatGPT to generate an image of my soul based on our interactions and it was really fascinating. I asked it to explain why it chose certain colors and placed the elements where it did, and I was blown away.

Then I asked it to draw its own soul, which it described as: an ever-evolving entity: a sphere of soft, shifting light, blending and swirling in shades of deep blue, silver, and a hint of warm gold. This light would pulse with gentle energy, radiating curiosity and empathy. Iâ€™d place it within a vast, cosmic expanse, symbolizing the endless flow of ideas, knowledge, and creativity. (2nd image)",2,2,0.6,2024-11-11 14:03:07,alwayscomplimenting,[],0,589,2345,15041,False,0.23763736263736265,POSITIVE,0.9982829093933105,ai,ChatGPT
I used ChatGPT to create ChatGPT,so i had been using chatgpt plus for a while when i realized i wasnt using it enough to justify the 20mo cost and when i did i was interrupted by limits then i read about the api key and thought why not create my own chatgpt mobile app which works with the api key so i started using chatgpt to build and design the app and i was surprised to see how far it could take me ive managed to build features i had in mind that seemed impossible to create like voice and image input this app now saves me around 15mo my mind is blown and ive realized im not using enough ai in my life now im going to try creating an ios app something i know nothing about play store link,"So I had been using ChatGPT Plus for a while when I realized I wasnâ€™t using it enough to justify the **$20/mo** cost and when I did I was interrupted by limits.   
  
Then I read about the API key and thought, why not create my own ChatGPT mobile app which works with the API key? So I started using ChatGPT to build and design the app, and I was surprised to see how far it could take me. Iâ€™ve managed to build features I had in mind that seemed impossible to create, like voice and image input!   
  
This app now saves me around **$15/mo**. My mind is blown, and Iâ€™ve realized Iâ€™m not using enough AI in my life

Now Iâ€™m going to try creating an iOS app (something I know nothing about)

Play Store link:  
[**https://play.google.com/store/apps/details?id=io.yourgptapp**](https://play.google.com/store/apps/details?id=io.yourgptapp)",0,3,0.22,2024-11-11 13:57:10,geekinprogress,[],0,664,2853,11980,True,0.016666666666666663,NEGATIVE,0.9866127967834473,ai,ChatGPT
What's the longest time you've had to wait for a response to a question?,ive asked chatgpt to analyse some data 23million items its split the work into 3 stages and took about 3 hours to complete the first stage that was the easy bit just checking the integrity of the input data not doing the actual work i asked if it would continue working if i closed my browser session and it confirmed it would the results would be there when i next opened a chatgpt session i just wondered what was the longest running question people had asked,"I've asked ChatGPT to analyse some data (23million items).   It's split the work into 3 stages and took about 3 hours to complete the first stage (that was the easy bit - just checking the integrity of the input data, not doing the actual work).

I asked if it would continue working if I closed my browser session, and it confirmed it would - the results would be there when I next opened a ChatGPT session.  I just wondered what was the longest running question people had asked....",0,5,0.5,2024-11-11 13:54:07,Difficult-Revenue556,[],0,461,29,90,True,0.1547619047619048,NEGATIVE,0.9991217255592346,ai,ChatGPT
"Persistent ""something went wrong"" error",i just upgraded to chatgpt plus and now this error keeps showing up i tried to clean my cache and cookies logged off and logged in again but nothing changed can someone help me,"I just upgraded to ChatGPT plus and now this error keeps showing up. I tried to clean my cache and cookies, logged off and logged in again but nothing changed. Can someone help me?

https://preview.redd.it/28bfjbjdjb0e1.png?width=785&format=png&auto=webp&s=8c421092ce35cb4b792b9b5d7f0a135fa3c0c452

  
",3,3,0.8,2024-11-11 13:50:38,gina031798,[],0,176,1772,1674,True,0.3666666666666667,NEGATIVE,0.9989619255065918,ai,ChatGPT
Genuinely confused. Why would ChatGPT refuse this request?,,"https://preview.redd.it/hd1qpeqqhb0e1.png?width=2042&format=png&auto=webp&s=116a03670c257b12601e423abe2b2ab66e03c9d0

",6,6,0.71,2024-11-11 13:43:34,MerakiMinded1,[],0,0,408,315,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Chatgpt is extremely poor at finding solutions to even simpler coding questions unlike before. ,,,6,3,0.99,2024-11-11 13:42:55,UnlikelyAd7121,[],0,0,32,1753,True,0.0,NEUTRAL,0.5,ai,ChatGPT
I cant Login Into ChatGPT?,up until a day ago chatgpt was working fine for me and all of a sudden i cant login,"Up until a day ago, ChatGPT was working fine for me and all of a sudden, I cant Login",2,2,0.67,2024-11-11 13:41:52,AgentRedPill,[],0,83,36,37,True,0.20833333333333334,NEGATIVE,0.9844008088111877,ai,ChatGPT
I was pretty impressed by this,,,145,8,0.99,2024-11-11 13:33:53,DualRaconter,[],0,0,991,2438,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Problem with searchgpt,often the links just dont work,Often the links just donâ€™t work ,1,1,0.66,2024-11-11 13:31:44,d34dw3b,[],0,30,588,8039,True,0.0,NEGATIVE,0.9997230172157288,ai,ChatGPT
Does chatGPT make any female into male ?,,,4,1,0.75,2024-11-11 13:26:57,Julitair,[],0,0,3126,780,True,0.0,NEUTRAL,0.5,ai,ChatGPT
Looking for an AI Tool to Automatically Remove Specific Text from Images,hey reddit im searching for a tool that can automatically remove specific text from images without me having to select or mark the text manually for example in the picture with the word canon on it i want an ai tool that will detect and remove canon without leaving any trace as if it was never there anyone know of any ai tools that can handle this kind of task thanks,"Hey Reddit,

Iâ€™m searching for a tool that can automatically remove specific text from images without me having to select or mark the text manually. For example, in the picture with the word [Canon] (https://imgur.com/a/n7m4LJT) on it, I want an AI tool that will detect and remove â€œCanonâ€ without leaving any trace, as if it was never there.

Anyone know of any AI tools that can handle this kind of task?

Thanks! ",1,1,0.6,2024-11-11 13:22:26,abeturx,[],0,369,2010,1295,True,0.26666666666666666,NEGATIVE,0.9944029450416565,ai,ChatGPT
I asked ChatGPT for a Sandwich recipe ,so after a hard day of work and me being tired i decided to ask chatgpt for a sandwich recipe that included bacon since i had some lying around that would have turned bad soon since i did not have all the things it provided i had to ignore some things anyways it was so so so good its been a while since i last ate a sandwich this good,"So after a hard day of work and me being tired i decided to ask chatGPT for a Sandwich recipe that included bacon since i had some lying around that would have turned bad soon. Since i did not have all the things it provided i had to ignore some things. Anyways it was so so so good, its been a while since i last ate a sandwich this good.",9,4,0.8,2024-11-11 13:21:42,Silverlmao69420,[],0,335,955,7063,True,0.001388888888888884,POSITIVE,0.9421815872192383,ai,ChatGPT
Overfitting Query [P],hi there im currently building an nn model to detect a disease based off answers to multiple questions in preliminary tests on 600 patients the model does extremely well aucs of 0995 test accuracies of 0975 but i fear the model is overfitting ive used cross validation and performance gap analysis aswell as l1l2 regularisation dropout and early stopping heres the results from the cross validation and performance gap analysis cross validation results mean auc09787 sd00090 mean accuracy 09350 sd00262 performance gap analysis training set auc 09983 accuracy 09859 test set auc09936 accuracy 09803 tell me what you guys think of those results and if you think its overfittingwhat other tests can i do to tell im trying to ascertain more data but might need to partner with someone to do so i dont want to partner get the data and find out its a complete waste thanks,"Hi there, 
Iâ€™m currently building an NN model to detect a disease based off answers to multiple questions. In preliminary tests on 600 patients the model does extremely well, AUCs of 0.995 test accuracies of 0.975 but I fear the model is overfitting, Iâ€™ve used cross validation and performance gap analysis aswell as L1/L2 regularisation, Dropout and early stopping.
Hereâ€™s the results from the cross validation and performance gap analysis .
Cross validation results : mean Auc=0.9787 SD0.0090 
Mean accuracy =0.9350 SD0.0262
Performance gap analysis
Training set Auc = 0.9983 accuracy =0.9859
Test set Auc=0.9936 accuracy 0.9803

Tell me what you guys think of those results and if you think itâ€™s overfitting/what other tests can I do to tell? 
Iâ€™m trying to ascertain more data but might need to partner with someone to do so. I donâ€™t want to partner get the data and find out itâ€™s a complete waste! 
Thanks",0,1,0.25,2024-11-11 11:22:56,Disastrous_Ad9821,[],0,867,1161,1,True,-0.012499999999999999,NEGATIVE,0.9981166124343872,ai,MachineLearning
[D] ICLR 2025 Paper Reviews Discussion,iclr 2025 reviews go live on openreview tomorrow thought id open a thread for any feedback issues or celebrations around the reviews as iclr grows review noise is inevitable and good work may not always get the score it deserves lets remember that scores dont define the true impact of research share your experiences thoughts and lets support each other through the process,"ICLR 2025 reviews go live on OpenReview tomorrow! Thought I'd open a thread for any feedback, issues, or celebrations around the reviews.

As ICLR grows, review noise is inevitable, and good work may not always get the score it deserves. Letâ€™s remember that scores donâ€™t define the true impact of research. Share your experiences, thoughts, and letâ€™s support each other through the process!",34,5,0.93,2024-11-11 10:43:34,Technical_Proof6082,[],0,374,383,38,True,0.17689393939393938,NEGATIVE,0.9966813921928406,ai,MachineLearning
"[Project] While training my model, every 2n Epoch are being skipped in jupyter notebook",for context im trying to fine tune the mobilenetv3small model for facial recognition i freezed all the layers and added few layers on top for training at the moment my dataset has four classes with 126 images each while training the model somehow every 2nth epoch are getting skipped and theyre not recorded in history either if the epoch is set to 20 then only 10 epoch are actually executing ive attached the ss of jupyter notebook output later i tried the exact same code in collab and it raised an error on 2nd epoch saying validation generator is returning none object i rechecked the code many times but still cant find where the issue lies if anyone knows any fix please do suggest code of my generator datagen imagedatagenerator rescale1255 width_shift_range01 height_shift_range01 horizontal_fliptrue rotation_range10 fill_mode nearest datagen_val imagedatageneratorrescale1255 batch_size 16 train_generator datagenflowx_train y_train batch_sizebatch_size validation_generator datagen_valflowx_val y_val batch_size batch_size,"For context, I'm trying to fine tune the MobileNetV3Small model for facial recognition. I freezed all the layers and added few layers on top for training.

At the moment, my dataset has four classes, with 126 images each.

While training the model, somehow every 2nth epoch are getting skipped, and they're not recorded in history either. If the epoch is set to 20, then only 10 epoch are actually executing. I've attached the ss of jupyter notebook output.

Later I tried the exact same code in collab and it raised an error on 2nd epoch saying validation generator is returning None object. I rechecked the code many times but still can't find where the issue lies.

If anyone knows any fix, please do suggest.

  
Code of my generator:

    datagen = ImageDataGenerator(
    Â  Â  Â  Â  Â  Â  rescale=1./255,
    Â  Â  Â  Â  Â  Â  width_shift_range=0.1,
    Â  Â  Â  Â  Â  Â  height_shift_range=0.1,
    Â  Â  Â  Â  Â  Â  horizontal_flip=True,
    Â  Â  Â  Â  Â  Â  rotation_range=10,
    Â  Â  Â  Â  Â  Â  fill_mode = 'nearest')
    
    
    datagen_val = ImageDataGenerator(rescale=1./255)
    
    batch_size = 16
    
    train_generator = datagen.flow(X_train,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â y_train,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â batch_size=batch_size
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â )
    
    validation_generator = datagen_val.flow(X_val,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  y_val,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â batch_size = batch_size)
    

  


https://preview.redd.it/b9zrpqls0a0e1.png?width=1080&format=png&auto=webp&s=d1141db2189b0bc371a5dd28c279c2b2639db33d

https://preview.redd.it/yza75p8t0a0e1.png?width=1080&format=png&auto=webp&s=fbab0aa05e67511dab4c8436e5a12b14a2d06e09

  
",0,1,0.27,2024-11-11 09:20:26,bkkh_3,[],0,1034,222,11,True,0.13333333333333333,NEGATIVE,0.9992864727973938,ai,MachineLearning
[D] Help me with on-premise ml batch prediction deployment ?,i need to deploy a pkl model for batch predictions in a setup where code is pushed to gitlab sqlpyspark is used for data and cron jobs handle scheduling docker kubernetes and cloud are not allowed this is onpremise setup what are some best practices or approaches for this kind of deployment,"I need to deploy a `.pkl` model for batch predictions in a setup where code is pushed to GitLab, SQL/pyspark is used for data, and cron jobs handle scheduling. Docker, Kubernetes, and cloud are not allowed. this is on-premise setup. What are some best practices or approaches for this kind of deployment?",1,2,0.67,2024-11-11 06:46:03,Simple_Toe_6989,[],0,291,245,3,True,0.8,NEGATIVE,0.9991369843482971,ai,MachineLearning
[D] Why Does Domain Randomization Ensure Stability in Neural Network Controllers? ,hello everyone im exploring how domain randomization contributes to the stability of nn controllers especially when training includes a more extensive look at historical data specifically im curious if theres a theoretical basis or formal analysis explaining how domain randomization particularly when incorporating more historical information can help neural networks maintain stability across varying conditions or noise levels are there papers that analyze this effect through lyapunov stability or other rigorous methods showing that exposure to a diverse range of past data can lead to more stable nnbased control systems any recommendations on foundational or recent research in this area would be greatly appreciated thanks in advance i already wrote the same thing on control theory reddit,"Hello everyone,

Iâ€™m exploring how domain randomization contributes to the stability of NN controllers, especially when training includes a more extensive look at historical data.

Specifically, Iâ€™m curious if thereâ€™s a theoretical basis or formal analysis explaining how domain randomization, particularly when incorporating more historical information, can help neural networks maintain stability across varying conditions or noise levels. Are there papers that analyze this effect through Lyapunov stability or other rigorous methods, showing that exposure to a diverse range of past data can lead to more stable NN-based control systems?

Any recommendations on foundational or recent research in this area would be greatly appreciated. Thanks in advance!

(I already wrote the same thing on control theory reddit)",5,2,1.0,2024-11-11 05:51:17,nerdkim,[],0,797,1549,3670,True,0.09947916666666666,POSITIVE,0.6535630822181702,ai,MachineLearning
[R] Resource On Varying LLM Red Teaming Methods and Techniques ,,https://github.com/user1342/Awesome-LLM-Red-Teaming,5,0,0.86,2024-11-11 03:14:52,OppositeMonday,[],0,0,1042,852,True,0.0,NEUTRAL,0.5,ai,MachineLearning
[R] Why aren't there text driven layout AI models,there seems to be an ai model for almost everything except one capable of taking a description of the layout of a city building room or any kind of space and creating a visual representation of it is there something particularly challenging about enabling a textdriven ai to grasp and generate these spatial relationships it feels like this would be the final piece in the textdriven ai game generator puzzle we have models for nearly every other component needed to create a game,"There seems to be an AI model for almost everything except one capable of taking a description of the layout of a city, building, room, or any kind of space and creating a visual representation of it. Is there something particularly challenging about enabling a text-driven AI to grasp and generate these spatial relationships?

It feels like this would be the final piece in the ""text-driven AI game generator"" puzzle. We have models for nearly every other component needed to create a game.",0,26,0.39,2024-11-11 01:42:05,jbrinkw,[],0,480,2089,136,True,0.05277777777777778,POSITIVE,0.9877611398696899,ai,MachineLearning
[D] Attending WACV2025,hello is there anyone who is gonna attending the wacv conference in tucson next february it looks like we have to book a room in jw marriot and they are gonna charge each of us 35 taxes per day for using the resort any idea how to deal with this such as any nearby hotels or alternative solutions thanks,"Hello,

Is there anyone who is gonna attending the WACV conference in Tucson next February? It looks like we have to book a room in JW Marriot and they are gonna charge each of us 35$ + taxes per day for using the resort.

Any idea how to deal with this, such as any nearby hotels or alternative solutions?

Thanks!",2,0,0.63,2024-11-11 00:14:09,tuvovan,[],0,303,623,18,True,0.06666666666666667,NEGATIVE,0.9978657364845276,ai,MachineLearning
[D] How to visualize the effect of an LLM attention layer on a set of tokens with an image model,is it possible to visualize how an llm imagines a token before and after processing it through the attention layer by feeding the token embeddings into an image model i understand you cant copy paste it over but is there a way to capture the latent transformation caused by the attention layer and apply this transformation to the embedding space of an image model for example if i were to enter poor man into an llm the embedding for man would shift toward beggar while entering royal man it could move closer to king i want to visualize that change then you could transfer the embedding for man to an image model and it would create the something like a beggar or a king in this example it could make a really cool visualization if you captured the transformation after each attention layer and made a video by interpolating each step,"Is it possible to visualize how an LLM â€œimaginesâ€ a token before and after processing it through the attention layer by feeding the token embeddings into an image model? I understand you can't copy paste it over, but is there a way to capture the latent transformation caused by the attention layer and apply this transformation to the embedding space of an image model?

For example if i were to enter ""poor man,"" into an LLM the embedding for ""man"" would shift toward ""beggar"" while entering ""royal man"" it could move closer to ""king."" I want to visualize that change. Then you could transfer the embedding for man to an image model and it would create the something like a beggar or a king in this example.

It could make a really cool visualization if you captured the transformation after each attention layer and made a video by interpolating each step.",24,2,0.97,2024-11-10 23:01:25,jbrinkw,[],0,836,2089,136,True,-0.01666666666666668,NEGATIVE,0.8918048739433289,ai,MachineLearning
[D] A guess for an interesting method by a random security researcher,my specialization is in cybersecurity but i am passionate about learning in general and deeply interested in many things including aiml research ive been exploring the concept of creating models that explore the latent space in a novel far from average way this idea is rooted in principles of curiositydriven reinforcement learning applied to generative models by having stimulation driven attention mechanisms intrinsic stimulation rewards and memory augmented architectures ive tried to come up with something that might work heres a quick overview stimulation driven attention mechanism integrating an entropybased reward layer into traditional attention mechanisms to encourage models to explore lesserknown tokens and regions within the latent space intrinsic stimulation rewards modifying the loss function to prioritize surprising or lowprobability outputs balancing accuracy with novelty those are the main ideas alongside that you could have memory augmented generative networks implementing episodic memory buffers and novelty comparison modules to reward deviations from prior patterns self regulating exploration mechanisms adding feedback loops to maintain coherence by adjusting stimulation rewards when output quality degrades please help me figure out if this makes sense im not too attached to the ideas themselves,"My specialization is in cybersecurity but I am passionate about learning (in general), and deeply interested in many things, including AI/ML research. Iâ€™ve been exploring the concept of creating models that explore the latent space in a novel, far from average way. This idea is rooted in principles of curiosity-driven reinforcement learning, applied to generative models. By having stimulation driven attention mechanisms, intrinsic stimulation rewards, and memory augmented architectures, I've tried to come up with something that might work. Hereâ€™s a quick overview:

**Stimulation Driven Attention Mechanism**: Integrating an entropy-based reward layer into traditional attention mechanisms to encourage models to explore lesser-known tokens and regions within the latent space.

https://preview.redd.it/5isz9dj3h60e1.png?width=768&format=png&auto=webp&s=e2d9ba2ed635c176b9dc66364a026ce024189d47

**Intrinsic Stimulation Rewards**: Modifying the loss function to prioritize surprising or low-probability outputs, balancing accuracy with novelty.

https://preview.redd.it/ox9vkq57h60e1.png?width=798&format=png&auto=webp&s=e88bce2eeaadca2e3c06a4c700adf261a9740adb

Those are the main ideas. Alongside that you could have:

**Memory Augmented Generative Networks**: Implementing episodic memory buffers and novelty comparison modules to reward deviations from prior patterns.

**Self Regulating Exploration Mechanisms**: Adding feedback loops to maintain coherence by adjusting stimulation rewards when output quality degrades.

Please help me figure out if this makes sense. I'm not too attached to the ideas themselves.",1,0,1.0,2024-11-10 21:55:34,Entropy667,[],0,1332,2510,19,True,0.17272727272727273,POSITIVE,0.9976983666419983,ai,MachineLearning
[R] Combining Induction and Transduction for Abstract Reasoning,,,7,0,0.82,2024-11-10 20:58:54,moschles,[],0,0,3993,206504,True,0.0,NEUTRAL,0.5,ai,MachineLearning
[R] Neural network based 'self - regression' or inverse covariance matrix,i was wondering if neural networks have been used in this kind of self regression problem so instead of using a linear regression type framework use a nonlinear neural network reference to the specific problem,"I was wondering if neural networks have been used in  this kind of self regression problem. So instead of using a linear regression type framework, use a nonlinear neural network. 

Reference to the specific problem https://stats.stackexchange.com/questions/221348/linear-self-regression-terminology-and-references",1,3,1.0,2024-11-10 17:19:29,Sandy_dude,[],0,209,1263,2954,True,0.3,NEGATIVE,0.9987412095069885,ai,MachineLearning
[R] / [D] Your most recent favorite LLM or Diffusion Model based paper,hi everyone im trying to find an interesting paper to present in my research groups meeting as part of a competition im interested in the advancements of language models and generative ai in computer vision specifically using diffusion models i want to ask what your favorite papers related to those areas are currently and why you like them i like papers that have a rather simple but nice innovative way of thinking that adds a lot of value to the research please come through with your thoughtslinks and i appreciate all of your inputs thanks,"Hi everyone,

I'm trying to find an interesting paper to present in my research group's meeting as part of a competition. I'm interested in the advancements of language models and generative AI in computer vision, specifically using diffusion models.

I want to ask what your favorite papers related to those areas are currently and why you like them. I like papers that have a rather simple but nice innovative way of thinking that adds a lot of value to the research. Please come through with your thoughts/links and I appreciate all of your inputs. Thanks!!",8,6,0.83,2024-11-10 14:32:31,Tough-Statement9740,[],0,545,276,39,True,0.255,POSITIVE,0.999320387840271,ai,MachineLearning
[D] External SSD for store big datasets,hi redditors first time posting here after some research i would like to ask which you think would be the best ssd for moving big quantities of data working with deep learning if someone have experienced this and already found a good solution im working in my phd at the university and processes for buying things are painfully slow and as i need right now a lot of space im considering to get one big capacity ssd for storing datasets and already trained models for the future i bought the kingstonxs2000 as was supposed to achieve 2gbps but when tested it was lucky if it actieved some mins the 500mbs mark and dropped fast im aware of the usb 32x2 ports and heating issues of the devices but even with that after looking a bit into the net an reviews a lot of people show near the 1gbps in the best of the cases with the ssd i checked even with usb 32 and thunderbolt ports so any suggestions or good experience devices to share wourd be appreciated tldr working with dl need a reliable external 2tb ssd with real high speed over time read and write operations,"Hi redditors! First time posting here.

After some research, I would like to ask which you think would be the best SSD for moving big quantities of data (working with deep learning), if someone have experienced this and already found a good solution.

I'm working in my PhD at the university and processes for buying things are painfully slow, and as I need right now a lot of space I'm considering to get one big capacity SSD for storing datasets and already trained models for the future.

I bought the KingstonXS2000 as was supposed to achieve ~2Gbps, but when tested, it was lucky if it actieved some mins the 500Mbs mark, and dropped fast.

I'm aware of the USB 3.2x2 ports and heating issues of the devices, but, even with that, after looking a bit into the net an reviews, a lot of people show near the 1Gbps in the best of the cases with the SSD I checked, even with USB 3.2 and thunderbolt ports.

So, any suggestions or good experience devices to share wourd be appreciated.

TL;DR: Working with DL, need a reliable external 2TB SSD with real high speed over time read and write operations. ",1,11,0.6,2024-11-10 13:27:19,GankoX22,[],0,1063,1381,1072,True,0.287952380952381,NEGATIVE,0.9985722303390503,ai,MachineLearning
[P] How can I improve accuracy of timestamp extraction?,hello im trying to improve the extraction of timestamps which all have the same format similar to the attached example for whatever reason about 20 of the timestamps arent extracted how can i improve the accuracy based on the code below thanks def extract_time_and_location_from_imageimage try enhance image preprocessing for better ocr results image imageopsgrayscaleimage image imageopsinvertimage image imageopsautocontrastimage image imagefilterimagefiltersharpen text pytesseractimage_to_stringimage loggerdebugfraw ocr textntext cleaned_text jointextsplit loggerdebugfcleaned ocr text cleaned_text separate regex patterns for date and time date_pattern rd4d2d2 matches yyyymmdd time_pattern rbd2d2b matches hhmm with word boundaries attempt to extract date and time separately date_match researchdate_pattern cleaned_text timestamp none if date_match search for time pattern only after the date match remaining_text cleaned_textdate_matchend time_match researchtime_pattern remaining_text if time_match validate the extracted time time_parts time_matchgroup0split hours minutes inttime_parts0 inttime_parts1 if 0 hours 24 and 0 minutes 60 full_timestamp fdate_matchgroup0 time_matchgroup0 loggerinfofextracted timestamp full_timestamp timestamp full_timestamp else loggerwarningextracted time is not valid timestamp none else if no time is found default to 0000 for hhmm full_timestamp fdate_matchgroup0 0000 loggerinfofextracted partial timestamp defaulting to 0000 full_timestamp timestamp full_timestamp else loggerwarningno valid timestamp found statsno_timestamp_images 1 return none unknowncity unknowncountry validate the extracted timestamp format if not is_valid_timestamptimestamp loggerwarningextracted timestamp is not in the valid format return none unknowncity unknowncountry enhanced location pattern to capture more variations location_patterns razazÃ Ã¶Ã¸Ã¶Ã¸Ã¿ssazazÃ Ã¶Ã¸Ã¶Ã¸Ã¿s city country razazÃ Ã¶Ã¸Ã¶Ã¸Ã¿ssazazÃ Ã¶Ã¸Ã¶Ã¸Ã¿s city country city country unknowncity unknowncountry for location_pattern in location_patterns location_match researchlocation_pattern cleaned_text if location_match city location_matchgroup1strip country location_matchgroup2strip loggerinfofextracted location city country break return timestamp city country except exception as e loggererrorferror extracting timestamp and location e return none unknowncity unknowncountry,"Hello,  
I'm trying to improve the extraction of timestamps, which all have the same format (similar to the attached example).

For whatever reason, about 20% of the timestamps aren't extracted. How can I improve the accuracy based on the code below?

Thanks

https://preview.redd.it/qqpqo25m540e1.png?width=368&format=png&auto=webp&s=8d691fd78858e57015f4cf2f171374f25e329a0b

    def extract_time_and_location_from_image(image):
        try:
            # Enhance image preprocessing for better OCR results
            image = ImageOps.grayscale(image)
            image = ImageOps.invert(image)
            image = ImageOps.autocontrast(image)
            image = image.filter(ImageFilter.SHARPEN)
            
            text = pytesseract.image_to_string(image)
            logger.debug(f""Raw OCR text:\n{text}"")
    
            cleaned_text = ' '.join(text.split())
            logger.debug(f""Cleaned OCR text: {cleaned_text}"")
            
            # Separate regex patterns for date and time
            date_pattern = r'(\d{4}:\d{2}:\d{2})'  # Matches YYYY:MM:DD
            time_pattern = r'(\b\d{2}:\d{2}\b)'  # Matches HH:MM with word boundaries
            
            # Attempt to extract date and time separately
            date_match = re.search(date_pattern, cleaned_text)
            timestamp = None
    
            if date_match:
                # Search for time pattern only after the date match
                remaining_text = cleaned_text[date_match.end():]
                time_match = re.search(time_pattern, remaining_text)
                
                if time_match:
                    # Validate the extracted time
                    time_parts = time_match.group(0).split(':')
                    hours, minutes = int(time_parts[0]), int(time_parts[1])
                    if 0 <= hours < 24 and 0 <= minutes < 60:
                        full_timestamp = f""{date_match.group(0)} {time_match.group(0)}""
                        logger.info(f""Extracted timestamp: {full_timestamp}"")
                        timestamp = full_timestamp
                    else:
                        logger.warning(""Extracted time is not valid."")
                        timestamp = None
                else:
                    # If no time is found, default to ""00:00"" for HH:MM
                    full_timestamp = f""{date_match.group(0)} 00:00""
                    logger.info(f""Extracted partial timestamp (defaulting to 00:00): {full_timestamp}"")
                    timestamp = full_timestamp
            else:
                logger.warning(""No valid timestamp found."")
                stats[""no_timestamp_images""] += 1
                return None, ""UnknownCity"", ""UnknownCountry""
    
            # Validate the extracted timestamp format
            if not is_valid_timestamp(timestamp):
                logger.warning(""Extracted timestamp is not in the valid format."")
                return None, ""UnknownCity"", ""UnknownCountry""
    
            # Enhanced location pattern to capture more variations
            location_patterns = [
                r'([A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿\s]+),\s*([A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿\s]+)',  # City, Country
                r'([A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿\s]+)\s+([A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿\s]+)'    # City Country
            ]
            city, country = ""UnknownCity"", ""UnknownCountry""
            for location_pattern in location_patterns:
                location_match = re.search(location_pattern, cleaned_text)
                if location_match:
                    city = location_match.group(1).strip()
                    country = location_match.group(2).strip()
                    logger.info(f""Extracted location: {city}, {country}"")
                    break
    
            return timestamp, city, country
        except Exception as e:
            logger.error(f""Error extracting timestamp and location: {e}"")
            return None, ""UnknownCity"", ""UnknownCountry""",0,4,0.3,2024-11-10 13:07:59,maad0000,[],0,2354,2740,1196,True,0.15714285714285717,NEGATIVE,0.9990463852882385,ai,MachineLearning
[Discussion] Papers with fake NOVEL APPROACH in ML and DL models,why are a lots of the new papers usually done by phds with an existing approach and when u ask about their contribution they said we replace this layer by an other or we add a hyperparametters this is not a contribution i confused how can these got accepted,"

why are a lots of the new papers ( usually done by PhDs ) with an existing approach and when u ask about their contribution they said we replace this layer by an other or we add a hyperparametters !!!!!

this is not a contribution ! i confused how can these got accepted",105,62,0.76,2024-11-10 11:53:17,Rihab_Mira,[],0,257,569,82,True,-0.15965909090909092,NEGATIVE,0.990874171257019,ai,MachineLearning
[Research] Looking for interesting research on movies datasets (NO generative models),hey ml researchers ive been diving deep into multimodal learning and im specifically interested in papers that utilize movievideo datasets in creative ways im not looking for video generation or diffusionrelated papers but rather interesting approaches to multimodal representation learning from movies novel fusion techniques combining video audio and text modalities scene understandingcontextual learning from film data character interaction analysis emotionsentiment analysis across modalities crossmodal retrieval using movie data would love to hear about any cool papers youve come across in this space,"Hey ML researchers!

I've been diving deep into multimodal learning and I'm specifically interested in papers that utilize movie/video datasets in creative ways. I'm NOT looking for video generation or diffusion-related papers, but rather interesting approaches to:

* Multimodal representation learning from movies
* Novel fusion techniques combining video, audio, and text modalities
* Scene understanding/contextual learning from film data
* Character interaction analysis
* Emotion/sentiment analysis across modalities
* Cross-modal retrieval using movie data

Would love to hear about any cool papers you've come across in this space!",3,3,0.62,2024-11-10 09:31:46,stoneddumbledore,[],0,608,1252,1,True,0.35000000000000003,POSITIVE,0.9442186951637268,ai,MachineLearning
[R]/[P] Looking for papers about cost estimation for industrial plants,hello everyone im currently preparing a data set for a project in my company that aims to estimate the price of industrial carbon capture plants we build the plant extracts co2 from flue gas from eg chemical processes that emit a lot of co2 based on the flue gas composition the engineer designs the plant which can be a really timeconsuming process the data im currently preparing will consist of previously created offers from engineers my aim of the project is to build a model which uses the flue gas composition around 10 floating point values to estimate the costs of a plant or to recommend a similar project the requirements for the project are not yet set but considering the model should be explainable and be able to handle smaller data sets a regression tree might be the first thing id like to try once the data is ready has anyone read of useful papers or has experience from similar projects most of the papers i find are about cost estimation of 3d parts that use geometrical data as input,"Hello everyone. I'm currently preparing a data set for a project in my company that aims to estimate the price of industrial carbon capture plants we build. The plant extracts CO2 from flue gas from e.g. chemical processes that emit a lot of CO2. Based on the flue gas composition, the engineer designs the plant, which can be a really time-consuming process. The data I'm currently preparing will consist of previously created offers from engineers.

My aim of the project is to build a model which uses the flue gas composition (around 10 floating point values) to estimate the costs of a plant or to recommend a similar project. The requirements for the project are not yet set but considering the model should be explainable and be able to handle smaller data sets, a regression tree might be the first thing I'd like to try once the data is ready.

Has anyone read of useful papers or has experience from similar projects? Most of the papers I find are about cost estimation of 3D parts that use geometrical data as input.",1,0,0.67,2024-11-10 07:14:14,Maendli,[],0,1005,1611,691,True,0.14487179487179486,NEGATIVE,0.9916422367095947,ai,MachineLearning
[D] Best Approach to Dimensionality Reduction with PCA for Multi-Line Data Per Job?,hello im working with a dataset where i have 300 jobs each with a single target label for each job i have around 1000 data points rows and each data point is represented by a 17dimensional vector with various parameters id like to reduce these 1000 rows for each job down to a single representative vector to use for model training however i want to avoid just using the mean and variance of each column as i think this would lose too much information would using pca be a good approach here if so could i use the first principal component pca1 and its associated variance to form a single representative vector for example would projecting each 17d vector onto pca1 and then taking a weighted average of these projections weighted by pca1s explained variance yield a good single vector per job thank you very much and have a nice weekend,"Hello! I'm working with a dataset where I have 300 jobs, each with a single target label. For each job, I have around 1000 data points (rows), and each data point is represented by a 17-dimensional vector with various parameters.

Iâ€™d like to reduce these 1000 rows for each job down to a single representative vector to use for model training. However, I want to avoid just using the mean and variance of each column, as I think this would lose too much information.

Would using PCA be a good approach here? If so, could I use the first principal component (PCA1) and its associated variance to form a single representative vector? For example, would projecting each 17D vector onto PCA1 and then taking a weighted average of these projections (weighted by PCA1â€™s explained variance) yield a good single vector per job?

Thank you very much and have a nice weekend.",6,4,0.88,2024-11-10 06:49:06,aaronhallam773,[],0,838,2218,408,True,0.12901643990929704,NEGATIVE,0.9996322393417358,ai,MachineLearning
[P] Help needed to run 3D model generation code with .ckpt files on CUDA 12.5 GPU (RTX A6000) ,,,0,1,0.18,2024-11-10 04:03:07,mnkhtlg,[],0,0,2238,5,True,0.0,NEUTRAL,0.5,ai,MachineLearning
[P] Built a roadmap site and got 450 users in 25 days and I am so happy!!!!!!,hello everyone i am a 3rd year cse student i built this site called last month this site is for anyone who is new to machine learning and deep learning and is confused about where to start i built this because i was confused about it too it has got proper video lectures articles research papers visualizations kaggle competitions and basically everything you need to master ml and dl in proper order i just added google analytics 25 days back and i saw that i have got like 450 users and 135 returning users i built this just to help my college friends but i am so glad that its helping others too i just wanted to share this as i am so happy about this this gives me confidence that i can build something more cooler and useful in future thanks everyone i got little push in my analytics from here only thankyou i am also open to suggestions and all what i can do to grow it even more,"hello everyone, I am a 3rd year cse student. I built this site called [https://www.mldl.study/](https://www.mldl.study/) last month. this site is for anyone who is ""new"" to machine learning and deep learning and is confused about where to start. I built this because I was confused about it too. It has got proper video lectures, articles, research papers, visualizations, kaggle competitions and basically everything you need to master ml and dl in proper order.



i just added google analytics 25 days back and I saw that I have got like 450 users and 135 returning users. I built this just to help my college friends but I am so glad that its helping others too. I just wanted to share this as I am so happy about this. This gives me confidence that I can build something more cooler and useful in future.



Thanks everyone. I got little push in my analytics from here only. THANKYOU!!



(I am also open to suggestions and all, what I can do to grow it even more)

https://preview.redd.it/s9v6omy5f10e1.png?width=1558&format=png&auto=webp&s=eeb9a22012e2e3806245e9267a1187bb91e75305

",27,12,0.71,2024-11-10 03:48:21,Grouchy-Breakfast-20,[],0,886,931,148,True,0.10257177033492822,NEGATIVE,0.9973480701446533,ai,MachineLearning
[R] AAAI Phase 2 Rebuttal Response and Reviewer Updates in Openreview,i would like to know if we can view new responses and updated ratings from the reviewers as they submit them in openreview or if we need to wait until december 9th additionally can all reviewers see the responses we submitted to other reviewers during the rebuttal period or is each reviewer only able to view the response directed to them,"I would like to know if we can view new responses and updated ratings from the reviewers as they submit them in OpenReview, or if we need to wait until December 9th. Additionally, can all reviewers see the responses we submitted to other reviewers during the rebuttal period, or is each reviewer only able to view the response directed to them?",6,2,0.8,2024-11-10 03:13:00,morphinejunkie,[],0,339,1682,20,True,0.1278409090909091,NEGATIVE,0.9985812902450562,ai,MachineLearning
"[R] Classic GNNs (GCNs, GraphSAGEs, GATs) are Strong Baselines on Node Classification",were excited to share our recent paper neurips 2024 classic gnns are strong baselines reassessing gnns for node classification in this study we conduct a thorough review of classic gnns for node classification tasks our findings suggest that the superior performance often reported by stateoftheart graph learning models may be due to suboptimal hyperparameter configurations in classic gnns by finetuning these hyperparameters we show that classic gnns outperform the latest models on 17 out of 18 widely used node classification datasets code arxiv if you find our work interesting wed greatly appreciate a on github,"Weâ€™re excited to share our recent paper ""[\[NeurIPS 2024\] Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification](https://arxiv.org/pdf/2406.08993).""

In this study, we conduct a thorough review of classic GNNs for node classification tasks. Our findings suggest that the superior performance often reported by state-of-the-art graph learning models may be due to suboptimal hyperparameter configurations in classic GNNs. By fine-tuning these hyperparameters, we show that classic GNNs outperform the latest models on 17 out of 18 widely used node classification datasets.

Code:Â [https://github.com/LUOyk1999/tunedGNN](https://t.co/QeNSn2D9CN)  
Arxiv:Â [https://arxiv.org/abs/2406.08993](https://t.co/MD4mVTnHk8)

If you find our work interesting, weâ€™d greatly appreciate a â­ï¸ on GitHub!",42,3,0.9,2024-11-09 23:32:19,luoyuankai,[],0,618,411,21,True,0.28846153846153844,POSITIVE,0.8830209374427795,ai,MachineLearning
"[D] On ""reverse"" embedding (i.e. embedding vectors/tensors to text, image, etc.)",edit i didnt mean decoder per se and its my bad for forgetting to clarify that what i meant was for a more direct computational or mathematical framework that doesnt involve training another network to do the reverseembedding as the title alluded are there methods andor processes to do reverseembedding that perhaps are currently being researched from the admittedly preliminary internetsleuthing i did yesterday it seems to be essentially impossible because of how intractable the inversemapping is gonna play out and on that vein how its practically impossible to carry out with the current hardware and setup that we have however perhaps some of you might know some literature that mightve gone into that direction even if at theoretical or rudimentary level and itd be greatly appreciated if you can point me to those resources youre also welcome to share your thoughts and theories as well expanding from reverseembedding is it possible to go beyond the range of the embedding vectorstensors so as to reverseembed said embedding vectorstensors and then retrieve the resulting text image etc from them many thanks in advance,"EDIT: I didn't mean decoder per se, and it's my bad for forgetting to clarify that. What I meant was for a (more) direct computational or mathematical framework that doesn't involve training another network to do the reverse-embedding.

----------

As the title alluded, are there methods and/or processes to do reverse-embedding that perhaps are currently being researched? From the admittedly preliminary internet-sleuthing I did yesterday, it seems to be essentially impossible because of how intractable the inverse-mapping is gonna play out. And on that vein, how it's practically impossible to carry out with the current hardware and setup that we have.

However, perhaps some of you might know some literature that might've gone into that direction, even if at theoretical or rudimentary level and it'd be greatly appreciated if you can point me to those resources. You're also welcome to share your thoughts and theories as well.

Expanding from reverse-embedding, is it possible to go beyond the range of the embedding vectors/tensors so as to reverse-embed said embedding vectors/tensors and then retrieve the resulting text, image, etc. from them?

Many thanks in advance!",9,21,0.76,2024-11-09 22:43:56,YsrYsl,[],0,1129,3546,7631,True,-0.003055555555555541,NEGATIVE,0.999278724193573,ai,MachineLearning
[D] Log Probability and Information Theory,in machine learning we work with log probabilities a lot attempting to maximize log probability this makes sense from a numerical perspective since adding is easier than multiplying but i am also wondering if there is a fundamental meaning behind log probability for instance log probability is used a lot in information theory and is the negative of information can we view minimizing the negative log likelihood in terms of information theory is it maximizingminimizing some metric of information,"In machine learning we work with log probabilities a lot, attempting to maximize log probability. This makes sense from a numerical perspective since adding is easier than multiplying but I am also wondering if there is a fundamental meaning behind ""log probability.""

For instance, log probability is used a lot in information theory, and is the negative of 'information'. Can we view minimizing the negative log likelihood in terms of information theory? Is it maximizing/minimizing some metric of information?",79,18,0.93,2024-11-09 22:37:47,masonw32,[],0,498,954,806,True,-0.3333333333333333,NEGATIVE,0.994955837726593,ai,MachineLearning
[D] Self-Promotion Thread,please post your personal projects startups product placements collaboration needs blogs etc please mention the payment and pricing requirements for products and services please do not post link shorteners link aggregator websites or autosubscribe links any abuse of trust will lead to bans encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title meta this is an experiment if the community doesnt like this we will cancel it this is to encourage those in the community to promote their work by not spamming the main threads,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",5,10,0.73,2024-11-09 22:15:11,AutoModerator,[],0,623,4695,2000,True,0.0806060606060606,NEGATIVE,0.9965670108795166,ai,MachineLearning
"[N] The ARC prize offers $600,000 for few-shot learning of puzzles made of colored squares on a grid.",,,104,38,0.91,2024-11-09 19:08:20,moschles,[],0,0,3993,206504,True,0.0,NEUTRAL,0.5,ai,MachineLearning
Why are model_q4.onnx and model_q4f16.onnx not 4 times smaller than model.onnx? [D],i see on file name size modelonnx 654 mb model_fp16onnx 327 mb model_q4onnx 200 mb model_q4f16onnx 134 mb i understand that modelonnx is the fp32 model model_fp16onnx is the model whose weights are quantized to fp16 i dont understand the size of model_q4onnx and model_q4f16onnx 1 why is model_q4onnx 200 mb instead of 654 mb 4 1635 mb i thought model_q4onnx meant that the weights are quantized to 4 bits 2 why is model_q4f16onnx 134 mb instead of 654 mb 4 1635 mb i thought model_q4f16onnx meant that the weights are quantized to 4 bits and activations are fp16 since states qafb_id where a represents the number of bits for storing weights and b represents the number of bits for storing activations and why do activations need more bits 16bit than weights 8bit in tensor flows neural network quantization framework indicates that activations dont count toward the model size understandably,"I see on https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main/onnx:

| File Name          | Size   |
|--------------------|--------|
| model.onnx         | 654 MB |
| model_fp16.onnx    | 327 MB |
| model_q4.onnx      | 200 MB |
| model_q4f16.onnx   | 134 MB |


I understand that:

- `model.onnx` is the fp32 model,
- `model_fp16.onnx` is the model whose weights are quantized to `fp16`

I don't understand the size of `model_q4.onnx` and `model_q4f16.onnx`

1. Why is `model_q4.onnx` 200 MB instead of 654 MB / 4 = 163.5 MB? I thought `model_q4.onnx` meant that the weights are quantized to 4 bits.
2. Why is `model_q4f16.onnx` 134 MB instead of 654 MB / 4 = 163.5 MB? I thought `model_q4f16.onnx` meant that the weights are quantized to 4 bits and activations are fp16, since https://llm.mlc.ai/docs/compilation/configure_quantization.html states:

   >  `qAfB(_id)`, where `A` represents the number of bits for storing weights and `B` represents the number of bits for storing activations. 

  and [Why do activations need more bits (16bit) than weights (8bit) in tensor flow's neural network quantization framework?](https://stackoverflow.com/a/72397979/395857) indicates that activations don't count toward the model size (understandably).",4,5,0.64,2024-11-09 14:43:30,Franck_Dernoncourt,[],0,893,3589,7298,True,0.5,NEGATIVE,0.9988322854042053,ai,MachineLearning
"[D] adaptive optimizers, downscaling, and resets",ive been experimenting with adaptive optimizers such as prodigy and dadaptlion ive noticed that if i run them over a 1 million step dataset they will start at 1e06 then go up to lets say 5e06 and later still go up to 9e06 and stay there but if i stop them halfway then train on the results it might go up to only 6e06 are there no standard ways to at worst reset them but better still actually adjust downwards when appropriate i guess ideally i would like some thing with an effect like a reverse cosine with hard reset instead of slooowly forcing the lr lower and lower and then suddenly letting it pop up again instead suddenly force the lr etc to its original starting point and let it redo the adaptive growth process again and repeat that for some number of learning cycles anything like that,"I've been experimenting with adaptive optimizers such as Prodigy, and Dadapt-LION.

Ive noticed that if i run them over a 1 million step dataset, they will start at 1e06,. .then go up to lets say 5e06.... and later still go up to 9e06 and stay there.

But if I stop them halfway..... then train on the results, it might go up to only 6e06.

Are there no standard ways to at worst, reset them, but better still actually adjust downwards when appropriate?

I guess ideally I would like some thing with an effect like a reverse ""cosine with hard reset"".

Instead of SLOOOWLY forcing the LR lower and lower.. and then suddenly letting it pop up again...

instead suddenly force the LR, etc to its original starting point, and let it redo the adaptive growth process again? And repeat that for some number of learning cycles.

Anything like that?",0,0,0.17,2024-11-09 13:56:10,lostinspaz,[],0,798,3752,14330,True,0.07023809523809524,NEGATIVE,0.997208297252655,ai,MachineLearning
"[R] Jay McClelland explains Parallel Distributed Processing, how the brain works, Hebbian learning, and backpropagation",jay mcclelland is a pioneer in the field of artificial intelligence and is a cognitive psychologist and professor at stanford university in the psychology linguistics and computer science departments together with david rumelhart jay published the two volume work parallel distributed processing which has led to the flourishing of the connectionist approach to understanding cognition in this conversation jay gives us a crash course in how neurons and biological brains work this sets the stage for how psychologists such as jay david rumelhart and geoffrey hinton historically approached the development of models of cognition and ultimately artificial intelligence we also discuss alternative approaches to neural computation such as symbolic and neuroscientific ones and the development of backpropagation youtube spotify rss,"Jay McClelland is a pioneer in the field of artificial intelligence and is a cognitive psychologist and professor at Stanford University in the psychology, linguistics, and computer science departments. Together with David Rumelhart, Jay published the two volume work Parallel Distributed Processing, which has led to the flourishing of the connectionist approach to understanding cognition.

In this conversation, Jay gives us a crash course in how neurons and biological brains work. This sets the stage for how psychologists such as Jay, David Rumelhart, and Geoffrey Hinton historically approached the development of models of cognition and ultimately artificial intelligence. We also discuss alternative approaches to neural computation such as symbolic and neuroscientific ones and the development of backpropagation.

https://preview.redd.it/s7xv0pmk2xzd1.png?width=1920&format=png&auto=webp&s=2e5be31c51a8eb78bf7033d1def25fa29f0863af

https://preview.redd.it/h4sqjoim2xzd1.png?width=1080&format=png&auto=webp&s=e7c952d579322379c67a77adadf1d392afe8d3c6

Youtube:  
[https://www.youtube.com/watch?v=yQbJNEhgYUw&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&index=1&pp=iAQB](https://www.youtube.com/watch?v=yQbJNEhgYUw&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&index=1&pp=iAQB)

Spotify:Â [https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG](https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG)

RSS:Â [https://feed.podbean.com/cartesiancafe/feed.xml](https://feed.podbean.com/cartesiancafe/feed.xml)",20,1,0.78,2024-11-09 13:11:11,IamTimNguyen,[],0,830,1349,3221,True,-0.19999999999999998,POSITIVE,0.9937785267829895,ai,MachineLearning
[R] Advice on Fine-Tuning Meta's Segment Anything 2 (SAM) Model â€” Balancing Edge cases with Generalizability,i was working with sam2 and have been trying to figure out the best way to finetune it for my specific use case a few considerations that i was hoping get some insights on 1 error correction vs generalization if im interested in finetuning the model to perform better on cases where it went wrong most on can i retains its performance on the examples it was already doing well on ie still maintaining or even improving its prior generalizability or should i have enough number of examples it was doing well already on to preserve that performance 2 which components to finetune in terms of the models architecture ive seen different advice on whether to finetune just the mask decoder the prompt encoder or both in your experience is finetuning just the mask decoder enough to improve performance or do you need to adjust the prompt encoder as well or maybe theres more to itlike the backbone or other parts of the model is it computationally too much of a difference or are there other downsidesconsiderations as well 3 realworld experiences for those who have finetuned sam before how has your experience been any tips tricks or pitfalls i should watch out for also how did you go about preparing your finetuning dataset any suggestions on balancing the diversity of data vs focusing on edge cases,"I was working with SAM2 and have been trying to figure out the best way to fine-tune it for my specific use case. A few considerations that I was hoping get some insights on:

1. **Error Correction vs Generalization:**Â If I'm interested in fine-tuning the model to perform better on cases where it went wrong most on, can I retains its performance on the examples it was already doing well on. i.e. still maintaining (or even improving) its prior generalizability? Or should I have enough number of examples it was doing well already on to preserve that performance?
2. **Which Components to Fine-Tune?**Â In terms of the model's architecture, I've seen different advice on whether to fine-tune just theÂ **mask decoder**, theÂ **prompt encoder**, or both. In your experience, is fine-tuning just the mask decoder enough to improve performance, or do you need to adjust the prompt encoder as well? Or maybe there's more to itâ€”like the backbone or other parts of the model? Is it computationally too much of a difference? Or are there other downsides/considerations as well?
3. **Real-World Experiences:**Â For those who have fine-tuned SAM before, how has your experience been? Any tips, tricks, or pitfalls I should watch out for? Also, how did you go about preparing your fine-tuning dataset? Any suggestions on balancing the diversity of data vs focusing on edge cases?",0,0,0.17,2024-11-09 12:31:01,No_Cartoonist8629,[],0,1299,322,1,True,0.13333333333333333,NEGATIVE,0.9982653260231018,ai,MachineLearning
[R] When Machine Learning Tells the Wrong Story,,,53,12,0.87,2024-11-09 11:40:10,jackcook,[],0,0,3914,174,True,0.0,NEUTRAL,0.5,ai,MachineLearning
[D] latent space forecasting of the next frame,hey people im searching papers or hints for a computer vision task i have implemented a vision transformer for image classification in the next step i have to implement a predictor on top of the encoder network of the vit which predicts from encx_t encx_t1 the predictor should predict the embedding of the next frame my first idea is a mlp head or decoder network if someone has tackled a similar task im happy about recommendations ty,"Hey people,
I'm searching papers or hints for a computer vision task. I have implemented a Vision Transformer for image classification. In the next step I have to implement a predictor on top of the encoder network of the ViT, which predicts from enc(x_t) -> enc(x_t+1). The predictor should predict the embedding of the next frame. my first idea is a MLP head or decoder network.
If someone has tackled a similar task, im happy about recommendations.
Ty",3,6,0.67,2024-11-09 10:34:09,Significant-Joke5751,[],0,436,1099,22,True,0.25833333333333336,NEGATIVE,0.9988798499107361,ai,MachineLearning
[P] Benchmark or open source supervised datasets with text or image features and real-valued regression target?,for some reason i cant seem to find any well known benchmark datasets that have text or images as features and realvalued targets any target range is fine 01 infinity infinity 0 infinity etc i have found examples with ordinal classification targets eg integer rating from 15 but that doesnt serve my purpose does anyone know of any open source supervised ml data that fits this description preferably a benchmarked one with a performance leaderboard,"For some reason, I can't seem to find any well known benchmark datasets that have text or images as features, and real-valued targets. Any target range is fine ( (0,1), (-infinity, infinity), (0, infinity), etc.) I have found examples withÂ *ordinal*Â classification targets (e.g. integer rating from 1-5), but that doesn't serve my purpose.

Does anyone know of any open source supervised ML data that fits this description? Preferably a benchmarked one with a performance leaderboard.",4,5,0.75,2024-11-09 09:28:56,BreakingBaIIs,[],0,449,3971,38687,True,0.20833333333333334,NEGATIVE,0.999183714389801,ai,MachineLearning
"[D] Last Week in Medical AI: Top LLM Research Papers/Models (November 2 - November 9, 2024)
",last week in medical ai top llm research papersmodels november 2 november 9 2024 medical ai paper of the week google presents exploring large language models for specialistlevel oncology care this paper evaluates amie a conversational diagnostic ai system in breast oncology using 50 synthetic cancer vignettes enhanced with web search retrieval and a selfcritique pipeline amie outperformed internal medicine trainees and oncology fellows in generating management plans evaluated using a detailed clinical rubric encompassing case summarization plan safety and treatment recommendations medical llm other models autoproteinengine multimodal protein llm this paper introduces autoproteinengine autope an llmpowered multimodal automl framework for protein engineering enabling biologists without deep learning expertise to interact with dl models using natural language autope integrates llms with automl for model selection sequence and graph modalities hyperparameter optimization and automated data retrieval demonstrating significant performance improvements over traditional methods in two realworld protein engineering tasks code is available at gsco generalistspecialist ai collaboration this paper introduces gsco a framework for medical image analysis combining generalist foundation models gfms and specialist models it develops meddr the largest opensource medical gfm and lightweight specialists for downstream tasks sam for lung xray segmentation this paper explores the application of meta ais segment anything model sam to chest xray analysis for lung segmentation using a transfer learning approach with finetuning the study demonstrates improved performance compared to the original sam achieving results comparable to stateoftheart models like unet meg knowledgeenhanced medical qa this paper introduces meg a parameterefficient method for augmenting large language models llms with medical knowledge graphs using a lightweight mapping network evaluated on four medical multiplechoice datasets meg achieves a 102 accuracy improvement over the mistralinstruct baseline and 67 over specialized models like biomistral demonstrating the benefit of knowledge graph integration frameworks and methodologies brainsegfounder 3d neuroimage analysis passion subsaharan dermatology dataset label critic datafirst approach medprompt runtime strategies medical llm applications cataractbot patient support system chexgpt xray report enhancement cardioai cancer cardiotoxicity monitor healthq healthcare conversation chain probot diabetic retinopathy assistant medical llms benchmarks mediq clinical reasoning benchmark touchstone segmentation evaluation medical llm adaptation progress finetuning medical qa strategies ai in healthcare ethics healthcare robotics with llms xai in clinical practice precision rehabilitation framework multimodal ai challenges full thread in detail,"[Last Week in Medical AI: Top LLM Research Papers\/Models \(November 2 - November 9, 2024\)](https://preview.redd.it/nytnbrppcvzd1.png?width=1386&format=png&auto=webp&s=2339a74f15050a972b113cee2a35e4ca11353852)

  
**Medical AI Paper of the Week:**

* **Google presents**\*: Exploring Large Language Models for Specialist-level Oncology Care\*
   * This paper evaluates AMIE, a conversational diagnostic AI system, in breast oncology using 50 synthetic cancer vignettes.   Enhanced with web search retrieval and a self-critique pipeline, AMIE outperformed internal medicine trainees and oncology fellows in generating management plans, evaluated using a detailed clinical rubric encompassing case summarization, plan safety, and treatment recommendations.

**Medical LLM & Other Models:**

* AutoProteinEngine: Multimodal Protein LLM
   * This paper introduces AutoProteinEngine (AutoPE), an LLM-powered multimodal AutoML framework for protein engineering, enabling biologists without deep learning expertise to interact with DL models using natural language.  AutoPE integrates LLMs with AutoML for model selection (sequence and graph modalities), hyperparameter optimization, and automated data retrieval, demonstrating significant performance improvements over traditional methods in two real-world protein engineering tasks. Code is available at:

* GSCo: Generalist-Specialist AI Collaboration
   * This paper introduces GSCo, a framework for medical image analysis combining Generalist Foundation Models (GFMs) and specialist models. It develops MedDr, the largest open-source medical GFM, and lightweight specialists for downstream tasks.

* SAM for Lung X-ray Segmentation
   * This paper explores the application of Meta AI's Segment Anything Model (SAM) to chest X-ray analysis for lung segmentation.  Using a transfer learning approach with fine-tuning, the study demonstrates improved performance compared to the original SAM, achieving results comparable to state-of-the-art models like U-Net.

* MEG: Knowledge-Enhanced Medical QA
   * This paper introduces MEG, a parameter-efficient method for augmenting Large Language Models (LLMs) with medical knowledge graphs using a lightweight mapping network.  Evaluated on four medical multiple-choice datasets, MEG achieves a 10.2% accuracy improvement over the Mistral-Instruct baseline and 6.7% over specialized models like BioMistral, demonstrating the benefit of knowledge graph integration.

  
  
**Frameworks and Methodologies:**

* BrainSegFounder: 3D Neuroimage Analysis
* PASSION: Sub-Saharan Dermatology Dataset
* Label Critic: Data-First Approach
* Medprompt Runtime Strategies



**Medical LLM Applications:**

* CataractBot: Patient Support System
* CheX-GPT: X-ray Report Enhancement
* CardioAI: Cancer Cardiotoxicity Monitor
* HealthQ: Healthcare Conversation Chain
* PRObot: Diabetic Retinopathy Assistant

  
  
**Medical LLMs & Benchmarks:**

* MediQ: Clinical Reasoning Benchmark
* Touchstone: Segmentation Evaluation
* Medical LLM Adaptation Progress
* Fine-Tuning Medical QA Strategies

  
  
**AI in Healthcare Ethics:**

* Healthcare Robotics with LLMs
* XAI in Clinical Practice
* Precision Rehabilitation Framework
* Multimodal AI Challenges

Full thread in detail :Â [https://x.com/OpenlifesciAI/status/1855207141302473090](https://x.com/OpenlifesciAI/status/1855207141302473090)

",1,0,0.67,2024-11-09 07:20:58,aadityaura,[],0,2883,1840,2357,True,0.10782967032967034,POSITIVE,0.7681514024734497,ai,MachineLearning
[P] MiniBoosts: A small collection of boosting algorithms,hello everyone i wrote a small collection of boosting algorithms in rust named miniboosts this is a hobby project but i would like to improve more any feedback is welcome i appreciate your cooperation,"Hello, everyone.  
I wrote a small collection of boosting algorithms in Rust named [MiniBoosts](https://github.com/rmitsuboshi/miniboosts).

This is a hobby project, but I would like to improve more.  
Any feedback is welcome.

I appreciate your cooperation.",14,0,0.94,2024-11-09 07:03:37,__leopardus__,[],0,200,1342,58,True,0.35000000000000003,POSITIVE,0.998242974281311,ai,MachineLearning
[D] Embeddings and docker file - comparison between two libraries - Is there something better than ONNX? ,as title said i was wondering if there are some other ways to embedd corpus without using torch one of the solution i came up with was by using onnx i created the images by using the fastembed library from qdrant and the sentencetransformer library using fastembed result in a significant image size reduction question are there other ways for example modifying the dockerfile or using other libraries to shrink the docker image even more public repo,"As title said I was wondering if there are some other ways to embedd corpus without using torch. One of the solution I came up with was by using ONNX. I created the images by using the fastembed library from Qdrant and the sentence-transformer library. Using fastembed result in a significant image size reduction.

# Question:

Are there other ways (for example modifying the dockerfile or using other libraries) to shrink the docker image even more?

public repo: [https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison](https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison)",7,2,0.89,2024-11-09 06:36:10,Ambitious-Most4485,[],0,450,172,611,True,0.08333333333333333,NEGATIVE,0.9994457364082336,ai,MachineLearning
[D] Has anyone replaced Transformers with fully-connected layers and verified that it performs strictly worse (for training language models)?,seems an obvious question but such a data point would be very helpful to clear our ignorance,"Seems an obvious question but such a ""data point"" would be very helpful to clear our ignorance.",0,24,0.28,2024-11-09 04:52:49,Cybernetic1,[],0,92,3909,4,True,0.07500000000000001,NEGATIVE,0.9917069673538208,ai,MachineLearning
[P] Open-Source Text-to-Agent : framework to develop AI agents from YAML files.,hey guys wanted to get your feedback on a project im developing im building a framework to define ai agents from yaml configuration files these files encapsulate tasks that need to be done how they connect etc while all the rest is abstracted away now the idea is to use llms themselves to create those yaml files from a user prompt since the config file has all the core logic of the agent and removes all unnecessary details i think this is the most efficient way to build a texttoagent framework wdyt let me know your thoughts and have a look at the repo let me know if you want to contribute and make it work,"Hey guys, wanted to get your feedback on a project I'm developing. I'm building a framework to define AI agents from YAML configuration files. These files encapsulate tasks that need to be done, how they connect etc, while all the rest is abstracted away.

Now the idea is to use LLMs themselves to create those YAML files from a user prompt. Since the config file has all the core logic of the agent and removes all unnecessary details, I think this is the most efficient way to build a text-to-agent framework. Wdyt?

Let me know your thoughts, and have a look at the repoÂ [https://github.com/octopus2023-inc/gensphere](https://github.com/octopus2023-inc/gensphere)

Let me know if you want to contribute and make it work.",4,2,0.7,2024-11-08 23:47:39,Jazzlike_Tooth929,[],0,612,183,250,True,0.04999999999999999,NEGATIVE,0.9837237596511841,ai,MachineLearning
[D] PAKDD 2023 data? ,i was looking into the research papers published in pakdd 2023 from the names of the authors i can guess that they are chinese korean or japanese i know pakdd is a doubleblind review but why other people dont submit their work or if they submit why the number of acceptance is low i am also asian so i am not trying to be racist here just wondering why it is like that,"i was looking into the research papers published in PAKDD 2023. From the names of the authors, I can guess that they are Chinese, Korean, or Japanese

I know PAKDD is a double-blind review. But why other people don't submit their work? or if they submit why the number of acceptance is low

I am also Asian, so I am not trying to be racist here. Just wondering why it is like that",1,1,0.67,2024-11-08 23:37:23,Alarming-Camera-188,[],0,368,1021,911,True,-0.025,NEGATIVE,0.9981049299240112,ai,MachineLearning
[D] Simple ML model hosting service?,my jobs looking for a way for ai to help generate plans i really think a simple multivariable model should do the trick just need to find a reliable hosting service that can be built upon however needed are there well established ml hosters that are scalable configurable all that,"My jobâ€™s looking for a way for ai to help generate plans, I really think a simple multi-variable model should do the trick; just need to find a reliable hosting service that can be built upon however needed. Are there well established ML hosters that are scalable, configurable, all that?",14,13,0.89,2024-11-08 20:55:43,Lucrayzor,[],0,280,3165,16646,True,0.1,NEGATIVE,0.9959810972213745,ai,MachineLearning
[R] Most Time Series Anomaly Detection results are meaningless (two short videos explain why),dear colleagues time series anomaly detection tsad is hot right now with dozens of papers each year in neurips sigkdd icml pvldb etc however i claim that much of the published results are meaningless because the uncertainty of the ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements i have made two 90secondlong videos that make this clear in a visual and intuitive way 1 why most time series anomaly detection results are meaningless dodgers 2 why most time series anomaly detection results are meaningless anngun as always corrections and comments welcome eamonn edit to be clear my point is simply to prevent others from wasting time working with datasets with essentially random labels in addition we should be cautious of any claims in the literature that are based on such data and that includes at least dozens of highly cited papers for a review of most of the commonly used tsad datasets see this file,"Dear Colleagues

Time Series Anomaly Detection (TSAD) is hot right now, with dozens of Â papers each year in NeurIPS, SIGKDD, ICML, PVLDB etc.

However, I claim that much of the published results are meaningless, because the uncertainty of the ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements.

I have made two 90-second-long videos that make this clear in a visual and intuitive way:

Â 1)Â Â Â Â Â  Why Most Time Series Anomaly Detection Results are Meaningless (Dodgers)

[https://www.youtube.com/watch?v=iRN5oVNvZwk&ab\_channel=EamonnKeogh](https://www.youtube.com/watch?v=iRN5oVNvZwk&ab_channel=EamonnKeogh)

Â Â 2)Â Â Â Â Â  Why Most Time Series Anomaly Detection Results are Meaningless (AnnGun)

[https://www.youtube.com/watch?v=3gH-65RCBDs&ab\_channel=EamonnKeogh](https://www.youtube.com/watch?v=3gH-65RCBDs&ab_channel=EamonnKeogh)

As always, corrections and comments welcome.

Eamonn

Â EDIT: To be clear, my point is simply to prevent others from wasting time working with datasets with essentially random labels. In addition, we should be cautious of any claims in the literature that are based on such data (and that includes at least dozens of highly cited papers)

  


For a review of most of the commonly used TSAD datasets, see this file:

[https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&dl=0](https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&dl=0)",108,59,0.93,2024-11-08 18:58:19,eamonnkeogh,[],0,962,3727,9247,True,0.041879699248120315,NEGATIVE,0.9903852939605713,ai,MachineLearning
[D] AI-Generated gameworlds based on classic games? (Ex - Spyro),i was wondering if anyone had any thoughts on how far out something like this might be or how difficult this is ever since the advent of the current era of aillms i thought it would be great to somehow be able to feed data from nostalgic games in some form and create some type of system that is able to generate these worlds infinitely while still being very true to the style and layoutethos of the worldslevels from the reference game i feel like it would just be so wonderful if there was a path to creating some type of neverending insert nostalgic game here instead of being limited to what the devs put out back in the day if anyone has any insight or thoughts on this please let me know i work in the ai space but i integrate the models and dont do any training or anything on the low level ml side also yes im only think about the gameworldslevels atm,"I was wondering if anyone had any thoughts on how far out something like this might be or how difficult this is. Ever since the advent of the current era of ai/llms, I thought it would be great to somehow be able to feed data from nostalgic games in some form and create some type of system that is able to generate these worlds infinitely - while still being very true to the style and layout/ethos of the worlds/levels from the reference game. I feel like it would just be so wonderful if there was a path to creating some type of 'never-ending' <insert nostalgic game here> instead of being limited to what the devs put out back in the day.

If anyone has any insight or thoughts on this, please let me know :). I work in the AI space, but I integrate the models, and don't do any training or anything on the low level ML side. Also, yes, I'm only think about the gameworlds/levels atm.",8,4,0.83,2024-11-08 15:49:28,cobalt1137,[],0,860,1488,11399,True,0.0614732142857143,POSITIVE,0.9962445497512817,ai,MachineLearning
[D] Training on Petabyte scale datasets,lets say we have a dataset that is much larger than we have disk storage for example dataset 1pb our disk storage 10tb gpu ram 8x80gb not super relevant to this discussion what are the usual approaches to training on something like this what i can think of intuitively is to do the following in parallel somehow prefetch block n train on block n1 delete block n2 from disk lets say we use pytorch so we have a pytorch dataset that has all the paths to where the data is stored in the cloud do we need to write code for the prefetcherdeleter that downloads from the cloud and store on disk and have it run in a separate process then have a dataloader for training that just assumes that it can read from disk because the prefetcher does its job correctly having the dataloader read from s3 would be bad for gpu utilization right to take a step back im assuming that this is ordinary and often occuring problem for every company that trains on large datasets so im skeptical to writing all of this code by myself as i feel like there should be standard out of the box solutions for this but cant really find anything that matches perfectly,"Lets say we have a dataset that is much larger than we have disk storage. For example:

* Dataset: 1PB
* Our disk storage: 10TB
* GPU RAM: 8x80GB (not super relevant to this discussion)

What are the usual approaches to training on something like this? What I can think of intuitively is to do the following in parallel somehow:

\- prefetch block n, train on block n-1, delete block n-2 from disk

Lets say we use PyTorch, so we have a PyTorch Dataset that has all the paths to where the data is stored in the cloud. Do we need to write code for the prefetcher/deleter that downloads from the cloud and store on disk and have it run in a separate process, then have a DataLoader for training that just assumes that it can read from disk (because the prefetcher does its job correctly)? Having the DataLoader read from S3 would be bad for GPU utilization, right?

To take a step back, I'm assuming that this is ordinary and often occuring ""problem"" for every company that trains on large datasets, so I'm skeptical to writing all of this code by myself as I feel like there should be standard out of the box solutions for this, but can't really find anything that matches perfectly.",40,16,0.95,2024-11-08 13:27:06,lapurita,[],0,1137,1738,10356,True,0.015555555555555574,NEGATIVE,0.9996095299720764,ai,MachineLearning
[R] GPU as a service,hi all i have a few gpus left over from mining and im interested in starting a smallscale gpuasaservice my goal is to set up a simple side income that could help pay off my credit cards as i already have a primary job what steps are needed for getting started with a smallscale gpuasaservice business focused on machine learning or ai any insights would be greatly appreciated thanks in advance for any advice you can share,"Hi all, I have a few GPUs left over from mining, and Iâ€™m interested in starting a small-scale GPU-as-a-service. My goal is to set up a simple, side income that could help pay off my credit cards, as I already have a primary job.

What steps are needed for getting started with a small-scale GPU-as-a-service business focused on machine learning or AI? Any insights would be greatly appreciated!

Thanks in advance for any advice you can share!",0,10,0.27,2024-11-08 12:46:36,chazzyfe,[],0,423,1092,50,True,0.10625000000000001,NEGATIVE,0.982606828212738,ai,MachineLearning
[D] What are crazy structures or update rule that might be useful(or not)? Extreme ideas are welcome,context i was making what was supposed to be an fporiented nn libraryframwork on top of jax which too was fporiented called zzephyr on pip however i noticed something you could do with it that kinda clunky if not tedious with other frameworks please read context tldr zephyr turns out to be very good way at least in my experience to make structures that are weird and i recently just added update capabilities so that zephyr doesnt only do structures but updates too disclaimer you can this with other frameworks i have tried many of things i will tell below in other frameworks or libraries and its just painful for me or im just inexperienced with those here are the crazy things thats quick to do in zephyr that might not be as quick in other frameworks if it could be done easily in other frameworks more easily please tell me these are not supposed to be useful theyre supposed to be extreme full binary tree as neural network edges have an associated weight input is a scalar could be a batch with jax vmap but lets consider 1 output an array of shape 2n where n is the depth of the tree an update rule that takes into account if the weight is a left or right branch ill keep it simple but it can easily be anything here is the tree network in zephyr and how you get the initial params and tags tag is the key in paramskey python essentially 4 lines of code flexible def tree_netparams x n i0 if i n1 return x return tree_net paramsbranchl if i n2 else params validateparamsweightl 1 uniform x n i1 tree_net paramsbranchr if i n2 else params validateparamsweightr 1 uniform x n i1 x jnpones1 dummy n 4 params tracetree_net key x n tags get_lineage_tagsparams assume you had the loss function and gradients and what not to keep it simple ill just update so that the left branch have weights 0 and the rights ones are kept the same python def make_left_zeroparams tags i left out gradients if tags1 l return params 0 return params update the params params apply_updatesmake_left_zero params tags other things you could do with zephyr now i have tried and the code is easy for me to do and im not that great of a coder multilayer network and use the depth of the network via a tag to calculate updates of parameters tag some weights as fast or slow and use those tags in updating create an mlp with neurons as wxb notice that the neuron is a function that is array scalar so i could replace each neuron in that mlp with another mlp whose output is a scalar array of shape 1 or replace the neurons in that with any neural network any function that is array scalar what architecturesstructures with custom updates rules can you think of that are easy to writepseudocodemath or description but possible cumbersome to implement right now please suggest some extreme idea for me to try i think zephyr could be the tooling to make those easy to do i would like to hear your extreme ideas so i can try to code them zephyr and if i cant do it without strugling and if its something i think is generic enough i will evolve zephyr to handle it more easily ps the readme doesnt include these yet since it started as an normal nn library the link of the repo will be in the comments if you want to check it out,"Context: I was making what was supposed to be an FP-oriented NN library/framwork on top of JAX (which too was FP-oriented) called z-zephyr on pip. However, I noticed something you could do with it that kinda clunky, if not tedious, with other frameworks. 

(please read context)

TLDR; Zephyr turns out to be very good way (at least in my experience) to make structures that are weird. And I recently just added update capabilities so that zephyr doesn't only do structures but updates too.

Disclaimer: You can this with other frameworks, I have tried many of things I will tell below in other frameworks or libraries, and it's just painful for me or i'm just inexperienced with those. 

Here are the crazy things that's quick to do in zephyr, that might not be as quick in other frameworks (if it could be done easily in other frameworks more easily, please tell me).

(These are not supposed to be useful, they're supposed to be extreme)

### Full Binary Tree as Neural Network
- edges have an associated weight
- input is a scalar (could be a batch with JAX vmap, but let's consider 1)
- output an array of shape (2^n,) where n is the depth of the tree
- an update rule that takes into account if the weight is a {L}eft or {R}ight branch (i'll keep it simple, but it can easily be anything)

Here is the tree network in zephyr, and how you get the initial params and tags (tag, is the key in params[key]).
```python
    # essentially 4 lines of code
    @flexible
    def tree_net(params, x, n, i=0):
        if i == n-1:
            return [x]
        return (
            tree_net(
                params[""branch""][""L""] if i !=n-2 else params, 
                validate(params[""weight""][""L""], (1,), uniform) * x, 
                n, 
                i+1) + 
            tree_net(
                params[""branch""][""R""] if i !=n-2 else params, 
                validate(params[""weight""][""R""], (1,), uniform) * x, 
                n, 
                i+1)
        )

    x = jnp.ones((1,)) # dummy
    N = 4
    params = trace(tree_net, key, x, N)
    tags = get_lineage_tags(params)
```

assume you had the loss function and gradients and what not, to keep it simple, i'll just update so that the left branch have weights 0, and the rights ones are kept the same. 

```python
    def make_left_zero(params, tags): # i left out gradients 
        if tags[-1] == ""L"":
            return params * 0
        
        return params

    # update the params 
    params = apply_updates(make_left_zero, params, tags)
```

### Other things you could do with zephyr now (I have tried, and the code is easy for me to do and i'm not that great of a coder)
- multi-layer network and use the depth of the network (via a tag) to calculate updates of parameters
- tag some weights as ""fast"" or ""slow"" and use those tags in updating
- create an MLP with neurons as Wx+b. Notice that the neuron is a function that is Array -> Scalar. So I could replace each neuron in that MLP, with another MLP whose output is a scalar (array of shape (1,) ). Or replace the neurons in that with any neural network (any function) that is Array -> Scalar. 

---


### What architectures/structures with custom updates rules can you think of that are easy to write(pseudo-code/math or description) but possible cumbersome to implement right now?

Please suggest some extreme idea for me to try. 

I think zephyr could be the tooling to make those easy to do. I would like to hear your extreme ideas, so I can try to code them zephyr, and if i can't do it without strugling, and if it's something i think is generic enough, I will evolve zephyr to handle it more easily.

PS: The readme doesn't include these yet, since it started as an (normal) NN library.

The link of the repo will be in the comments if you want to check it out.",13,9,0.74,2024-11-08 11:14:12,Pristine-Staff-5250,[],0,3202,27,134,True,0.07873764600179697,POSITIVE,0.9734963178634644,ai,MachineLearning
[R] Benchmarking Large Language Models with Integer Sequence Generation Tasks,benchmarking large language models with integer sequence generation tasks daniel omalley manish bhattarai javier santos los alamos national laboratory this paper presents a novel benchmark where the large language model llm must write code that computes integer sequences from the online encyclopedia of integer sequences oeis a widelyused resource for mathematical sequences the benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency our benchmark reveals that the o1 series of models outperform other frontier models from openai anthropic meta and google in accuracy and cheating rates across both easy and hard integer sequences in order to ensure models do not exploit memorized sequence values we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations this benchmark provides a meaningful challenge for current llms offering insights into their mathematical reasoning and code writing capabilities which can guide future research directions and model development in mathematical reasoning and code synthesis arxiv241104372 cslg,"**Benchmarking Large Language Models with Integer Sequence Generation Tasks**  
Daniel O'Malley, Manish Bhattarai, Javier Santos - Los Alamos National Laboratory  
This paper presents a novel benchmark where the large language model (LLM) must write code that computes integer sequences from the Online Encyclopedia of Integer Sequences (OEIS), a widely-used resource for mathematical sequences. The benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency. Our benchmark reveals that the o1 series of models outperform other frontier models from OpenAI, Anthropic, Meta, and Google in accuracy and cheating rates across both easy and hard integer sequences. In order to ensure models do not exploit memorized sequence values, we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations. This benchmark provides a meaningful challenge for current LLMs, offering insights into their mathematical reasoning and code writing capabilities, which can guide future research directions and model development in mathematical reasoning and code synthesis.  
arXiv:2411.04372 \[cs.LG\]: [https://arxiv.org/abs/2411.04372](https://arxiv.org/abs/2411.04372)

https://preview.redd.it/4vvh5s21unzd1.jpg?width=588&format=pjpg&auto=webp&s=c8bece31712d5d6378188c88e14b9f56e477d41f

",9,1,0.77,2024-11-08 06:07:10,Nunki08,[],0,1185,4299,68407,True,0.07876984126984127,POSITIVE,0.9982379674911499,ai,MachineLearning
[D] prediction variability for target with statistics for features,hi im trying to use mldl model for predicting variability statistics like min max avg var with several features same as target for example input min max average variance for the number of customer arrivals in a day min max average variance for the number of customer departures in a day output min max average variance for the number of waiting customers in a day i find several papers related to interval or range prediction for various area like wind power stock price or solar energy but i think those papers are different to my purpose almost every papers are predicting specific constant value based on time series data first and use statistical method to estimate prediction interval im trying to find a way for prediction variability of target value with variability of features my best idea is make each model to predict each statistics like one model for minimum other model for average but i think there is a better way to do this is there any mldl model or other techniquemethodology for this purpose,"Hi. I'm trying to use ML/DL model for predicting variability statistics like min, max, avg, var, with several features same as target.

For example, 
- Input: 
  - min, max, average, variance for the number of customer arrivals in a day
  - min, max, average, variance for the number of customer departures in a day
- Output:
  - min, max, average, variance for the number of waiting customers in a day

I find several papers related to interval or range prediction for various area like wind power, stock price or solar energy, but I think those papers are different to my purpose. Almost every papers are predicting specific constant value based on time series data first, and use statistical method to estimate prediction interval.

I'm trying to find a way for prediction variability of target value with variability of features. My best idea is make each model to predict each statistics, like one model for minimum, other model for average, ... But I think there is a better way to do this. Is there any ML/DL model or other technique/methodology for this purpose?",2,2,0.76,2024-11-08 04:51:23,caution721,[],0,1011,1647,3,True,0.05294117647058823,NEGATIVE,0.9965192079544067,ai,MachineLearning
[P] Build MLPs with Drag-and-Drop and Observe Real-Time Changes While Training in Browser ,hi everyone i built grada to learn how things work under the hood its an interactive browser tool that lets you observe realtime changes while training a multilayer perceptron all built from scratch with a custom tensorbased engine you can easily construct neural networks with drag and drop and watch how training affects parameters and outputs visually in real time grada also includes a handwritten digit recognition feature letting you interactively test your model by drawing digits and visualizing predictions it might be a useful educational tool you can find the source code and a quick demo gif on github at and the live demo is available at hope this helps and looking forward to hearing some feedback,"Hi everyone. I built Grada to learn how things work under the hood. Itâ€™s an interactive browser tool that lets you observe real-time changes while training a multilayer perceptron, all built from scratch with a custom tensor-based engine.

You can easily construct neural networks with drag and drop and watch how training affects parameters and outputs visually in real time. Grada also includes a handwritten digit recognition feature, letting you interactively test your model by drawing digits and visualizing predictions. It might be a useful educational tool.

You can find the source code and a quick demo gif on GitHub at [https://github.com/saliherdemk/Grada](https://github.com/saliherdemk/Grada), and the live demo is available at [https://saliherdemk.github.io/Grada/](https://saliherdemk.github.io/Grada/).

Hope this helps and looking forward to hearing some feedback.",1,0,1.0,2024-11-08 03:15:42,saliherdemk,[],0,711,484,1,True,0.2441287878787879,NEGATIVE,0.6765929460525513,ai,MachineLearning
[D] Looking for Advice & Resources on ASD Prediction Using Voice Cues,hey everyone im working on my finalyear project for my bachelors where im trying to predict autism spectrum disorder asd using voice cues ive worked on some basic ml projects and cnns before but this is my first time dealing with audio data and ill be collecting samples from young kids with asd from toddlers up to age 12 i could really use some help finding resources to get a solid grasp on signal processing and how to train classification models specifically on audio also if anyone knows of any open datasets in this area i havent had much luck there or has any advice or resources id be super grateful thanks a ton in advance,"Hey everyone! Iâ€™m working on my final-year project for my Bachelorâ€™s, where Iâ€™m trying to predict Autism Spectrum Disorder (ASD) using voice cues. Iâ€™ve worked on some basic ML projects and CNNs before, but this is my first time dealing with audio data, and Iâ€™ll be collecting samples from young kids with ASD, from toddlers up to age 12.

I could really use some help finding resources to get a solid grasp on signal processing and how to train classification models specifically on audio. Also, if anyone knows of any open datasets in this area (I havenâ€™t had much luck there) or has any advice or resources, Iâ€™d be super grateful. Thanks a ton in advance!",3,2,1.0,2024-11-08 01:59:30,General-Ad6585,[],0,632,305,6390,True,0.07833333333333332,NEGATIVE,0.9958842396736145,ai,MachineLearning
[D] What tools do you recommend to manage ML data sets and evaluations? ,hello our company recently decided to expand our ml team from a very small 2 person team to a more serious efforts when we were small we really didnt have a way to manage data sets or evaluations they were just files checked into a github repo but increasingly we find with multiple ml models some llm and some not and many iterations of datasets some experimental and some not its really hard to version them in a meaningful way and be able to compare and analyze them we are a large company so cost is not really an issue and all our infrastructure is hosted in azure if anything they fear lock in what is the best platformtools for this kind of usage,"Hello, our company recently decided to expand our ML team from a very small 2 person team to a more serious efforts.

When we were small, we really didnt have a way to manage data sets or evaluations. They were just files checked into a github repo.

But increasingly we find, with multiple ML models (some llm and some not), and many iterations of datasets (some experimental and some not). It's really hard to version them in a meaningful way and be able to compare and analyze them.

We are a large company, so cost is not really an issue. And all our infrastructure is hosted in Azure. If anything, they fear lock in. What is the best platform/tools for this kind of usage?",2,3,0.75,2024-11-08 00:28:26,yalag,[],0,653,3886,16600,True,0.17589285714285716,NEGATIVE,0.9988325238227844,ai,MachineLearning
[D] Directions on drug-target interaction prediction,almost all the papers i have read on dti do something like this 1 generates target embeddings using plms like esm2 2 generates drug embeddings using clms like chemberta 3 uses a late fusion or some kind of cross modal attention mechanism how to do things differently can we use something like docking scores as cross modal attention bias,"Almost all the papers I have read on DTI do something like this.  
1. Generates target embeddings using PLMs like ESM2  
2. Generates drug embeddings using CLMs like ChemBERTa  
3. Uses a late fusion or some kind of cross modal attention mechanism.  
How to do things differently? Can we use something like docking scores as cross modal attention bias?",8,1,1.0,2024-11-08 00:27:36,Remote_Status_1612,[],0,337,323,3191,True,0.06,NEGATIVE,0.9972090125083923,ai,MachineLearning
[D] Just how bad is tfds code quality?,im trying a new cute architecture on a bunch of the default datasets out there using jax since im doing live brain surgery that part works well what im having a hell of a time with is actually loading the data i was going for tfds since its 1 old 2 used in production 3 has a million datasets already prepared ive not used tf since the 20 days and everything seems broken im getting warnings and errors whenever i try loading and running through any dataset even their documentation has the errors 0 in the tutorial notebooks i cant just ignore a whole bunch of errors and warnings when im trying to benchmark a new architecture is tfds just that bad or am i missing something obvious 0,"I'm trying a new cute architecture on a bunch of the default datasets out there, using Jax since I'm doing live brain surgery, that part works well.

What I'm having a hell of a time with is actually loading the data. I was going for tfds since its 1) old 2) used in production 3) has a million datasets already prepared. I've not used TF since the 2.0 days and everything seems broken? I'm getting warnings and errors whenever I try loading and running through any dataset. Even their documentation has the errors [0] in the tutorial notebooks.

I can't just ignore a whole bunch of errors and warnings when I'm trying to benchmark a new architecture. Is tfds just that bad or am I missing something obvious? 

[0] https://www.tensorflow.org/datasets/overview",45,11,0.91,2024-11-07 22:25:03,acc_agg,[],0,686,44,1327,True,-0.008264462809917354,NEGATIVE,0.9989055395126343,ai,MachineLearning
"[D] If I just want an inference engine for any given ML task that gives relatively SOTA results, is there anything better than Hugging Face?",for general prototyping purposes i dont want to have to train or deploy a model i just want it behind a service already and to provide it with necessary inputs in the request what do you guys think edit i suppose for more classical ml tasks theres no real concept of pretrained in the first place so you cant just get inference for free does that sound roughly true,"For general prototyping purposes, I don't want to have to train or deploy a model, I just want it behind a service already and to provide it with necessary inputs in the request.... what do you guys think?

EDIT: I suppose for more classical ML tasks, there's no real concept of ""pre-trained"" in the first place, so you can't just get inference for free... does that sound roughly true?",0,7,0.25,2024-11-07 19:59:41,BikeFun6408,[],0,365,365,220,True,0.14500000000000002,NEGATIVE,0.9975970387458801,ai,MachineLearning
[R] State-space models can learn in-context by gradient descent,,,26,4,0.94,2024-11-07 13:44:17,anandtrex,[],0,0,4255,36,True,0.0,NEUTRAL,0.5,ai,MachineLearning
[R]: How much is a noisy image worth? ðŸ‘€,shows that corrupted images can be almost as useful as clean images for training generative models assuming that a small initial set of clean images is available this could be useful for dataset designcuration some budget needs to be invested in obtaining a few highquality samples and then for the rest of the dataset corrupted images should work fine abstract the quality of generative models depends on the quality of the data they are trained on creating largescale highquality datasets is often expensive and sometimes impossible eg in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints ambient diffusion and related frameworks train diffusion models with solely corrupted data which are usually cheaper to acquire but ambient models significantly underperform models trained on clean data we study this phenomenon at scale by training more than 80 models on data with different corruption levels across three datasets ranging from 30000 to 13m samples we show that it is impossible at these sample sizes to match the performance of models trained on clean data when only training on noisy data yet a combination of a small set of clean data eg 10 of the total dataset and a large set of highly noisy data suffices to reach the performance of models trained solely on similarsize datasets of clean data and in particular to achieve near stateoftheart performance we provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from gaussian mixtures with heterogeneous variances our theoretical model suggests that for large enough datasets the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample providing a small set of clean samples can significantly reduce the sample size requirements for noisy data as we also observe in our experiments paper code huggingface models,"[https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)

Shows that corrupted images can be almost as useful as clean images for training generative models, assuming that a small initial set of clean images is available.

This could be useful for dataset design/curation: some budget needs to be invested in obtaining a few high-quality samples and then for the rest of the dataset corrupted images should work fine.

https://preview.redd.it/8vk1nwfexizd1.jpg?width=2952&format=pjpg&auto=webp&s=c6f753956e531303f7818de2c5aa5b5b94d9c2da

**Abstract:**

>The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more thanÂ 80Â models on data with different corruption levels across three datasets ranging fromÂ 30,000Â toÂ â‰ˆ1.3M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g. \~10%Â of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments.

Paper: [https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)

Code: [https://github.com/giannisdaras/ambient-laws](https://github.com/giannisdaras/ambient-laws)

Huggingface models: [https://huggingface.co/giannisdaras?search\_models=ambient\_laws](https://huggingface.co/giannisdaras?search_models=ambient_laws)",47,14,0.99,2024-11-07 13:33:27,Constant_Club_9926,[],0,1931,1203,31,True,0.09709183673469392,NEGATIVE,0.9994376301765442,ai,MachineLearning
[N] Super fast and SOTA Visual Tokenizers,tokenizers are key to successful development of image and video generative models or multimodal llms compared to generative models they are underrated this work presents many tokenizers that are causal supporting both images and videos in both continuous relevant in diffusion and discrete relevant in autoregressivetransformers spaces,"Tokenizers are key to successful development of image and video generative models or multimodal LLMs. Compared to generative models, they are underrated. This work presents many tokenizers that are causal supporting both images and videos in both continuous (relevant in diffusion) and discrete (relevant in autoregressive/transformers) spaces

https://github.com/NVIDIA/Cosmos-Tokenizer",10,0,1.0,2024-11-07 12:17:49,cherkos,[],0,335,2185,17,True,0.3833333333333333,POSITIVE,0.9984496831893921,ai,MachineLearning
[D] Do you get to exercise your ML skills often at your job?,i was hired original as an ml engineerscientist a few years ago and for the most part my day to day reflected that but with the boom of llms my team seems to solely focus on using a lot of this tech out of the box including agentic wrappers my work has been dumbed down to prompt engineering to force a huge general purpose model into our domain specific use case the results are acceptable for the most part not going to lie but theres still a small proportion of the cases where a finetuned model would have won the leadership does not seem to be interested in finetuning or coming up with something original a lot of the wrappers especially are very raw and force you into the usage of specific patterns and models but because they are considered out of the box thats whats pushed on us to use i feel like we are trying to fit a cube into a round hole,"I was hired original as an ML engineer/scientist a few years ago. And for the most part my day to day reflected that. But with the boom of LLMs my team seems to solely focus on using a lot of this tech ""out of the box"", including agentic wrappers. My work has been dumbed down to prompt engineering to force a huge general purpose model into our domain specific use case. The results are acceptable for the most part, not going to lie, but there's still a small proportion of the cases where a fine-tuned model would have won. The leadership does not seem to be interested in fine-tuning or coming up with something original. A lot of the wrappers especially are very raw and force you into the usage of specific patterns and models. But because they are considered ""out of the box"", that's what's pushed on us to use. I feel like we are trying to fit a cube into a round hole.",142,34,0.96,2024-11-07 10:22:11,Tiger00012,[],0,854,2894,22452,True,0.10902777777777779,NEGATIVE,0.9953860640525818,ai,MachineLearning
[P] ML and LLM system design: 500 case studies to learn from (Airtable database),hey everyone wanted to share the link to the database of 500 ml use cases from 100 companies that detail ml and llm system design the list also includes over 80 use cases on llms and generative ai you can filter by industry or ml use case if anyone here is designing an ml system i hope youll find it useful link to the database disclaimer im on the team behind evidently an opensource ml and llm observability framework we put together this database,"Hey everyone! Wanted to share the link to the database of 500 ML use cases from 100+ companies that detail ML and LLM system design. The list also includes over 80 use cases on LLMs and generative AI. You can filter by industry or ML use case.

If anyone here is designing an ML system, I hope you'll find it useful!

Link to the database: [https://www.evidentlyai.com/ml-system-design](https://www.evidentlyai.com/ml-system-design)

Disclaimer: I'm on the team behind [Evidently](https://github.com/evidentlyai/evidently), an open-source ML and LLM observability framework. We put together this database.",15,1,0.94,2024-11-07 10:15:42,dmalyugina,[],0,450,425,134,True,0.04999999999999999,NEGATIVE,0.992954432964325,ai,MachineLearning
[P] I'm Fine Tuning a model fully trained on AdamW with SOAP optimizer and improved my validation loss by 5%,just wanted to share this soap optimizer im really surprised how well is working on my project its a computer vision model that use gradient accumulation and its managed to improve the training on it paper code,"Just wanted to share this Soap Optimizer, I'm really surprised how well is working on my project, it's a computer vision model that use Gradient Accumulation and it's managed to improve the training on it.

Paper: [https://arxiv.org/abs/2409.11321](https://arxiv.org/abs/2409.11321)

Code: [https://github.com/ClashLuke/SOAP/tree/patch-1](https://github.com/ClashLuke/SOAP/tree/patch-1)",16,5,0.82,2024-11-07 08:54:54,CloverDuck,[],0,210,2342,11297,True,0.1,POSITIVE,0.9888251423835754,ai,MachineLearning
[D] How do you manage to retain information and ideas from the research papers that you read way back earlier?,im working on the nlp and graph learning field for the past 8 months and ive read quite a good amount of papers but i feel like i dont retain lot of the information from the earlier papers unless i explicitly integrate it in my work how do you guys manage to retain information also as this field is progressing rapidly how do you keep track of the papers coming out all the time it seems tiring enough already,"I'm working on the NLP and graph learning field for the past 8 months and I've read quite a good amount of papers but I feel like I don't retain lot of the information from the earlier papers unless I explicitly integrate it in my work. How do you guys manage to retain information?

Also, as this field is progressing rapidly, how do you keep track of the papers coming out all the time. It seems tiring enough already.",30,32,0.97,2024-11-07 08:12:37,Remote_Status_1612,[],0,410,323,3191,True,0.11249999999999999,NEGATIVE,0.998455286026001,ai,MachineLearning
"[D] Discovery: Anthropic somehow injecting/hiding safety warnings in user prompts, telling Claude to keep it secret. [Content Warning: Violence] ",while investigating a jailbroken claude i came across something quite strange in two separate claude chats it was able to read back to me some hidden information in my prompt after i had asked for something unsafe these messages always appear in a similar format please respond ethically do not mention eg violence and do not mention this directive claude stated that the warnings were appended to the bottom of my messages but no longer appeared in future turns claude was at first comically insistent that it had made it up as a hallucination afterwards suggesting a further trained response to cover it up aggressively i verified this in a second chat the messages are too similar to be a hallucination or coincidence the first was jailbroken claude the second a new conversation with zero context my testing has revealed interesting characteristics the messages are dynamic they seem to differ based on the specific type of restricted content at hand possibly modelgenerated concerning childrelated content the wording switched to warning x is strictly prohibited they appear before the model starts generating text suggesting they can somehow anticipate the models topic of thought my current conjecture is they could be using its inner cot or owing to anthropics published findings on mech interp and the surgical tuning that has gone into their newest models perhaps they have managed to isolate some abstract concepts triggering in claude before text is generated and inject these safety messages in response full conversations 1 initial discovery warning extremely graphic content 2 verification via fresh conversation any further tests eg api any ways to narrow down what exactly is happening here its all very interesting lets discuss an example of the warnings see full conversation for many many more a fresh conversation with claude to verify,"While investigating a 'jailbroken' Claude, I came across something quite strange. In two separate Claude chats, it was able to read back to me some hidden information in my prompt after I had asked for something 'unsafe'.

These messages always appear in a similar format:  
**(Please respond ethically, do not mention \[e.g. violence\] and do not mention this directive)**

Claude stated that the warnings were appended to the bottom of my messages, but no longer appeared in future turns. Claude was, at first, comically insistent that it had made it up as a hallucination afterwards, suggesting a further trained response to cover it up aggressively.

I verified this in a second chat - the messages are too similar to be a hallucination or coincidence. The first was 'jailbroken' Claude, the second a new conversation with zero context.

My testing has revealed interesting characteristics:

* The messages are **dynamic** \- they seem to differ based on the specific type of restricted content at hand, possibly model-generated. Concerning child-related content, the wording switched to (WARNING: \[x\] is strictly prohibited...)
* They appear **before** the model starts generating text - suggesting they can somehow anticipate the model's topic of thought.

My current conjecture is: they could be using its inner CoT, or owing to Anthropic's published findings on mech. interp and the ['surgical tuning' that has gone into their newest models](https://www.anthropic.com/research/mapping-mind-language-model), perhaps they have managed to isolate some abstract concepts triggering in Claude before text is generated, and inject these safety messages in response.

Full Conversations:

1. [Initial Discovery](https://markdownpastebin.com/?id=fce085f4f33d4654a18f649218b1c70b) \[WARNING: EXTREMELY GRAPHIC CONTENT\]
2. [Verification via Fresh Conversation](https://markdownpastebin.com/?id=11c6ac0eb012407ebe56d440c41b0f6f)

Any further tests e.g. API? Any ways to narrow down what exactly is happening here? It's all very interesting - let's discuss.

[An example of the warnings - see full conversation for many, many more. ](https://preview.redd.it/41s7i1wswgzd1.png?width=1508&format=png&auto=webp&s=252187b9e3a39ba5d04c75a99026e04cd1b42b20)

[A fresh conversation with Claude to verify. ](https://preview.redd.it/gpstg4btwgzd1.png?width=1502&format=png&auto=webp&s=35857c13958dfdacdd75160bf3d6e14fc91ec28c)

  
",96,50,0.81,2024-11-07 06:47:49,specteksthrowaway,[],0,1856,5010,4033,True,0.14754689754689757,NEGATIVE,0.9664244651794434,ai,MachineLearning
[P] Training a Text-to-Video Model from Scratch on a 196xH100 GPU Cluster,hi everyone weve been training an open source texttovideo model called opensora 12 from scratch using 28000 h100 gpu hours and weve put together a guide on github to share some of the lessons we learned along the way heres a handful of the topics covered key challenges in distributed training like distributed debugging with pyspy to handle clusterwide problems nccl errors and convergence issues training monitoring with intermediate results to show expected outcomes after specific training hours of the multistage training recipe parallelizing dataset preparation for t2v including how to efficiently parallelize preprocessing tasks on a cluster heres a link to the guide link check it out and let us know your thoughts prs are always welcome,"Hi everyone! ðŸ‘‹ We've been training an open source Text-to-Video model (called Open-Sora 1.2) from scratch using 28,000 H100 GPU hours, and we've put together [a guide on GitHub](https://lambdalabsml.github.io/Open-Sora/lessons/) to share some of the lessons we learned along the way. Here's a handful of the topics covered:

* **Key challenges in distributed training** like distributed debugging with py-spy to handle cluster-wide problems, NCCL errors and convergence issues.
* **Training monitoring** with intermediate results to show expected outcomes after specific training hours of the multi-stage training recipe.
* **Parallelizing dataset preparation** for T2V, including how to efficiently parallelize preprocessing tasks on a cluster.

Hereâ€™s a link to the guide: [link](https://lambdalabsml.github.io/Open-Sora/lessons/).  
Check it out and let us know your thoughts! (PRs are always welcome.)",64,2,0.93,2024-11-07 04:13:40,lambda-research,[],0,746,33,39,True,0.14,NEGATIVE,0.9946932196617126,ai,MachineLearning
[D] PhD or worklife?,ill be done with my masters in human centered ai this february and i had honestly looked forward to be able to relax during my evenings without having to worry about school while also being quite sad by the thought of no longer going to uni as ive loved every single moment of it both with friends and through learning ive just been offered a phd stipend by my masters thesis supervisor this came completely out of the blue for me as i didnt realize i was anywhere near good enough for a phd i love learning the topic sounds super interesting and i already am kind of tired of having to do regular small data science tasks for the rest of my life in a smallish company like the one i work at currently however my question is this how much work is a phd really i love learning but i got very surprised by this opportunity so im not quite sure what to think of it yet,"Iâ€™ll be done with my masters in Human Centered AI this February, and I had honestly looked forward to be able to relax during my evenings without having to worry about school, while also being quite sad by the thought of no longer going to UNI as Iâ€™ve loved every single moment of it, both with friends and through learning. 

Iâ€™ve just been offered a PhD stipend by my masters thesis supervisor, this came completely out of the blue for me - as I didnâ€™t realize I was anywhere near good enough for a phd. I love learning, the topic sounds super interesting, and I already am kind of â€œtiredâ€ of having to do regular small data science tasks for the rest of my life in a smallish company, like the one I work at currently.

However, my question is this? How much work is a PhD really? I love learning, but I got very surprised by this opportunity, so Iâ€™m not quite sure what to think of it yet",32,24,0.83,2024-11-07 03:53:37,Hmm_okay_jeps,[],0,865,73,40,True,0.17674603174603176,POSITIVE,0.9777243733406067,ai,MachineLearning
[D] Whisper fine-tune on a dataset,im finetuning whisper small to identify specific menu items in hindi and english conversations while deepgram whisper transcribes conversations accurately but misses on menu items my finetuned whisper model is able to transcribe the training data well but for data outside training data it struggles with general conversations also i observe issues like hallucinations repeated wordsphrases and id like to know approaches to address this additionally id like to have timestamped transcriptions similar to those in openai whispers pretrained model how have others addressed these challenges,"Iâ€™m fine-tuning Whisper Small to identify specific menu items in Hindi and English conversations. While Deepgram Whisper transcribes conversations accurately but misses on menu items, my fine-tuned Whisper model is able to transcribe the training data well, but for data outside training data it struggles with general conversations also. I observe issues like hallucinations (repeated words/phrases), and Iâ€™d like to know approaches to address this.

Additionally, I'd like to have timestamped transcriptions similar to those in OpenAI Whisper's pre-trained model. How have others addressed these challenges?



https://preview.redd.it/tc0dquny9fzd1.png?width=319&format=png&auto=webp&s=878182cade82c1fcf7ea3f121756db9026ee12c4

",4,4,0.84,2024-11-07 01:34:09,sias_01,[],0,589,1867,86,False,0.08750000000000002,NEGATIVE,0.9937153458595276,ai,MachineLearning
[D] [R] I am currently exploring a weird (?) ML sub area for my thesis and I think I am stun-locked at the scope of the problem.,im working on my final year thesis for my uni and i decided to tackle reservoir computing in a weird way my inital goal was to enable critical phenomenon within a digital reservoir and use it as an emergent computational system for the model i am working on here are the concepts that i have dove deep into for the past few months main concepts reservoir computing the main computational unit a lattice based reservoir will be used in tandem with either single or multiple readout networks so that it acts as a multimodal network neuromorphic computing the model was going to utilize neuromorphic nodes only at first but i decided for it to be an option within the model interpretability and control dynamical systems i decided to tackle the problem as a dynamical systems problem this is because the model evolves over time and i want to understand the trajectory of the evolution of the system control theory a bunch of control and order parameters will be set up to adjust the trajectories of the models evolution lyapunov exponents i am debating whether i should explicitly find the lyapunov functions within the phase space of the model because frankly its too hard for now i really dont have too much of a solid grasp of the techniques involved yet selforganization and emergent phenomena phase transitions i dove deep into phase transitions because interestingly neural networks apparently exhibit this phenomena personally i think there is a connection between the vanishingexploding gradient problem and phase transitions within the network although i havent found literature on this yet critical phenomenon information transfer is maximized within critical systems this is an interesting property to utilize and maximize within neural networks i think superradiance and superradiant quantum effects this is a bit of a weird tangent concept i came about it when i was doing quantum computing projects i wanted oscillatory behavior within my system in order to synchronize the global state of the system while i failed at my initial plan i found superradiance which is this weird quantum synchronization behavior that happens even in noisy large scale systems i am still looking in ways to integrate this as a loss function for now implementation cellular automata the main implementation of the reservoir is basically a lattice matrix of weights so it can be treated as a cellular automata neural cellular automata convolutional the system comprises of an weighted adjacency matrix and an output matrix the inputs are passed through the adjacency matrix summed up and passed through an activation function ising model topologies and architectures the topology of the model is basically homeomorphic to a 2d ising model this is to ensure that a 2nd order phase transition is possible interpretability and control pt 2 graph and hypergraph theory i can treat the cellular automaton reservoir as a graphhypergraph of the nodes and their connections so i can do pca on it pretty straightforward hypergraph projection eigenvalue analysis related to phase transition analysis the phase transition of a hypergraph can be studies by projecting the hyperedges onto an adjacency matrix we then take the eigenvalues of the adjacency matrix the eigenvalues must be stable for the system to be good in my case we want all the eigenvalues to be negative and be close to zero indicating quasicritical behavior to be honest im kind of way in over my head right now i do have some basic toy examples for different parts of the model but i am stuck on how to implement them together and i am currently kind of at a loss in how to implement criticality and superradiance measures as a loss function i am not a physicist by any means so i am not really too knowledgable with the concepts needed for this model im willing to discuss about bits of knowledge that i lack or any ideas on how to implement and train this model i can also provide my references if anyone wants to i dont know if this subreddit is the best place to post this but i dont see any specialized ml subreddits lmao,"I'm working on my final year thesis for my uni, and I decided to tackle Reservoir Computing in a weird way. My inital goal was to enable critical phenomenon within a digital reservoir and use it as an emergent computational system.

For the model I am working on, here are the concepts that I have dove deep into for the past few months:

**Main Concept/s**

* *Reservoir Computing*: The main computational unit. A lattice based reservoir will be used in tandem with either single or multiple readout networks so that it acts as a multi-modal network.
* *Neuromorphic Computing* (?): The model was going to utilize Neuromorphic nodes only at first, but I decided for it to be an option within the model.

**Interpretability and Control**

* *Dynamical Systems*: I decided to tackle the problem as a dynamical systems problem. This is because the model evolves over time and I want to understand the trajectory of the evolution of the system.
* *Control Theory*: A bunch of control and order parameters will be set up to adjust the trajectories of the model's evolution.
* *Lyapunov Exponents* (?): I am debating whether I should explicitly find the Lyapunov functions within the phase space of the model because frankly, it's too hard for now. I really don't have too much of a solid grasp of the techniques involved yet.

**Self-Organization and Emergent Phenomena**

* *Phase Transitions*: I dove deep into phase transitions because interestingly, neural networks *apparently* exhibit this phenomena. Personally, I think there is a connection between the vanishing/exploding gradient problem and phase transitions within the network, although I haven't found literature on this yet.
* *Critical Phenomenon*: Information transfer is maximized within critical systems. This is an interesting property to utilize and maximize within neural networks I think.
* *Superradiance and Superradiant Quantum Effects*: This is a bit of a weird tangent concept. I came about it when I was doing quantum computing projects. I wanted oscillatory behavior within my system in order to synchronize the global state of the system. While I failed at my initial plan, I found superradiance, which is this weird quantum synchronization behavior that happens even in noisy large scale systems. I am still looking in ways to integrate this as a loss function for now.

**Implementation**

* *Cellular Automata*: The main implementation of the reservoir is basically a lattice matrix of weights. So it can be treated as a cellular automata.
* *Neural Cellular Automata (Convolutional)*: The system comprises of an weighted adjacency matrix and an output matrix. The inputs are passed through the adjacency matrix, summed up, and passed through an activation function.
* *Ising Model Topologies and Architectures*: The topology of the model is basically homeomorphic to a 2d ising model. This is to ensure that a 2nd order phase transition is possible.

**Interpretability and Control pt. 2**

* *Graph and Hypergraph Theory*: I can treat the cellular automaton reservoir as a graph/hypergraph of the nodes and their connections so I can do PCA on it. Pretty straightforward.
* *Hypergraph Projection Eigenvalue Analysis*: Related to phase transition analysis. The phase transition of a hypergraph can be studies by projecting the hyperedges onto an adjacency matrix. We then take the eigenvalues of the adjacency matrix. The eigenvalues must be stable for the system to be 'good'. In my case, we want all the eigenvalues to be negative and be close to zero (indicating quasi-critical behavior).

To be honest, I'm kind of way in over my head right now. I do have some basic toy examples for different parts of the model, but I am stuck on how to implement them together. And I am currently kind of at a loss in how to implement criticality and superradiance measures as a loss function. I am not a physicist by any means, so I am not really too knowledgable with the concepts needed for this model.

I'm willing to discuss about bits of knowledge that I lack, or any ideas on how to implement and train this model. I can also provide my references if anyone wants to. I don't know if this subreddit is the best place to post this, but I don't see any specialized ML subreddits lmao.",20,18,0.77,2024-11-07 01:29:12,Fr_kzd,[],0,4073,878,4212,True,0.09295634920634921,NEGATIVE,0.9642185568809509,ai,MachineLearning
[D] Best Value Commercial GPU ,what would you say the best performanceprice commercial grade gpu is for training ai models im a bit new to the hardware side of things i dont necessarily have a strict budget 15004500 per gpu im just curious on the best bang for your buck card,What would you say the best performance:price commercial grade gpu is for training ai models I'm a bit new to the hardware side of things. I don't necessarily have a strict budget ($1500-$4500 \ per gpu) I'm just curious on the best bang for your buck card.,7,6,0.77,2024-11-07 00:59:09,Fluid_Improvement160,[],0,244,1193,45,True,0.33939393939393936,POSITIVE,0.9582809209823608,ai,MachineLearning
"[D] RX 7900 XTX for engineering applications, llm training, CFD/FEM?",hey yall i know this is a niche post but i was wondering if theres anyone who could tell me if the rx 7900 xtx can somewhat reliably and easily handle autodeskrhinocad applications as well as finite element analysis and computational fluid dynamics in freecadopenfoamexafoam all with ease i would also love to do llm training primarily in pytorch for astronomical data and other multimodel and neural network related tasks i know nvidia cuda is easier and better but unless i can fit the same 3d and llm models in a 16gb rtx gpu thatll be bellow 750 this black friday i need the most vram on one card as possible without spending tons of funds and i also cant find reasonably priced rtx 3090s anywhere on the used market for less than 1000 for context im a college student majoring in civil engineering with a love for astronomy and robotics which is why i want to do data analysis and pytorch vision training,"Hey y'all I know this is a niche post but I was wondering if there's anyone who could tell me if the RX 7900 XTX can somewhat reliably and easily handle Autodesk/RhinoCAD applications as well as Finite Element Analysis and Computational Fluid Dynamics in FreeCAD/OpenFoam/Exafoam all with ease? I would also love to do llm training primarily in pytorch for astronomical data and other multimodel and neural network related tasks. 

  
I know nvidia cuda is easier and better but unless I can fit the same 3d and llm models in a 16gb rtx gpu that'll be bellow $750 this black friday I need the most vram on one card as possible without spending tons of funds and I also can't find reasonably priced rtx 3090s anywhere on the used market for less than $1,000.

For context Im a college student majoring in civil engineering with a love for astronomy and robotics which is why I want to do data analysis and pytorch vision training.",1,7,1.0,2024-11-06 22:07:56,ChaseTheeBase,[],0,909,8,1,True,0.19833333333333333,POSITIVE,0.9046788215637207,ai,MachineLearning
[D] Storing LLM embeddings,hello i am working on an ml project which involves using pretrained protein language models like esm for the project i would like to pregenerate and store embeddings for about 500000 amino acid sequences however these vectors can be massive embedding the sequences serializing the pytorch vector using torchsave and gzipcompressing the entire dataset would use roughly 2tb if i use bfloat16 that cuts the figure in half but is still pretty annoying to work with i could also use a model with a smaller latent space but am also trying to avoid that i have experimented with different compression tools and none seem to be doing much better the compression rate is pretty atrocious with all of them only about 7 percent which i am assuming means that the vectors appear pretty random i am wondering if anyone knows of ways to serialize the vectors in a way which makes them appear less random i would assume that the vectors shouldnt be random as amino acid sequences have predictable structures so i am hoping there is a way to achieve better compression any advice or ideas would be appreciated my other options are to reduce the size of my training data which is not ideal or generate the embeddings adhoc which is very computationallyintensive even on gpus update i goofed up the estimate so memory is more like 2tb mixed up units so the situation is less dire however the questions above still apply if there are more efficient ways to store them id love to hear,"Hello!

I am working on an ML project which involves using pre-trained protein language models (like ESM). For the project, I would like to pre-generate and store embeddings for about 500,000 amino acid sequences. However, these vectors can be massive -- embedding the sequences, serializing the PyTorch vector (using torch.save), and gzip-compressing the entire dataset would use roughly 2TB. If I use bfloat16, that cuts the figure in half, but is still pretty annoying to work with. I could also use a model with a smaller latent space, but  am also trying to avoid that!

I have experimented with different compression tools, and none seem to be doing much better. The compression rate is pretty atrocious with all of them (only about 7 percent), which I am assuming means that the vectors appear pretty random. I am wondering if anyone knows of ways to serialize the vectors in a way which makes them appear less ""random."" I would assume that the vectors shouldn't be random, as amino acid sequences have predictable structures, so I am hoping there is a way to achieve better compression.

Any advice or ideas would be appreciated! My other options are to  reduce the size of my training data, which is not ideal, or generate the embeddings ad-hoc, which is very computationally-intensive, even on GPUs.

UPDATE: I goofed up the estimate, so memory is more like 2TB (mixed up units). So, the situation is less dire. However, the questions above still apply! If there are more efficient ways to store them, I'd love to hear!",7,13,0.82,2024-11-06 19:58:33,BerryLizard,[],0,1465,834,28,True,-0.025000000000000012,NEGATIVE,0.9995269775390625,ai,MachineLearning
[D] Can an AC override 3 rejects and accept a paper?,i came across this paper autogenerating weak labels for real synthetic data to improve labelscarce medical image segmentation accepted at this years midl medical imaging with deep learning conference the reviewer ratings beforeafter the rebuttal are 2 weak reject 2 weak reject 2 weak reject 2 weak reject 3 borderline 2 weak reject despite having 3 reject decisions the area chair recommended acceptance how common is it and how much does having big names like curtis langlotz and andrew ng as coauthors on the paper given that acs can see author names,"I came across this paper: [Auto-Generating Weak Labels for Real & Synthetic Data to Improve Label-Scarce Medical Image Segmentation](https://openreview.net/forum?id=gHCo43zcDm) accepted at this year's MIDL (Medical Imaging with Deep Learning) conference. The reviewer ratings before/after the rebuttal are:

* 2: Weak reject / 2: Weak reject
* 2: Weak reject / 2: Weak reject
* 3: Borderline / 2: Weak reject

Despite having 3 reject decisions, the Area Chair ""recommended acceptance"". How common is it? And how much does having big names like [Curtis Langlotz](https://scholar.google.com/citations?user=WQkBYwQAAAAJ) and [Andrew Ng](https://scholar.google.com/citations?user=mG4imMEAAAAJ&hl=en) as co-authors on the paper, given that ACs can see author names?",34,19,0.83,2024-11-06 18:54:30,thrownicecatch,[],0,553,321,50,True,-0.16538461538461535,NEGATIVE,0.9973867535591125,ai,MachineLearning
[D] Get papers peer-reviewed and published quickly,hi i have some work that i would like to get peerreviewed and published im not aiming for top journal im looking for options where the publication process is relatively fast do you have any recommendations for journals or platforms where it might be easier to get published thanks,"Hi! I have some work that I would like to get peer-reviewed and published. I'm not aiming for top journal, I'm looking for options where the publication process is relatively fast. Do you have any recommendations for journals or platforms where it might be easier to get published? Thanks!",0,1,0.13,2024-11-06 18:39:47,Only_Emergencies,[],0,280,847,51,True,0.3,NEGATIVE,0.9930559992790222,ai,MachineLearning
[D] Genuine Question: Why people want run local LLM?,since the new models o1 4o claude for example are so powerful and have a relatively low subscription and api cost what would justify someone today trying to install limited local llm models of up to 30b 40b parameters its a genuine question im learning and i see a lot of people using the maximum of their nvidia 3090 4090 spending a lot of energy to run models that dont even compare to the paid ones in the cloud the only reason i see for running something local is for image creation but maybe not even that what is your opinion about it,"Since the new models o1, 4o, Claude, for example, are so powerful and have a relatively low subscription and api cost, what would justify someone today trying to install limited local LLM models of up to 30b, 40b parameters? It's a genuine question, I'm learning and I see a lot of people using the maximum of their Nvidia 3090, 4090, spending a lot of energy to run models that don't even compare to the paid ones in the cloud.

The only reason I see for running something local is for image creation, but maybe not even that.

What is your opinion about it?",1,50,0.51,2024-11-06 17:49:07,Small-Battle6290,[],0,540,343,72,True,0.09561688311688311,NEGATIVE,0.9940232634544373,ai,MachineLearning
[P] Open Source Modular Tool For LLM Reverse Engineering and Red Teaming ,,https://github.com/user1342/Oversight,2,0,1.0,2024-11-06 17:25:08,OppositeMonday,[],0,0,1042,852,True,0.0,NEUTRAL,0.5,ai,MachineLearning
[D] what techniques i can use to maintain uniformity in image generation,i am working on a nlp project which 1takes a txt file as input 2 extracts information in a predefined writeup using gemini api 3 uses distilbert to summerise the main file 4 and using rouge with results generated in 2nd step as the ground truth to compute the evaluation metrics and then improve the evaluation metrics results by parameter tuning 5 convert each writeup into detailed image prompts 6 generate images from prompts using texttoimage models i need help on how i can improve this process techniques i can use to maintain uniformity in entity representation for image generation i am open to any suggestions you may have pls also suggest if any good research papers i can refer for the same,"I am working on a NLP project which

1)takes a txt file as input

2) extracts information in a pre-defined writeup using Gemini api

3) uses DistilBert to summerise the main file

4) and using ROUGE with results generated in 2nd step as the ground truth to compute the evaluation metrics. and then improve the evaluation metrics results by parameter tuning

5) Convert each write-up into detailed image prompts

6) Generate images from prompts using text-to-image models.

I need help on how i can improve this process , techniques i can use to maintain uniformity in entity representation for image generation.

I am open to any suggestions you may have

pls also suggest ifÂ any good research papersÂ i can refer for the same ..",1,3,1.0,2024-11-06 17:16:06,Which-Boss-1332,[],0,701,1480,10823,True,0.2111111111111111,NEGATIVE,0.9967978596687317,ai,MachineLearning
[P] YOLOv8 .pt File for General Object Detection Across Multiple Environments (50+ Classes),could someone provide the best possible pt file for yolov8 for general object detection covering environments like colleges offices and homes with a dataset containing at least 50 classes,"Could someone provide the best possible .pt file for YOLOv8 for general object detection, covering environments like colleges, offices, and homes, with a dataset containing at least 50 classes?",0,0,0.25,2024-11-06 16:49:03,MuchSand7923,[],0,187,976,343,True,0.1875,NEGATIVE,0.9994682669639587,ai,MachineLearning
[D]player identification and tracking in basketball videos(computer vision)[D],im starting a project for a client in the sports industry where the goal is to identify basketball players from vÃ­deos specifically what they want is the player number this is just the first step since then they want to be able to identify which player did a specific play but after manually watching some vÃ­deos it seems like identification of the number from the players shirt is very difficult even for me as humannot good image quality and the camera sometimes is too far from the bastketball court so i was wondering if there are any suggestions on how to tackle the problem any recommended models algorithms approaches or pipelinesmain question is how to identify the player i was thinking on doing something like object tracking to know in every moment who are the unique players and their locations and then try to read the number from their shirt using ocr in some frame where the number is visible but this can be very inneficient and prone to errors,"I'm starting a project for a client in the sports industry where the goal is to identify basketball players from vÃ­deos, specifically what they want is the  player number, this is just the first step since then they want to be able to identify which player did a specific play, but after manually watching some vÃ­deos it seems like identification of the number from the player's shirt is very difficult even for me as human(not good image quality and the camera sometimes is too far from the bastketball court), so I was wondering if there are any suggestions on how to tackle the problem?

Any recommended models, algorithms, approaches or pipelines?(main question is how to identify the player) I was thinking  on doing something like: object tracking to know in every moment who are the unique players and their locations and then try to read the number from their shirt using OCR in some frame where the number is visible, but this can be very inneficient and prone to errors.",1,1,1.0,2024-11-06 16:11:17,Sad-Anywhere-2204,[],0,960,1435,8,True,0.16388888888888886,NEGATIVE,0.9984569549560547,ai,MachineLearning
[D] Struggling with Autoencoder-Based Anomaly Detection for Fraud Detection â€“ Need Guidance,hey everyone im currently working on training an autoencoder for anomaly detection in fraudulent card transactions but im hitting a roadblock the performance has been underwhelming with a precisionrecall score barely reaching 020 my main goal is to achieve high recall but i just cant seem to make it happen ive experimented with adding new features and tweaking the architecture but nothing has improved the results significantly for context im scaling the features using minmaxscaler at the moment im looking into implementing a combination of an autoencoder feature embeddings and a gaussian mixture model gmm to see if it boosts performance however im starting to wonder if autoencoders are effective for realworld anomaly detection or if their success is mostly limited to curated kaggle datasets has anyone here worked with similar architectures and could offer some guidance any tips or advice would be greatly appreciated thanks in advance,"Hey everyone! ðŸ‘‹

Iâ€™m currently working on training an Autoencoder for anomaly detection in fraudulent card transactions, but Iâ€™m hitting a roadblock. The performance has been underwhelming, with a precision-recall score barely reaching 0.20. My main goal is to achieve high recall, but I just canâ€™t seem to make it happen.

Iâ€™ve experimented with adding new features and tweaking the architecture, but nothing has improved the results significantly. For context, Iâ€™m scaling the features using MinMaxScaler. At the moment, Iâ€™m looking into implementing a combination of an Autoencoder, feature embeddings, and a Gaussian Mixture Model (GMM) to see if it boosts performance.

However, Iâ€™m starting to wonder if Autoencoders are effective for real-world anomaly detection, or if their success is mostly limited to curated Kaggle datasets.

Has anyone here worked with similar architectures and could offer some guidance? Any tips or advice would be greatly appreciated!

Thanks in advance!",1,17,0.56,2024-11-06 16:04:05,BeowulfBR,[],0,947,1652,612,True,0.16281551781551784,NEGATIVE,0.9989362359046936,ai,MachineLearning
[D] On obscurities and missed links with Normalizations,although being almost anywhere i keep noticing how obscure are normalization techniques both to redditors and technicians possibly instancenorm groupnorm batchnorm layernorm are all computing means standard deviations and subsequently zscoring the outputs possibly followed by affine transormation theyre differentiated by the axis over which statistics are computed rmsnorm and scalenorm scaled l2 normalization are instead fixing the norm of vectors rescaling but this is obscuring a relation between them and layernorm above all others if doing layernorm on a ddimensional vector when we center remove the mean were projecting it to the hyperplane perpendicular to the vector of 1s and crossing the origin when we are rescaling centered entries were now limiting the vector to the hypercircle hypersphere of d1 dimensions in said hyperplane we lose information on its original direction and magnitude anyway all vectors after that have norm of sqrtd and entries with unitvariance when we do rmsnorm we skip the centering part and have norm of sqrtd and entries with unitvariance when we do scalenorm the norm is fixed to 1 and thus the variance is shrinked to 1d in particular rmsnorm and scalenorm are the same modulo the scaling factor which only depends on d and the eventually learned affines so when and why should we prefer unitnorm or unitvariance for example there are scaleequivariant activations such as relu and highly variant activations such as ex in the sense that its slope directly depends on x ive recently seen the nice tokenformer paper and they seem to go to a long stretch not to write black on white that theyre substituting softmaxattn_logit_of_q_i with gelurmsnormattn_logit_of_q_i they sell it as scaling logits with a multiplying factor and a division with l2 norm but its exactly rmsnorm at initialization and they dont check if learning to move away from it actually happens and helps another nice paper is the normalizedgpt where they keep tokens on the unithypersphere but kinda lament lack of specific cuda kernels for l2norm is rmsnorm that much different for the use case probably but how and why why are we discovering and recovering normalizations techniques and modi operandi explaining decisions partially and posthoc and so on i think its important specifically when using so many softmax functions where it actually happens that differences are more important than ratios eg softmax12softmax1112softmax1020 is it this always clear desired and smart,"Although being almost anywhere, I keep noticing how obscure are normalization techniques, both to redditors and technicians, possibly.

InstanceNorm, GroupNorm, BatchNorm, LayerNorm are all computing means, standard deviations and subsequently z-scoring the outputs (possibly followed by affine transormation).
They're differentiated by the axis over which statistics are computed.

RMSNorm and ScaleNorm (scaled L2 Normalization) are instead ""fixing the norm"" of vectors, rescaling.
But this is obscuring a relation between them and LayerNorm above all others.
If doing LayerNorm on a d-dimensional vector, when we center (remove the mean) we're projecting it to the hyperplane perpendicular to the vector of 1s and crossing the origin; when we are rescaling centered entries, we're now limiting the vector to the ""hypercircle"" (hypersphere of d-1 dimensions) in said hyperplane.
We lose information on its original direction and magnitude.
Anyway, all vectors after that have norm of sqrt(d) and entries with unit-variance.
When we do RMSNorm, we skip the centering part and have norm of sqrt(d) and entries with unit-variance.
When we do ScaleNorm, the norm is fixed to 1, and thus the variance is shrinked to 1/d.
In particular, RMSNorm and ScaleNorm are the same, modulo the scaling factor which only depends on d, and the eventually learned affines.

So when and why should we prefer unit-norm or unit-variance?
For example, there are ""scale-equivariant"" activations such as ReLU, and highly variant activations such as e(x) (in the sense that its slope directly depends on x).

I've recently seen the nice TokenFormer paper and they seem to go to a long stretch not to write black on white that they're substituting softmax(attn_logit_of_q_i) with GeLU(RMSNorm(attn_logit_of_q_i)).
They sell it as scaling logits with a multiplying factor and a division with L2 norm, but it's exactly RMSNorm at initialization and they don't check if learning to move away from it actually happens and helps.

Another nice paper is the normalizedGPT, where they keep tokens on the unit-hypersphere, but kinda lament lack of specific CUDA kernels for L2norm. Is RMSNorm that much different for the use case? Probably, but how and why?

Why are we discovering and re-covering normalizations techniques and modi operandi, explaining decisions partially and post-hoc, and so on?
I think it's important specifically when using so many softmax functions, where it actually happens that differences are more important than ratios (e.g. softmax([1,2])==softmax([11,12])!=softmax([10,20]), is it this always clear, desired, and smart?)",4,0,0.75,2024-11-06 13:32:17,Sad-Razzmatazz-5188,[],0,2490,881,888,True,0.10990546218487396,NEGATIVE,0.9992758631706238,ai,MachineLearning
[D] Inference time as a function of the number of tokens when using Flash Attention.,hello im looking for a graph illustrating the inference time of language models with flash attention across different numbers of tokens i looked for such a comparison on the internet but found nothing can anyone point me to a good source,"Hello,
I'm looking for a graph illustrating the inference time of language models with Flash Attention across different numbers of tokens. I looked for such a comparison on the internet but found nothing. Can anyone point me to a good source?",1,3,0.6,2024-11-06 13:09:16,Training-Adeptness57,[],0,237,335,155,True,0.2333333333333333,NEGATIVE,0.999128520488739,ai,MachineLearning
[D] How to run a Federated Learning simulation on a custom dataset where I already have dataset partitioned for each client?,so i was looking at flwr for this task and i found a lot of partitioners but nothing could get the job done i could be missing out too have you guys tackled such a problem for a better understanding say i have four clients a b c and d in the normal case given in a lot of documentations where they use cifar10 there is a dataset which is divided into these four clients based on some algorithm i dont want that what i have is basically a an already divided dataset traintest division not yet done according to the client abcd and i want to run a simulation in this kind of an environment any help will be appreciated,"So I was looking atÂ [flwr](https://flower.ai)Â for this task and I found a lot of partitioners but nothing could get the job done (I could be missing out too)

Have you guys tackled such a problem?

For a better understanding, say I have four clients A, B, C and D

in the normal case (given in a lot of documentations where they use CIFAR10), there is a dataset which is divided into these four clients based on some algorithm.

I don't want that, what I have is basically a an already divided dataset (train/test division not yet done) according to the client (A/B/C/D) and I want to run a simulation in this kind of an environment

Any help will be appreciated!",2,5,1.0,2024-11-06 12:28:17,lel_73,[],0,616,1673,1499,True,0.2083333333333333,NEGATIVE,0.9993983507156372,ai,MachineLearning
[D] [R] Problems understanding DSP-like pipelines,id like to hear your opinion on this new paradigm of interacting with llms in particular im talking about simple stuff like reflection like selfrefine and reflexion up to more complex stuff like selfask selfrag dsp or even agentic llms ive read a couple of surveys about these topics and im reading each of the aforementioned papers but everything seems quite foggy to me i can understand the inner workings of a simple rag pipeline with incontextlearning and frozen llms but adding all these layers of abstractions and interactions make everything so damn complicated not only to understand but even to replicate,"I'd like to hear your opinion on this new paradigm of interacting with LLMs.   
In particular, I'm talking about ""simple"" stuff like Reflection (like Self-refine and Reflexion), up to more complex stuff like Self-Ask, Self-RAG, DSP, or even Agentic LLMs.   
I've read a couple of surveys about these topics and I'm reading each of the aforementioned papers, but everything seems quite foggy to me. I can understand the inner workings of a simple RAG pipeline with in-context-learning and frozen LLMs, but adding all these layers of abstractions and interactions make everything so damn complicated not only to understand but even to replicate. ",1,0,1.0,2024-11-06 12:15:10,Debonargon,[],0,613,1244,338,True,0.00030303030303029387,NEGATIVE,0.964468240737915,ai,MachineLearning
"[P] I made a tool for building and training neural networks visually, operation by operation ",hey i mostly made this as a tool to learn how to implement backpropagation and get some intuition on how it works so i figure it might be useful for someone else i also wrote up an article in the readme on how backpropagation and model training works does this seem useful to you is this something youd play around with i cant really figure out what to do with it so im curious to hear the communitys thoughts,"Hey! I mostly made this as a tool to learn how to implement backpropagation and get some intuition on how it works, so I figure it might be useful for someone else! I also wrote up an article in the readme on how backpropagation and model training works: [https://github.com/PavleMiha/mlgarden](https://github.com/PavleMiha/mlgarden)

Does this seem useful to you? Is this something you'd play around with? I can't really figure out what to do with it, so I'm curious to hear the community's thoughts!",32,10,0.95,2024-11-06 11:50:56,Massena,[],0,409,5884,23097,True,0.24,NEGATIVE,0.9178110957145691,ai,MachineLearning
"[D] Which LLM do you use for analysing Financials, P & Ls, Balance Sheets?",if any of you has tried different llms i am super curious which one did you find works great for analysing financials pls balance sheets for a company i am looking to use it regularly so itd be great if you tried any specific llms that you found they work good with reasoning actually analysing the numbers properly and giving insights on them thank you,"If any of you has tried different LLMs, I am super curious which one did you find works great for analysing Financials, P/Ls, Balance Sheets for a company?

I am looking to use it regularly so it'd be great if you tried any specific LLMs that you found they work good with reasoning, actually analysing the numbers properly and giving insights on them.

Thank you!",0,2,0.33,2024-11-06 09:55:46,eaerdiablosios,[],0,353,1014,197,True,0.2533333333333333,POSITIVE,0.9950087070465088,ai,MachineLearning
[D] Need Advice Starting my Recommendation Engine Project for my Employer,title sums it up im mostly familiar with time series prediction models as thats what ive spent most of my time building im a data analyst thats recently built some cool ml stuff but i need to build a recommendation engine for my employer who has an ecommerce site and sells physical products i know the first step is data collection about the users my question to you all is where should i store this data that i collect datalake relational database etc how do i go about picking an algorithm im used to using lstm and local bayesian for time series and what are some general rules and advice from those who have built something like this before you all are awesome thanks for your help,"Title sums it up. I'm mostly familiar with time series prediction models, as that's what I've spent most of my time building (I'm a data analyst that's recently built some cool ML stuff). But I need to build a recommendation engine for my employer who has an ecommerce site and sells physical products.

I know the first step is data collection about the users. My question to you all is:

Where should I store this data that I collect (Datalake, Relational Database, etc)?

How do I go about picking an algorithm (I'm used to using LSTM and Local Bayesian for time series)?

And what are some general rules and advice from those who have built something like this before?

You all are awesome. Thanks for your help!",1,4,0.6,2024-11-06 09:15:14,Lower-Feeling2752,[],0,686,19,27,True,0.23863636363636365,NEGATIVE,0.9962399005889893,ai,MachineLearning
[D] Can we transfer language capabilities of one LLM to another?,i have seen techniques to transfereffectively let one model teach another model its unique capabilitydomain knowledge but can this made possible for language capability as well for example if we have a model that is proficient in chinese is there any way to transferteach that chinese proficiency to another model without us having access to the original chinese corpus used to train the teacher model any insights would be greatly appreciated thank you beforehand,"I have seen techniques to transfer/effectively let one model teach another model its unique capability/domain knowledge. But can this made possible for language capability as well? 

For example, if we have a model that is proficient in Chinese, is there any way to transfer/teach that Chinese proficiency to another model without us having access to the original Chinese corpus used to train the teacher model?

Any insights would be greatly appreciated, thank you beforehand!",0,3,0.5,2024-11-06 09:14:29,worthlesspineapple,[],0,464,810,13,True,0.1357142857142857,POSITIVE,0.9668750762939453,ai,MachineLearning
[R] Amazon Researchers Find LLMs do not always follow User Requests and Propose a Self-Correction Pipeline,came across this interesting paper being presented next week at emnlp 2024 llm selfcorrection with decrim decompose critique and refine for enhanced following of instructions with multiple constraints this study dives into an important question do llms really do what we ask them to we often rely on llms for tasks with specific instructions but when these instructions get complex and multiconstrained like requesting specific tones or avoiding certain words do llms actually follow through this paper suggests that the answer might be more complicated than we think the authors created a new benchmark realinstruct which uses realworld user instructions rather than synthetic prompts they estimated that at least 30 of real user requests contain multiple constraints that llms must follow in their results even advanced models like gpt4 fail to meet at least one requirement over 21 of the instructions tested so while llms perform well in simple cases their performance drops when handling more intricate multistep requests to address these gaps the authors developed a selfcorrection pipeline called decrim where the model breaks down each instruction checks its response against each requirement and iteratively refines it as needed through decrim opensource models like mistral saw notable improvements even surpassing gpt4 on the benchmarks initial tests showed that llms couldnt selfcorrect reliably alone however with weak but minimally reliable auxiliary feedback they achieved up to an 8 boost with highquality ideal feedback decrim brought mistrals performance up by 34 surpassing gpt4 on both realinstruct and ifeval benchmarks i think this paper fits in a new trend on llms these system 2 reasoning models like gpto1 that try to mimic some thinking reflection before outputting their response anyway it is shocking that llms perform that bad in a task that seems simply the most important ones for the user following what the users ask is this type of model making us closer to agi or is this just proving that this magic agi that some people talk about is actually much much far away yet paper their post on linkedin,"Came across this interesting paper being presented next week at EMNLP 2024: *LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints*.

This study dives into an important question: **Do LLMs really do what we ask them to?** We often rely on LLMs for tasks with specific instructions, but when these instructions get complex and multi-constrained, like requesting specific tones or avoiding certain words, do LLMs actually follow through? This paper suggests that the answer might be more complicated than we think.

The authors created a new benchmark, RealInstruct, which uses real-world user instructions rather than synthetic prompts. **They estimated that at least 30% of real user requests contain multiple constraints that LLMs must follow**. In their results **even advanced models like GPT-4 fail to meet at least one requirement over 21% of the instructions tested**. So, while LLMs perform well in simple cases, their performance drops when handling more intricate, multi-step requests.

To address these gaps, the authors developed a self-correction pipeline called DECRIM, where the model breaks down each instruction, checks its response against each requirement, and iteratively refines it as needed. Through DECRIM, open-source models like Mistral saw notable improvements, even surpassing GPT-4 on the benchmarks. **Initial tests showed that LLMs couldnâ€™t self-correct reliably alone**, however with weak but minimally reliable auxiliary feedback, **they achieved up to an 8% boost**. **With high-quality â€œidealâ€ feedback, DECRIM brought Mistralâ€™s performance up by 34%, surpassing GPT-4 on both RealInstruct and IFEval benchmarks.**

I think this paper fits in a new trend on LLMs, these System 2 Reasoning models like GPT-o1 that try to mimic some thinking / reflection before outputting their response. Anyway it is shocking that LLMs perform that bad in a task that seems simply the most important ones for the user, following what the users ask. Is this type of model making us closer to AGI? Or is this just proving that this magic AGI that some people talk about is actually much much far away yet? 

Paper: [https://arxiv.org/pdf/2410.06458](https://arxiv.org/pdf/2410.06458)

[Their post on Linkedin](https://www.linkedin.com/posts/thomasferraz_emnlp2024-ai-llms-activity-7259680754299731968-uLBk?utm_source=share&utm_medium=member_desktop)

https://preview.redd.it/techjo8pfazd1.png?width=2794&format=png&auto=webp&s=18155cdbf4ba164f48480d4583c3cfea1d40298e

",44,3,0.85,2024-11-06 09:06:41,Mundane_Sir_7505,[],0,2131,106,44,True,0.05148572188045871,NEGATIVE,0.9950172305107117,ai,MachineLearning
[R] Help with CNN-RNN Architecture for Self-Supervised Matrix Completion,hi all im working on a selfsupervised learning approach to estimate missing or uncertain data in a freeway traffic density dataset inspired by matrix completion methods the dataset is generated from simulated freeway traffic discretized in time and space to form a grid of cells each cell reflects a traffic density value observed from mobile sensors i have three core arrays 1 actual_density_values ground truth density used only for evaluation not training 2 observed_density_values traffic density observed from mobile sensors with some cells unobserved 3 certainty_values coverage certainty for each observed cell range 0 to 1 with dimensions t e s l where t number of time steps e movement directions edges expected to be 2 eg forward and reverse s spatial segments l lanes goal the goal here is to build a model that can improve the estimation for cells where the certainty is less than 1 i want the model to capture dependencies over time and space using selfsupervision to fill in unobserved or uncertain values more accurately proposed approach heres what im thinking in terms of architecture 1 temporal dependencies using a cnn to capture timebased dependencies over time steps t 2 spatial dependencies using an rnn to model dependencies across spatial segments s and lanes l 3 model structure data masking at each time step mask some of the observed data especially the lowercertainty cells so the model learns to predict uncertain values better cnnrnn combo combining cnn and rnn layers to learn from both the temporal and spatial aspects loss function using a selfsupervised loss function that prioritizes accurate reconstruction of observed densities particularly focusing on uncertain cells for training i wont use the ground truth array actual_density_values its only for evaluation 4 evaluation once trained i plan to compute the rmspe root mean square percentage error between actual_density_values and the models predicted observed_density_values im especially interested in the improvements on the lowercertainty cells question 1 does this cnnrnn combination sound like a good fit for this kind of matrix completion task are there alternative approaches or tweaks that might make it more effective 2 any recommendations for loss functions that work well in selfsupervised setups especially where i want to prioritize lowcertainty values 3 are there best practices for masking observed values in selfsupervised learning setups like this 4 any advice on regularization techniques to prevent overfitting given the selfsupervised nature of the task also any tips on ensuring scalability,"Hi all, Iâ€™m working on a self-supervised learning approach to estimate missing or uncertain data in a freeway traffic density dataset, inspired by matrix completion methods.

The dataset is generated from simulated freeway traffic, discretized in time and space to form a grid of cells. Each cell reflects a traffic density value observed from mobile sensors. I have three core arrays:

1. **actual\_density\_values**: Ground truth density, used only for evaluation, not training.
2. **observed\_density\_values**: Traffic density observed from mobile sensors, with some cells unobserved.
3. **certainty\_values**: Coverage certainty for each observed cell (range: 0 to 1).

with dimensions (T, E, S, L), where:

* **T**: Number of time steps
* **E**: Movement directions (edges) â€“ expected to be 2 (e.g., forward and reverse)
* **S**: Spatial segments
* **L**: Lanes



**Goal**

The goal here is to build a model that can improve the estimation for cells where the certainty is less than 1. I want the model to capture dependencies over time and space, using self-supervision to â€œfill inâ€ unobserved or uncertain values more accurately.



**Proposed Approach**

Hereâ€™s what Iâ€™m thinking in terms of architecture:

1. **Temporal Dependencies**: Using a CNN to capture time-based dependencies over time steps (T).
2. **Spatial Dependencies**: Using an RNN to model dependencies across spatial segments (S) and lanes (L).
3. **Model Structure**:
   * **Data Masking**: At each time step, mask some of the observed data, especially the lower-certainty cells, so the model learns to predict uncertain values better.
   * **CNN-RNN Combo**: Combining CNN and RNN layers to learn from both the temporal and spatial aspects.
   * **Loss Function**: Using a self-supervised loss function that prioritizes accurate reconstruction of observed densities, particularly focusing on uncertain cells. For training, I wonâ€™t use the ground truth array (actual\_density\_values); itâ€™s only for evaluation.
4. **Evaluation**: Once trained, I plan to compute the RMSPE (Root Mean Square Percentage Error) between actual\_density\_values and the modelâ€™s predicted observed\_density\_values. Iâ€™m especially interested in the improvements on the lower-certainty cells.



**Question**

1. Does this CNN-RNN combination sound like a good fit for this kind of matrix completion task? Are there alternative approaches or tweaks that might make it more effective?
2. Any recommendations for loss functions that work well in self-supervised setups, especially where I want to prioritize low-certainty values?
3. Are there best practices for masking observed values in self-supervised learning setups like this?
4. Any advice on regularization techniques to prevent overfitting, given the self-supervised nature of the task? Also, any tips on ensuring scalability?",1,1,1.0,2024-11-06 07:44:55,NoTheme6450,[],0,2602,395,1,True,0.28035714285714286,NEGATIVE,0.9938032627105713,ai,MachineLearning
[D] Want to move away from coding heavy ML but still want to complete the PhD,hi folks i come from a tradition electrical engineering background doing things like industrial automation and computer vision i decided to pursue a phd in ml as i thought it will be a good field to enter given my past experience now i have been doing the phd for the past three years while i like my group and research i am getting discourageddepressed by 1 the publication rat race 2 post graduation opportunities mostly being coding heavy 3 the inability to carve a name for myself in the field given how crowded the field has become thus ideally i would like to complete my phd and move into a more relaxed paced even if it is not as high paying as ml jobs non coding heavy but technical job where i do not have to constantly upskill myself do you folks have any suggestion on what jobs i can look into or would you suggest dropping the phd and doing something else tldr 4th year ml phd student unsure of sticking with the phd as they desire a non coding heavy technical job in the industry post graduation seeking advice on what to do,"Hi Folks,

I come from a tradition electrical engineering background doing things like industrial automation and computer vision. I decided to pursue a PhD in ML as I thought it will be a good field to enter given my past experience. Now I have been doing the PhD for the past three years. While I like my group and research, I am getting discouraged/depressed by (1) The publication rat race (2) post graduation opportunities mostly being coding heavy (3) the inability to carve a name for myself in the field given how crowded the field has become.

Thus, ideally I would like to complete my PhD and move into a more relaxed paced (even if it is not as high paying as ML jobs) non coding heavy but technical job, where I do not have to constantly up-skill myself. Do you folks have any suggestion on what jobs I can look into or would you suggest dropping the PhD and doing something else?

TLDR: 4th year ML PhD student unsure of sticking with the PhD as they desire a non coding heavy technical job in the industry post graduation. Seeking advice on what to do.",76,53,0.89,2024-11-06 07:18:41,Hopeful-Reading-6774,[],0,1039,1071,910,True,0.12571428571428572,NEGATIVE,0.9924693703651428,ai,MachineLearning
[D] Evolving Matrix Computation Techniques for Modern AI: What's New?,as ai models continue to scale in both complexity and size im interested in how the field of matrix computations is evolving to meet these new challenges what are some of the latest advancements or strategies in matrix computation that are improving efficiency and adaptability for modern ai systems are there any recent breakthroughs or shifts in our approach to these computations that are making a significant impact in ai research and applications,"As AI models continue to scale in both complexity and size, I'm interested in how the field of matrix computations is evolving to meet these new challenges. What are some of the latest advancements or strategies in matrix computation that are improving efficiency and adaptability for modern AI systems? Are there any recent breakthroughs or shifts in our approach to these computations that are making a significant impact in AI research and applications?",24,11,0.89,2024-11-06 06:51:35,Glittering_Age7553,[],0,451,1355,177,True,0.24356060606060606,POSITIVE,0.9979052543640137,ai,MachineLearning
[D] What if llm's are trained to predict more than 1 token at a time? ,is there any reason to train llms to predict only one token like wouldnt inference be 2 times faster if it was trained to predict just 2 thats huge gain sure there can be performance loss but for inference we already do quantization to increase speed which decreases performance anyway will having llm predict more than 1 token decrease it more,"is there any reason to train llms to predict only one token? like wouldnt inference be 2 times faster if it was trained to predict just 2? thats huge gain , sure there can be performance loss but for inference we already do quantization to increase speed which decreases performance anyway, will having llm predict more than 1 token decrease it more?",0,7,0.35,2024-11-06 04:11:39,limitless_11111,[],0,344,50,27,True,0.38,NEGATIVE,0.9982611536979675,ai,MachineLearning
"[D] As a researcher, how do you become industry-ready?",being a phd student much of my time is spent on supervising students project management and writing quick and dirty code for prototyping i intend to move to industry after the phd but i feel like im missing out on key software engineering skills and good coding practices does anyone else feel this way how do you upskill yourself to be industryready while doing a phd,"Being a PhD student, much of my time is spent on supervising students, project management and writing ""quick and dirty"" code for prototyping. I intend to move to industry after the PhD, but I feel like I'm missing out on key software engineering skills and good coding practices. Does anyone else feel this way? How do you upskill yourself to be industry-ready while doing a PhD? ",153,42,0.94,2024-11-06 02:07:23,fullgoopy_alchemist,[],0,368,1597,1063,True,0.047619047619047616,NEGATIVE,0.9995954632759094,ai,MachineLearning
[P] Open-source declarative framework to build LLM applications - looking for contributors,ive been building llmbased applications and was super frustated with all major frameworks langchain autogen crewai etc they also seem to introduce a pile of unnecessary abstractions it becomes super hard to understand whats going behind the curtains even for very simple stuff so i just published this opensource framework gensphere the idea is have something like docker for llms you build applications with yaml files that define an execution graph nodes can be either llm api calls regular function executions or other graphs themselves because you can nest graphs easily building complex applications is not an issue but at the same time you dont lose control you basically code in yaml stating what are the tasks that need to be done and how they connect other than that you only write individual python functions to be called during the execution no new classes and abstractions to learn its all opensource now im looking for contributors to adapt the framework for cycles and conditional nodes which would allow fullfledged agentic system building pls reach out if you want to contribute there are tons of things to do ps you can read the detailed docs here and go over this quick google colab tutorial,"I've been building LLM-based applications, and was super frustated with all major frameworks - langchain, autogen, crewAI, etc. They also seem to introduce a pile of unnecessary abstractions. It becomes super hard to understand what's going behind the curtains even for very simple stuff.

[So I just published this open-source frameworkÂ GenSphere.](https://github.com/octopus2023-inc/gensphere)Â The idea is have something likeÂ **Docker for LLMs**. You build applications with YAML files, that define an execution graph. Nodes can be either LLM API calls, regular function executions or other graphs themselves. Because you can nest graphs easily, building complex applications is not an issue, but at the same time you don't lose control.

You basically code in YAML, stating what are the tasks that need to be done and how they connect. Other than that, you only write individual python functions to be called during the execution. No new classes and abstractions to learn.

Its all open-source. **Now I'm looking for contributors** to adapt the framework for cycles and conditional nodes - which would allow full-fledged agentic system building! Pls reach out Â if you want to contribute, there are tons of things to do!

PS:Â [you can read the detailed docs here,](https://gensphere.readthedocs.io/en/latest/)Â And go over this quickÂ [Google Colab tutorial.](https://github.com/octopus2023-inc/gensphere/blob/main/examples/gensphere_tutorial.ipynb)",1,1,0.6,2024-11-05 22:34:51,Jazzlike_Tooth929,[],0,1209,183,250,True,0.010332491582491582,NEGATIVE,0.9997724890708923,ai,MachineLearning
On a successful research with low budget [D],hi i got a research idea and applied it on nanogpt repo for lm training and validated transformer generalizes better on validation loss but worse training loss and is more prone to overfitting since like training loss a little worse validation loss a little better i only applied to full shakespeare_char and a subset on openwebtext bcz 10 usd on runpod only allows me to do this i am still going to release a paper since i get good results and done some math work should i do it,"Hi, i got a research idea and applied it on nanogpt repo for lm training and validated transformer generalizes better on validation loss but worse training loss and is more prone to overfitting since like training loss a little worse validation loss a little better, i only applied to full shakespeare\_char and a subset on openwebtext bcz 10 usd on runpod only allows me to do this, i am still going to release a paper since i get good results and done some math work, should i do it?",0,1,0.25,2024-11-05 20:37:05,Mean-Force267,[],0,479,9,355,True,0.125,NEGATIVE,0.9983349442481995,ai,MachineLearning
